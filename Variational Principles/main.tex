\documentclass{article}
\usepackage{../header}
\title{Varitational Principles}
\author{Notes by Finley Cooper}
\begin{document}
  \maketitle
  \newpage
  \tableofcontents
  \newpage
  \section{Calculus on $ \R^n $}
  Consider $ f:\R^n\to \R $. We'll write $ (x_1,x_2,\dots,x_n) $ as $ \mathbf x $. Assume $ f $ is sufficiently differentiable (at least in $ C^2 $).
  \subsection{Stationary points}
  \begin{definition}
	  (Stationary point) A point $\mathbf a\in \R^n  $ is a \textit{stationary point} of $ f $ if and only if $ (\nabla f)(\mathbf a)=0 $
  \end{definition}
  Taylor expand about such an $ \mathbf a $:
  \[
	  f(\mathbf x)=f(\mathbf a)+\sum_{i=1}^n(x_i-a_i)(\nabla_i f)(\mathbf a)+\frac 12 \sum_{i,j}(x_i-a_i)(x_j-a_j)H_{ij}(\mathbf a)+O(|\mathbf x-\mathbf a|^3).
  \]
  Since $ \mathbf a $ is stationary point, the linear term is zero, hence the behaviour of $ f $ around $ \mathbf a $ is determined by the Hessian.
  \[
	  H_{ij}=\frac{\partial^2 f}{\partial x_i\partial x_j}=H_{ji}
  \]
  We can assume \textit{wlog} that $ \mathbf a = 0 $ by a translation, so
  \[
	  f(\mathbf x)-f(\mathbf a)=\frac 12 x_iH_{ij}x_j+O(|\mathbf x|^3).
  \]
  Since $ H_{ij} $ is symmetric, we can diagonalise $ H $ by a rotation matrix so $ x_i=R_{ij}x_j' $ we choose $ R $ so that
  $ H'=R^HR=\mathrm{diag}(\lambda_1,\lambda_2,\dots,\lambda_n) $. Since $ H $ is symmetric, $ \lambda_i\in \R $ for all $ i $. So the series becomes
  \[
	  f(\mathbf x)-f(\mathbf a)=\frac 12 \sum_{i=1}^n\lambda_i(x'_i)^2+O(|\mathbf x|^3).
  \]
  If $ \lambda_i>0 $ for all $ i $ then $ f(\mathbf x)>f(\mathbf a) $ for small enough $ \mathbf x $ so we get a local minimium.\\
  If $ \lambda_i<0 $ for all $ i $ then $ f(\mathbf x)<f(\mathbf a) $ for small enough $ \mathbf x $ so we get a local maximium.\\
  Otherwise if all non-zero we get a saddle point, and if some are zero then we get a degenerate stationary point, so we need to look at the next term in the Taylor expansion to describe the behaviour at the point.\par
  If we're working in $ \R^2 $ we can work with determinates and traces since $ \det H=\lambda_1\lambda_2 $ and $ \tr(H)=\lambda_1+\lambda_2 $ so
  \begin{enumerate}
	  \item If $ \det H>0 $, $ \tr H>0 \implies $ local minimium.
	  \item If $ \det H>0 $, $ \tr H<0 \implies $ local maximium.
	  \item If $ \det < 0 $ we get a saddle point.
	  \item If $ \det = 0 $ we're in the degenerate case.
  \end{enumerate}
  \begin{remark}
    A local minimium is not a global minimium. There might be some other stationary point which is the global minimium, the minimium may also occur on the boundary of the domain or the function might be unbounded from below in the domain and not achieve a global minimium. We can make the same statements for maximiums.
\end{remark}
Let's see an example with the function $ f(x,y)=x^3+y^3-3xy $.
\[
  \nabla f=(3x^2-3y,3y^2-3x)
\]
So $ \nabla f = 0\iff x^2=y $ and $ y^2=x $. This gives the solutions $ (0,0),(1,1) $ only.
\[
	H= 
  \begin{pmatrix}
	  6x & -3 \\
	  -3 & 6y
  \end{pmatrix}
\]
So
\[
	\det H = 9(4xy-1) \text{ and } \tr H = 6(x+y).
\]
Hence our point $ (1,1) $ is a local minimium and the point $ (0,0) $ is a saddle point.\\
We can work out the eigenvectors of the point $ (0,0) $ which are
\begin{align*}
	\lambda_1=-3 &\quad e_1=\begin{pmatrix}
	  1 \\
	  1
	\end{pmatrix}\\
		\lambda_2=3 &\quad e_2=\begin{pmatrix}
		  1 \\
		  -1
		\end{pmatrix}
\end{align*}
So it is a maximium along the line $ y=x $ and a minimium along the line $ y=-x $.\\
For this example there does not exist a global minimium or maximium since $ f $ increases/decreases without bound on the domain.
\subsection{Convex functions}
\begin{definition}
	(Convex set) A set $ S $ is \textit{convex} if and only if $ \forall x,y\in S $ and $ \forall t\in (0,1) $ we have that $ (1-t)x+ty\in S $.
\end{definition}
Now we'll define convexity for functions. Let $ f $ be a function domain with domain $ D(f)\subseteq \R^n  $. The graph of $ f $ is the surface $ z=f(\mathbf x) $ in $ \R^{n+1} $ with coordinates $ (\mathbf x,z) $. A chord of $ f $ is a line section joining two points of its graph.
\begin{definition}
	(Convex function) A function $ f:S\to \R $ is convex if
	\begin{enumerate}
		\item $ S $ is a convex set
		\item The graph of $ f $ lies below on or below all of its chords i.e we have that
			\[
			  f((1-t)\mathbf x + t\mathbf y)\le (1-t)f(\mathbf x)+tf(\mathbf y)
			\]
			for all $ t\in (0,1) $ and for all $ \mathbf x, \mathbf y \in S $.\\
			We say that $ f $ is \textit{strictly convex} if the condition in (ii) is strict when $ \mathbf x\ne \mathbf y $.
	\end{enumerate}
\end{definition}
\begin{remark}
See that we require the property $ (i) $ to exists for property (ii) to be well-defined.
\end{remark}
For strict convexity we replace the $ \le $ sign with the $ < $ sign.
\begin{lemma}
  $ f $ is strictly concave if and only if $ f $ is strictly convex.
\end{lemma}
\pf Trivial. \qed
\subsection{First order conditions}
\begin{theorem}
  If $ f $ is differentiable $ f $ on a convex domain $ D(f) $, then $ f $ is convex if and only if
  \[
    f(\mathbf y) \ge  f(\mathbf x) + (\mathbf y -\mathbf x) \cdot\nabla f(\mathbf x).
  \]
\end{theorem}
\begin{corollory}
  If $ f $ is convex and differentiable then any stationary point is a global minimium.
\end{corollory}
\begin{remark}
   We have similiar statements for concave functions and global maximiums.
\end{remark}
\pf (Of the corollory) If $ \mathbf a $ is a stationary point, then $ \nabla \mathbf a = 0 $ so taking $ \mathbf x = \mathbf a $ in the theorem we get that \[
  f(\mathbf y)\ge f(\mathbf x)\quad\forall\mathbf y\in D(f)
\]
hence $ \mathbf x $ is a global minimium \qed\par
\pf (Of the theorem) Let's first show the forward direction and let 
\[
  h(t)=(1-t)f(\mathbf x)+tf(\mathbf y)-f((1-t)\mathbf x + t\mathbf y).
\]
Since $ f $ is differentiable we can take the derivative with respect to $ t $. This gives that
\[
  h'(0)=-f(\mathbf x)+f(\mathbf y)-(\mathbf y - \mathbf x)\cdot\nabla f(\mathbf x).
\]
Now consider
\[
	h'(0)=\lim_{t\to 0}\frac{h(t)-h(0)}t=\lim_{t\to 0}\frac{h(t)}t
\]
and from our assumptions we have that $ h(t) $ is positive in $ (0,1) $ so $ h'(0)\ge 0 $ so we have the statement in our theorem.\\
For the converse, we have that
\begin{align*}
	f(\mathbf x)&\ge f(\mathbf z)+(\mathbf x - \mathbf z)\cdot\nabla f(\mathbf z)\\
	f(\mathbf y) &\ge f(\mathbf z) + (\mathbf y - \mathbf z)\cdot \nabla f(\mathbf z)
\end{align*}
which gives that
\[
	(1-t)f(\mathbf x)+tf(\mathbf y)\ge f(\mathbf z)+[(1-t)\mathbf x +t\mathbf y - \mathbf z]\cdot\nabla f(\mathbf z)
\]
Now if we choose
\[
  \mathbf z=(1-t)\mathbf x +t \mathbf y
\]
we get the required result, which completes the proof.\qed\par
We also have an alternative first order condition which is equivalent.
\begin{claim}
  The previous theorem is also equivalent to the inequality
  \[
	  (\mathbf y-\mathbf x)\cdot\left[\nabla f(\mathbf y)-\nabla f(\mathbf x)\right]\ge 0\quad \forall \mathbf x,\mathbf y\in D(f). 
  \]
\end{claim}
For $ n=1 $ the claim says that $ (y-x)(f'(y)-f'(x)) $ which implies that $ f'(y)\ge f'(x) $ for $ y>x $ i.e. that $ f'(x) $ is increasing on the domain.\par
\pf Assuming that $ f(\mathbf y)\ge f(\mathbf x) +(\mathbf y-\mathbf x)\cdot\nabla f(\mathbf x) $ and the same equation formed by replacing $ y\to x $ and $ x\to y $ and adding the two equations we exactly get the equation required.\\
For the converse assume our claim. Now define $ \mathbf z = (1-t)\mathbf x + t\mathbf y $.
\begin{align*}
	f(\mathbf y) - f(\mathbf x)=\left[f(z)\right]^{t=1}_{t=0}&=\int_0^1\mathrm dt \frac{\mathrm d}{\mathrm dt}f(\mathbf z)\\
								 &=\int_0^1\mathrm dt(\mathbf y-\mathbf x)\cdot\nabla f(\mathbf z) \\
	f(\mathbf y)-f(\mathbf x)-(\mathbf y-\mathbf x)\cdot\nabla f(\mathbf x)&= \int_0^1\mathrm dt\left\{(\mathbf y-\mathbf x)\cdot\left[\nabla f(\mathbf z)-\nabla f(\mathbf x)\right]\right\}							
\end{align*}
Replacing $ \mathbf y\to\mathbf z $ and using our claim, we see that the integand is positive hence the LHS $ \ge 0 $ so we've proved our claim.\qed
\subsection{Second order conditions}
\begin{claim}
  If $ f\in C^2 $ then the first order conditions are equivalent to all the eigenvalues of the Hessian matrix being non-negative $ \forall \mathbf x\in D(f) $.
\end{claim}
\pf Let's assume the first order conditions, replacing $ \mathbf y = \mathbf x +\mathbf h $ so we get that 
\[
	\mathbf h\cdot[\nabla f(\mathbf x+\mathbf h-\nabla f(\mathbf x)]\ge 0.
\]
Taylor expanding we get that
\[
	\nabla_i f(\mathbf x+\mathbf h)=\nabla_i f(\mathbf x)+h_jH_{ij}(\mathbf x)+o(h^2)
\]
therefore 
\[
	h_ih_jH_{ij}+o(h^3)\ge 0
\]
If $ H(\mathbf x) $ had a negative eigenvalue $ \lambda $ with eigenvector $ \mathbf e $ set $ \mathbf h=h\mathbf e $.
\begin{align*}
  \implies \lambda h^2\mathbf e^2+o(h^3)\ge 0
\end{align*}
but the LHS is less than $ 0 $ for small enough $ h $ contradiction.\par
Now for the converse assume that $ n=1 $. So $ H(x)=f''(x) $. Hence we have that $ f''(x)\ge 0 $ for all $ x $.
\begin{align*}
	0&\ge \mathrm{sgn}(y-x)\int_x^yf''(z)\mathrm dz\\
	 &=\mathrm{sgn}(y-x)(f'(y)-f'(x))\\
	 &= (y-x)(f'(y)-f'(x))
\end{align*}
which is the first order condition. Hence we've proved both directions.\qed
\section{Legendre Transform}
\begin{definition}
	(Legendre transform) The \textit{Legendre transform} of a function $ f:D(f)\to \R $ is defined as
	\[
		f^*(\mathbf p)=\sup_{\mathbf x}\left[\mathbf p\cdot \mathbf x-f(\mathbf x)\right]
	\]
	with the domain of $ f^* $ being the subset of $ \R^n $ where the suprenum exists.
\end{definition}
\begin{claim}
  $ f^* $ is convex.
\end{claim}
\pf Let $ \mathbf p,\mathbf q\in D(f^*) $ and take $ t\in (0,1) $. Then we need to show that
\begin{align*}
	\sup\left\{\left[(1-t)\mathbf p +t\mathbf q\right]\cdot \mathbf x-f(\mathbf x)\right\}&=\sup\left\{(1-t)[\mathbf p \cdot \mathbf x-f(\mathbf x)]+t[\mathbf q\cdot \mathbf x-f(\mathbf x)]\right\}\\
									&\le (1-t)\sup[\mathbf p\cdot\mathbf x- f(\mathbf x)]+t\sup[\mathbf q\cdot \mathbf x - f(\mathbf x)]
\end{align*}
So RHS finite $ \implies $ LHS finite $ \implies (1-t)\mathbf p + t\mathbf q \in D(f^*)$. Hence we have that $ D(f^*) $ convex and $ f^*((1-t)\mathbf p+t\mathbf q) \le (1-t)f^*(\mathbf p)+tf^*(\mathbf q) $ \qed
\begin{claim}
	If $ f $ is a convex function, then $ F_{\mathbf p}(\mathbf x)=f(\mathbf x)-\mathbf p\cdot\mathbf x $ is also convex
\end{claim}
\pf Exercise.
\begin{corollary}
  If $ f $ convex and differentiable then any stationary point of $ \mathbf p\cdot\mathbf x - f(\mathbf x) $ is a global maximium occuring at $ \mathbf x(\mathbf p)  $ found by solving $ \nabla f(\mathbf x)=\mathbf p $.
\end{corollary}
Legendre transform of $ f $ is then $ f^*(\mathbf p)=\mathbf p \cdot \mathbf x(\mathbf p)-f(\mathbf x(\mathbf p)) $
\end{document}
