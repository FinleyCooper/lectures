\documentclass{article}
\usepackage{../header}
\title{Linear Algebra}
\author{Notes by Finley Cooper}
\newcommand{\F}{\mathbb{F}}
\newcommand{\n}{\mathrm{n}}
\newcommand{\rk}{\mathrm{rk}}
\begin{document}
  \maketitle
  \newpage
  \tableofcontents
  \newpage
  \section{Vector Spaces}
  \subsection{Definitions}
  For this lecture course, $ \F $ will always be field.
  \begin{definition}
	  (Vector Space) A $ \F $\textit{-vector space} (or a vector space over $ \F $) is an abelian group $ (V,+,\boldsymbol 0) $ equipped with a function
	  \begin{align*}
	    \F\times V\to V \\
	    (\lambda, v)\to \lamda v
	  \end{align*}
	  which we call scalar multiplication such that $ \forall v,w\in V, \forall \lambda,\mu\in\F $
	  \begin{enumerate} 
		  \item $ (\lambda + \mu)v=\lambda v + \mu v $
		  \item $ \lambda(v + w)=\lambda v + \lambda w $
		  \item $ \lambda(\mu v)=(\lambda \mu)v $
		  \item $ 1\cdot v = v\cdot 1 = v $
	  \end{enumerate}
  \end{definition}
  Remember that $ \boldsymbol 0 $ and $ 0 $ are not the same thing. $ 0 $ is an element in the field $ \F $ and $ \boldsymbol 0 $ is the additive identity in $ V $.\par
  For an example consider $ \F^n $ n-dimensional column vectors with entries in $ \F $. We also have the example of a vector space $ \C^n $ which is a complex vector space, but also a real vector space (taking either $ \C $ or $ \R $ as the underlying scalar field).\par
  We also can see that $ M_{m\times n}(\F) $ form a vector space with $ m $ rows and $ n $ columns.\\
  For any non-empty set $ X $, we denote $ \F^X $ as the space of functions from $ X $ to $ \F $ equipped with operations such that:
\begin{align*}
	f+g \text{ is given by } (f+g)(x)=f(x)+g(x)\\
	\lambda f \text{ is given by } (\lambda f)(x)=\lambda f(x)
\end{align*}
\begin{proposition}
  For all $ v\in V $ we have that $ 0\cdot v = \boldsymbol 0 $ and $ (-1)\cdot v=-v $ where $ -v $ denotes the additive inverse of $ v $.
\end{proposition}
\pf Trivial.
\begin{definition}
	(Subspace) A \textit{subspace} of a $ \F $-vector space $ V $ is a subset $ U\subseteq V $ which is a $ \F $-vector space itself under the same operations as $ V $. Equivalently, $ (U,+) $ is a subgroup of $ (V,+) $ and $ \forall \lambda\in \F $, $\forall u\in U $ we have that $ \lambda u \in U $.
\end{definition}
\begin{remark}
  Axioms (i)-(iv) are always automatically inherited into all subspaces.
\end{remark}
\begin{proposition}
	(Subspace test) Let $ V $ be a $ \F $-vector space and $ U\subseteq V $ then $ U $ is a subspace of $ V $ if and only if,
	\begin{enumerate}
		\item $ U $ is nonempty.
		\item $ \forall \lambda\in\F $ and $ \forall u,w\in U $ we have that $ u+\lambda w \in U $.
	\end{enumerate}
\end{proposition}
\pf If $ U $ is a subspace then $ U $ satisfies (i) and (ii) since it contains $ 0 $ and is closed. Conversely suppose that $ U\subseteq V $ satisfies (i) and (ii). Taking $ \lambda = -1 $ so $ \forall u,w\in V $, $ u-w\in U $ hence $ (U,+) $ is a subgroup of $ (V,+) $ by the subgroup test. Finally taking $ u=\boldsymbol 0 $ so we have that $ \forall w\in U,\forall\lambda\in \F $ we have that $ \lambda w\in U $. So $ U $ is a subspace of $ V $.\qed\par
We notate $ U $ by $ U\le V $.\par
For some examples
\begin{enumerate}
	\item \[
	\left\{\begin{pmatrix}
			x \\
			y \\
			z \\
	\end{pmatrix}\in \R^3:x+y+z=t\right\}\subseteq \R^3, 
\]
for fixed $ t\in \R $ is a subspace of $ \R^3 $ iff $ t = 0 $.\par
\item Take $ \R^\R $ as all the functions from $ \R $ to $ \R $ then the set of continuous functions is a subspace.
\item Also we have that $ C^\infty(\R) $, the set of infintely differentiable functions from $ \R $ to $ \R $ is a subspace of $ \R^\R $ and the subspace of continuous functions.
\item A further subspace of all of those subspaces is the set of polynomial functions.
\end{enumerate}
\begin{lemma}
  For $ U,W\le V $ we have that $ U\cap W\le V $.
\end{lemma}
\pf We'll use the subspace test. Both $ U,W $ are subspaces so they contain $ \boldsymbol 0 $ hence $ \boldsymbol 0\in U\cap W $ so $ U\cap W $ is nonempty. Secondly take $ x,y\in U\cap W $ with $ \lambda \in \F $. Then $ U\le V $ and $ x,y\in U $ so $ x+\lambda y\in U $. Similarly with $ W $ so $ x+\lambda y \in W $ hence we have that $ x+\lambda y \in U \cap W $ hence $ U\cap W\le V $\qed
\begin{remark}
  This does not apply for subspaces, in fact from IA Groups, we know it doesn't even hold for the underlying abelian group.
\end{remark}
\begin{definition}
	(Subspace sum) For $ U,W\le V $, the \textit{subspace sum} of $ U, W $ is
	\[
		U+W=\{u+w:u\in U, w\in W\}.
	\]
\end{definition}
\begin{lemma}
  If $ U, W\le V $ then $ U+W\le V $.
\end{lemma}
\pf Simple application of the subspace test.
\begin{remark}
  $ U+W $ is the smallest subgroup of $ U, W $ in terms of inclusion, i.e. if $ K $ is such that $ U\subseteq K $ and $ W\subseteq K $ then $ U+W\subseteq K $.
\end{remark}
\subsection{Linear maps, isomorphisms, and quotients}
\begin{definition}
	(Linear map) For $ V $, $ W $ $ \F $-vector spaces. A \textit{linear map} from $ V $ to $ W $ is a group homomorphism, $ \varphi $, from $ (V,+) $ to $ (W,+) $ such that $ \forall v\in V $
	\[
	  \varphi(\lambda v) = \lambda\varphi(v)
	\]
\end{definition}
Equivalently to show any function $\alpha: V\to W $ is a linear map we just need to show that $ \forall u,w\in V $, $ \forall \lambda \in\F $ we have
\[
  \alpha(u+\lambda w)=\alpha(u)+\lambda\alpha(w).
\]
For some examples of linear maps
\begin{enumerate}
	\item $ V=\F^n, W=\F^m $ $ A\in M_{m\times n}(\F) $. Then let $ \alpha:V\to W $ be given by $ \alpha(v)=Av $. Then $ \alpha $ is linear.
	\item $ \alpha:C^\infty(\R)\to C^\infty(\R) $ defined by taking the derivative.
	\item $ \alpha: C(\R)\to \R $ defined by taking the integral from $ 0 $ to $ 1 $.
	\item $ X $ any nonempty  set, $ x_0\in X $,
		\begin{align*}
		  \alpha:\F^X\to \F \\
		  f\to f(x_0)
		\end{align*}
	\item For any $ V,W $ the identity mapping from $ V $ to $ V $ is linear and so is the zero map from $ V $ to $ W $.
	\item The composition of two linear maps is linear.
	\item For a non-example squaring in $ \R $ is not linear. Similiarly adding constants is not linear, since linear maps preserve the zero vector.
\end{enumerate}
\begin{definition}
	(Isomorphism) A linear map $ \alpha:V\to W $ is an \textit{isomorphism} if it is bijective.\par
	We say that $ V $ and $ W $ are isomorphic, if there exists an isomorphism from $ V\to W $ and denote this by $ V\cong W $.
\end{definition}
An example is the vector space $ V=\F^4 $ and $ W=M_{2\times 2}(\F) $ we can define the map
\begin{align*}
	\alpha:V &\to W\\
	        \begin{pmatrix}
	         a\\b\\c\\d
	       \end{pmatrix}
		 &\to \begin{pmatrix}
		       a & b \\
		       c & d 
	       \end{pmatrix}
\end{align*}
Then $ \alpha $ is an isomorphism.
\begin{proposition}
  If $ \alpha: V\to W $ is an isomorphism then $ \inv\alpha:W\to V $ is also an isomorphism.
\end{proposition}
\pf Clearly $ \inv\alpha $ is a bijection. We need to prove that $ \inv\alpha $ is linear. Take $ w_1,w_2\in W $ and $ \lambda\in \F $. So we can write $ w_i=\alpha(v_i) $ for $ i=1,2 $. Then \[ \inv\alpha(w_1+\lambda w_2)=\inv \alpha(\alpha(v_1)+\lambda\alpha(v_2))=\inv\alpha(\alpha(v_1+\lambda v_2))=v_1+\lambda v_2=\inv \alpha(w_1)+\lambda\inv \alpha(w_2) \]. Hence $ \inv\alpha $ is linear, so $ \inv\alpha $ is an isomorphism.\qed 
\begin{definition}
	(Kernal) Let $ V,W $ be $ \F $-vector spaces. Then the \textit{kernal} of the linear map $ \alpha:V\to W $ is
	\[
		\ker(\alpha)=\{v\in V:\alpha(v)=\mathbf 0_W\}\subseteq V
	\]
\end{definition}
\begin{definition}
	(Image) Let $ V,W $ be $ \F $-vector spaces. Then the \textit{image} of a linear map $ \alpha:V\to W $ is
	\[
		\ima(\alpha)=\{\alpha(v):v\in V\}\subseteq W
	\]
\end{definition}
\begin{lemma}
  For a linear map $ \alpha:V\to W $ the following hold.
  \begin{enumerate}
	  \item $ \ker\alpha\le V $ and $ \ima \alpha \le W $
	  \item $ \alpha $ is surjective if and only if $ \ima \alpha =W $
	  \item $ \alpha $ is injective if and only if $ \ker\alpha=\{\mathbf 0_V\} $
  \end{enumerate}
\end{lemma}
\pf $ \mathbf 0_V+\mathbf 0_V=\mathbf 0_V $, so applying $ \alpha $ to both sides any using the fact that $ \alpha $ is linear gives that $ \alpha(\mathbf 0_V)=\mathbf 0_W $. So $ \ker\alpha $ is nonempty. The rest of the proof is a simple application of the subspace test.\\
The second statement is immediate from the definition.\\
For the final statement suppose $ \alpha $ injective. Suppose $ v\in \ker \alpha $. Then $ \alpha(v)=\mathbf 0_W=\alpha(\mathbf 0_w) $ so $ v=\mathbf 0_V $ by injectivity. Hence $ \ker\alpha $ is trivial.
Conversely suppose that $ \ker \alpha=\{0_V\} $ Let $ u,v\in V $ and suppose that $ \alpha(u)=\alpha(v) $. The $ \alpha(u-v)=\mathbf 0_W $, so $ u-v\in\ker\alpha $, so $ u=v $.\qed\par

For $ V $ a $ \F $-vector space, $ W\le V $ write
	\[
		\frac VW=\{v+W:v\in V\}
	\]
	as the left cosets of $ W $ in $ V $. Recall that two cosets $ v + V $ and $ u+W $ are the same coset if and only if $ v-u\in W $.
\begin{proposition}
  $ V/W $ is an $ \F $-vector space under operations
  \begin{align*}
   (u+W)+(v+W)&=(u+v)+W \\
   \lambda (v+W)&=(\lambda v)+W
\end{align*}
We call $ V/W $ the quotient space of $ V $ by $ W $.
\end{proposition}
\pf The proof is long and requires a lot of vector space axioms so we'll just sketch out the proof.\\
We check that operations are well-defined, so for $ u,\overline u, v,\overline v \in V $ and $ \lambda\in \F $ if
\[
  u+W=\overline u + W,\quad v+W=\overline v + W
\]
then
\[
	(u+v)+W=(\overline u+\overline w)+W
\]
and
\[
	(\lambda u) + W = (\lambda\overline u)+W
\]
The vector space axioms are inherited from $ V $.\qed
\begin{proposition}
	(Quotient map) The function $ \pi_W: V\to \frac VW $ called a \textit{quotient map} is given by
  \[
    \pi_W(v)=v+W
  \]
  is a well-defined, surjective, linear map with $ \ker\pi_W=W $.
\end{proposition}
\pf Surjectivity is clear. For linearity let $ u,v \in V $ and $ \lambda\in \F $. Then 
\begin{align*}
	\pi_W(u+\lambda v)&=(u+\lambda v) + W \\
			  &= (u+W)+(\lambda v+W)\\
			  &= (u+W)+\lambda(v+W) \\
			  &= \pi_W(u)+\lambda\pi_W(v)
\end{align*}
For $ v\in V $, we have that  $ v\in\ker \pi_W \iff \pi_W(v) = \mathbf 0_{V/W} $. So $ v+W=\mathbf 0_V+W $ so finally $ v=v-\mathbf 0_V\in W $.\qed
\begin{theorem}
	(First isomorphism theorem) Let $ V,W $ be $ \F $-vector spaces and $ \alpha:V\to W  $ linear. Then there is an isomorphism
	\[
		\overline\alpha:\frac{V}{\ker \alpha}\to\ima\alpha
	\]
	given by $ \overline\alpha(v+\ker\alpha)=\alpha(v) $
\end{theorem}
\pf For $ u,v\in V $,
\begin{align*}
	u+K=v=K &\iff u-v\in K
		&\iff \alpha(u-v)=\mathbf 0_W
		&\iff \alpha(u)=\alpha(v)
		&\iff \overline \alpha(u+\ker\alpha)=\overline\alpha(v+\ker\alpha)
\end{align*}
The forward direction shows that $ \overline\alpha $ is well-defined, and the converse shows that $ \overline\alpha $ is injective. For surjectivity given $ w\in\ima\alpha $, there exists some $ v\in V \st w=\alpha(v)$. Then $ w=\overline\alpha(v+\ker\alpha) $.\\
Finally for linearity given $ u,v\in V $, $ \lambda\in\F $,
\begin{align*}
	\overline\alpha((u+\ker\alpha)+\lambda(v+\ker\alpha))&=\overline\alpha((u+\lambda v)+\ker\alpha)\\
  &= \alpha(u+\lambda v)\\
  &= \alpha(u)+\lambda\alpha(v) \\
  &= \overline\alpha(u+\ker\alpha)+\lambda\overline\alpha(v+\ker\alpha)
\end{align*}
So $ \overline \alpha$ is linear hence is an isomorphism\qed
\subsection{Basis}
\begin{definition}
	(Span) Let $ V $ be a $ \F $-vector space. Then the \textit{span} of some subset $ S\subseteq V $ is
	\[
		\langle S\rangle = \left\{\sum_{s\in S}\lambda_s\cdot s: \lambda_s\in \F\right\}
	\]
	where $ \sum $ denotes finite sums. An expression the form above is called a \textit{linear combination} of $ S $.\\
	We say that $ S $ \textit{spans} $ V $ if $ \langle S\rangle =V $
\end{definition}
\begin{definition}
	(Finite-dimensional) For a vector space $ V $ we say that it is \textit{finite-dimensional} if there exists a finite spanning set.
\end{definition}
We'll give some simple remarks without proof.
  \begin{enumerate}
	  \item $ \langle S \rangle\le V $ and conversely if $ W\le V $ and $ S\subseteq W $ then $ \langle S\rangle \le W $.
	  \item If $ S,T\subseteq W $ and $ S $ spans $ V $ and $ S\subseteq \langle V\rangle $ then $ T $ spans $ V $.
	  \item By convention $ \langle\emptyset\rangle =\{\mathbf 0_V\} $.
	  \item $ \langle S\cup T\rangle = \langle S\rangle + \langle T\rangle $
  \end{enumerate}
For an example consider $ V=\R^3 $ and consider the sets
\begin{align*}
  S=\left\{\begin{pmatrix}
    1\\0\\0
  \end{pmatrix},\begin{pmatrix}
    1\\1\\2
\end{pmatrix}\right\}\\
T=\left\{\begin{pmatrix}
  2\\1\\2
\end{pmatrix},\begin{pmatrix}
  0\\1\\2
\end{pmatrix},\begin{pmatrix}
  -1\\2\\4
\end{pmatrix}\right\}
\end{align*}
Then $ \langle S\rangle = \langle T\rangle =\left\{\begin{pmatrix}
  x\\y\\2y
\end{pmatrix}:x,y\in \R\right\} \le \R^3.$\par
For a second example consider $ V=\R^\N $ and set $ T=\{\delta_n:n\in \N\} $. This is not a spanning set, since we require infinitely many elements from $ T $ to make an element in $ V $. In fact we can write that
\[
	\langle T\rangle =\{f\in \R^\N:f(n)=0\text{ for all but finitely many terms}\}.
\]
\begin{definition}
	(Linear Independence) A subset $ S\subseteq V $ is called \textit{linearly independent} if, for all finite linear combinations
\[
	\sum_{s\in S}\lambda_ss\quad \text{of S}
\]
if the sum is the zero vector in $ V $ the $ \lambda_s=0 $ for all $ s\in S $.
\end{definition}
If $ S $ is not linearly indepedent we say that $ S $ is linearly dependent.\par
We'll make some more remarks
\begin{enumerate}
	\item If $ \mathbf 0 \in S $ then $ S $ is not linearly independent.
	\item If we have a finite set, then to show linearly independent, we only need to consider the linear combination of all elements, not all finite lienar combinations.
	\item However is $ S $ is infinite, then we have to consider every possible finite subset of $ S $ and show it's linearly independent.
	\item Every subset of a linearly independent set is itself linearly indepedent.	
\end{enumerate}
\begin{definition}
	(Basis) A subset $ S\subseteq V $ is a \textit{basis} for $ V $ if $ S $ is linearly independent and a spanning set.
\end{definition}
For an example consider $ e_i\in \F^n $ be given by
\[
  e_i=\begin{pmatrix}
    0\\ \vdots \\ 0 \\ 1 \\ 0 \\\vdots \\ 0
\end{pmatrix}\quad \text{with the 1 in the } i\text{th entry}
\]
then the set $ \{e_i:1\le i\le n\} $ is the standard basis for $ \F^n $.\par
For $ P(\R) $ the set of real polynomial functions and let $ p_n\in P(\R) $ be given by $ p_n(x)=x^n $, then $ \{p_n:n\in\Z_{\ge 0}\} $ is a basis for $ P(\R) $.
\begin{proposition}
  If $ S\subseteq V $ is a finite spanning set, then there exists a subset $ S'\subseteq S $ such that $ S' $ is a basis.
\end{proposition}
\pf If $ S $ is linearly independent then we're done. Otherwise write $ S=\{v_1,\dots, v_n\} $. Then there exists $ \lambda_1,\dots, \lambda_n $ such that $ \lambda_1v_1+\cdots\lambda_nv_n=\mathbf 0 $ wlog suppose that $ \lambda_n $ is nonzero. Then
\begin{align*}
	v_n=-\frac 1{\lambda_n}\sum_{i=1}^{n-1}\lambda_iv_i
\end{align*}
so $ v_n $ is in the span of the other vectors. Hence $ S\setminus \{v_n\} $ is still a spanning set. Repeat which the set is linearly independent, must terminate since the set is finite and the empty set is not a spanning set.\qed
\begin{corollary}
  Every finite-dimensional vector space has a finite basis.
\end{corollary}
\pf Trivial application of the proposition\qed
\begin{theorem}
	(Steinitz Exchange Lemma) Let $ S,T\subseteq V $ finite with $ S $ linearly independent and $ T $ a spanning set of $ V $. Then
	\begin{enumerate}
		\item $ |S|\le |T| $,
		\item and there exists $ T'\subseteq T $ which has size $ |T'|=|T|-|S| $ and $ S\cup T' $ spans $ V $.
	\end{enumerate}
\end{theorem}
\pf To come later...\par
Let's look at some consequences of the lemma first.
\begin{corollary}
	For a finite-dimensional vector space $ V $,
  \begin{enumerate}
	  \item Every basis for $ V $ is finite.
	  \item All finite basis have the same size.
  \end{enumerate}
\end{corollary}
\pf $ V $ has a finite basis $ B $, suppose we have some other basis $ B' $ infinite. Let $ B''\subseteq B' $ with $ |B''|=|B|+1 $ then $ |B''| $ is linearly independent, so applying (i) of the Steinitz exchange lemma with $ S=B'' $ and $ T=B $ we get a contradiction.\par
For the second part, let $ B_1,B_2 $ be finite basis for $ V $ then apply Steinitz symmetrically since both are spanning set and linearly independent, so we get that $ |B_1|\ge |B_2| $ and $ |B_1|\ge |B_2| $ so $ |B_1|=|B_2| $.\qed
\begin{definition}
	(Dimension) For a vector space $ V $ the \textit{dimension} of $ V $ is the size of any basis. We write this as \dim V.
\end{definition}
This definition is well-defined by the previous corollary.\par
For an example $ \dim \F^n=n $ since we've shown the standard basis has size $ n $. As a complex vector space $ \C $ is one-dimensional as a complex vector space and two-dimension as a real vector space, with basis $ \{1\} $ and $ \{1,i\} $ repectively.
\begin{corollary}
  For a vector space $ V $ let $ S,T\subseteq V $ finite, with $ S $ linearly independent and $ T $ a spanning set, then
  \begin{align*}
	  |S|\le \dim V\le |T|
	  \end{align*}
	  with equality if and only if $ S $ spans or $ V $ is linearly independent respectively.
\end{corollary}
\pf The inequalities are immediate from Steinitz. If $ S $ is a basis then $ |S|=\dim V $ from the previous corollary. Conversely if $ |S|=\dim V $ and let $ B $ be a basis for $ V $ so we have that $ |B|=|S| $ so $ B $ is a spanning set. So we can apply Steinitz (ii) to $ B $ so there exists $ B'\subseteq B $ with $ |B'|=|B|-|S|=0 $ and $ S\cup B'=S\cup \emptyset $ spans $ V $. So $ S $ is a basis. Similiar we have a very similiar proof for equality in $ V $.\qed\par We will not prove that every vector space has a basis, however some non-finitely dimensional vector spaces have an infinite basis, for example $ P(\R) $.
\begin{proposition}
  If $ V $ is a finite-dimensional vector space, then if $ U\le V $ then $ U $ is finite-dimensional, namely, $ \dim U \le \dim V $ with equality if and only if $ U=V $.
\end{proposition}
\pf If $ U=\{\mathbf 0\} $, we're done. Otherwise let $ \mathbf 0\ne u_1\in U $. Then $ \{u_1\}\subseteq U $ is linearly indepedent. Repeating, after repeating $ k $ times suppose we have $ \{u_1,\dots, u_k\} $ linearly indepedent with $ k\le \dim(V) $ by the previously corollary. If the set spans $ U $ we're done, if not we'll add another vector, $ u_{k+1} $ outside of the span of our space. If $ \{u_1,\dots, u_{k+1}\} $ is not linearly indepedent, we can write $ \mathbf 0 $ non-trivially, so
\[
	\sum_{i=1}^{k+1}\lambda_i u_i=\mathbf 0
\]
with $ \lambda_{k+1}\ne 0 $ since $ \{u_1,\dots, u_k\} $ linearly indepedent. Thus we have that
\[
	u_{k+1}=-\frac 1{\lambda_{k+1}}\left(\sum_{i=1}^k\lambda_iu_i\right)
\]
this process must terminate after at most $ \dim V $ many steps, by the previous corollary. If $ \dim U=\dim V $ apply the previous corollary with $ S $ being any basis for $ U $.\qed
\begin{proposition}
	(Extending a basis) Let $ U\le V $. For any basis $ B_U $ of $ U $ there exists a basis $ B_V $ of $ V $ such that $ B_U\subseteq B_V $.
\end{proposition}
\pf Apply the second result from Steinitz with $ S=B_U $ and $ T $ is any basis for $ V $. We obtain that $ T'\subseteq T\st $ 
\[
  |T'|=|T|-|S|=\dim V-\dim U
\]
and $ B_V=B_U\cup T' $ spans $ V $. But we have that
\[
  |B_V|\le |B_U|+|T'|=\dim V
\]
so by the previous corollary, $ B_V $ is a basis for $ V $.\qed\par
Now we'll finally prove the Steinitz exchange lemma.\par
\pf Let $ S=\{u_1,\dots, u_m\} $, $ T=\{v_1,\dots, v_n\} $ with $ |T|=m $ and $ |T|=n $. If $ S $ is empty then we're done. Otherwise there exists $ \lambda_i\in \F $ such that 
\[
	u_1=\sum_{i=1}^n\lambda_iv_i
\]
so by renumbering we can say that $ \lambda_1\ne 0 $. Then
\[
	v_1=\frac 1{\lambda_1}\left(u_1-\sum_{i=2}^n\lambda_iv_i\right)
\]
So $ \{u_1,v_2,\dots, v_n\} $ spans $ V $. After repeating $ k $ times with $ k<m $ suppose $ \{u_1,\dots, u_k,v_{k+1},\dots, v_n\} $ spans V, then there exists $ \lambda_i,\mu_j\in\F $ such that
\begin{align*}
	u_{k+1}=\sum_{j=1}^k\mu_ju_j+\sum_{i=k+1}^n\lambda_iv_i
\end{align*}
If for all $ \lambda_i= 0 $ then
\[
	\left(\sum_{j=1}^k\mu_ju_j\right) -u_{k+1}=\mathbf 0
\]
which is a contradiction since $ S $ is linearly independent. So by relabeling we have that $ \lambda_{k+1}\ne 0 $ such that 
\[
	v_{k+1}=\frac 1{\lambda_{k+1}}\left(u_{k+1}-\sum_{j=1}^k\mu_ju_j-\sum_{i=k+1}^n\lambda_iv_i\right)
\]
so $ (u_1,\dots, u_{k+1},v_{k+2},\dots, v_n\}$ spans $ V $. So we can conclude that $ m\ne n $ and $ \{u_1,\dots, u_m,v_{m+1},\dots, v_n\} $ spans $ V $ hence the set $ T'=\{v_{m+1},\dots, v_n\} $ exists as claimed. \qed
\begin{definition}
	(Nullity) For a linear map $ \alpha:V\to W $ we define the \textit{nullity} of $ \alpha $ as
	\[
	  \n(\alpha)=\dim\ker\alpha.
	\]
\end{definition}
\begin{definition}
	(Rank) For a linear map $ \alpha:V\to W $ we define the \textit{rank} of $ \alpha $ as
	\[
	  \rk(\alpha)=\dim\ima\alpha.
	\]
\end{definition}
\begin{theorem}
	(Rank-nullity theorem) If $ V $ is a finite dimensional $ \F $-vector space and $ W  $ is a $ \F $-vector space. Then if $ \alpha:V\to W $ is linear then $ \ima \alpha $ is finite dimensional and
	\[
	  \dim V=\n(\alpha)+\rk(\alpha).
	\]
\end{theorem}
\pf Recall the first isomorphism theorem so
\[
	\frac V{\ker \alpha}\cong \ima\alpha
\]
It is sufficient to prove the lemma
\begin{lemma}
  For $ U\le V $,
  \[
    \dim(V/U)=\dim V-\dim U
  \]
\end{lemma}
\pf Let $ B_U=\{u_1,\dots, u_m\} $ be a basis of $ U $. Extend to a basis $ B_V=\{u_1,\dots, u_m,v_{m+1},\dots, v_n\} $ of $ V $ where $ m=\dim U $ and $ n=\dim V $.\\
Set $ B_{V/U}=\{v_i+U:m+1\le i\le n $. The we claim that $ B_{V/U} $ is a basis for $ V/U $ of size $ n-m $. To show spanning, for $ v\in V $ write
	\[
	  v=\sum_i\lambda_iv_i+\sum_j\mu_jv_j
	\]
	Then $ v+U=\sum_i\lambda_i(v_i+U)\in \langle B_{V/U}\rangle $. For linear independence, suppose
	\[
	  \sum_i\lambda_i(v_i+U)=\mathbf 0+U
  \]
	hence 
	\begin{align*}
	  =\left(\sum_i\lambda_iv_i\right)+U \\
	  \sum_i\lambda_iv_i\in U\\
	  \sum_i\lambda_iv_i=\sum_j\mu_ju_j
	\end{align*}
	since $ B_V $ is linearly independent, we have that all $ \lambda_i $ and $ \mu_j $ are zero. Similiarly if $ v_i+U=v_j+U $ with $ i\ne j $ then we can write $ v_i-v_j=\sum_j\mu_ju_j $ which is a contradiction.\qed
	\begin{remark}
	   We can maek a direct proof without quotient spaces by rearranging some of the arguments of the proof.
	\end{remark}
\begin{corollary}
	(Linear Pigeonhole principle) If $ \dim V=\dim W=n $ and $ \alpha: V\to W $ then the following conditions are equivalent.
	\begin{enumerate}
		\item $ \alpha $ is injective,
		\item $ \alpha $ is surjective,
		\item $ \alpha $ is an isomorphism.
	\end{enumerate}
\end{corollary}
\pf If $ \alpha $ injective then $ \n(\alpha)=0 $ so by rank nullity we have that $ \rk(\alpha)=n $ so $ \alpha $ is surjective. If $ \alpha $ is surjective then $ \rk(\alpha)=n $ so by rank nullity, the dimension of the kernal is $ 0 $ hence the kernal is trivial, so $ \alpha $ injective, hence $ \alpha $ is an isomorphism. If $ \alpha $ is an isomorphism, clearly it's injective, so all equivalent.\qed
\begin{proposition}
  Suppose $ V $ is a vector space with a basis $ B $. For any vector space $ W	 $ and any function $ f:B\to W $ there is a unique linear map $ F:V\to W $ such that $ F(B)=W $.
\end{proposition}
\pf First we'll show existance. For $ v\in V $ write $ v=\sum_b\lambda_bb $ for a finite sum. Then define
\[
  F(v)=\sum_b \lambda_bf(b).
\]
This is well-defined, since $ B $ is a basis the $ \lambda_b $ are uniquely determined by $ v $. For $ u,v\in V $ and $ \lambda\in \F $ we write
\[
  u=\sum_b\mu_bb,\quad \sum_b\lambda_bb.
\]
Then \begin{align*}
	F(u+\lambdav)&=F(\sum_b(\mu_b+\lambda\lambda_b)f(b)\\
		     &= \sum_b\mu_bf(b)+\lambda\sum_b\lambda_bf(b)\\
		     &= F(u)+\lambda F(v).
\end{align*}
So $ F $ is linear. To show uniqueness $ \overline F:V\to W $ is another linear map extending $ f $ then,
\[
  \overline F\left(\sum_b\lambda bb\right)=\sum_b\lambda_b\overline F(b)
\]
which is the same as our definition for $ F $ hence they are the same function.
\begin{corollary}
	For a vector space, $ V $, with $ \dim V=n $ with a basis $ B=\{v_1,\dots, v_n\} $ for $ V $ then there is a isomorphism
  \begin{align*}
	  F_B:V&\to \F^n\\
	       \sum_{i=1}^n\lambda_iv_i &\to \begin{pmatrix}
	        \lambda_1\\
		\vdots \\
		\lambda_n
	      \end{pmatrix}
  \end{align*}
  \end{corollary}



































\end{document}
