\documentclass{article}
\usepackage{../header}
\title{Linear Algebra}
\author{Notes by Finley Cooper}
\newcommand{\F}{\mathbb{F}}
\begin{document}
  \maketitle
  \newpage
  \tableofcontents
  \newpage
  \section{Vector Spaces}
  \subsection{Definitions}
  For this lecture course, $ \F $ will always be field.
  \begin{definition}
	  (Vector Space) A $ \F $\textit{-vector space} (or a vector space over $ \F $) is an abelian group $ (V,+,\boldsymbol 0) $ equipped with a function
	  \begin{align*}
	    \F\times V\to V \\
	    (\lambda, v)\to \lamda v
	  \end{align*}
	  which we call scalar multiplication such that $ \forall v,w\in V, \forall \lambda,\mu\in\F $
	  \begin{enumerate} 
		  \item $ (\lambda + \mu)v=\lambda v + \mu v $
		  \item $ \lambda(v + w)=\lambda v + \lambda w $
		  \item $ \lambda(\mu v)=(\lambda \mu)v $
		  \item $ 1\cdot v = v\cdot 1 = v $
	  \end{enumerate}
  \end{definition}
  Remember that $ \boldsymbol 0 $ and $ 0 $ are not the same thing. $ 0 $ is an element in the field $ \F $ and $ \boldsymbol 0 $ is the additive identity in $ V $.\par
  For an example consider $ \F^n $ n-dimensional column vectors with entries in $ \F $. We also have the example of a vector space $ \C^n $ which is a complex vector space, but also a real vector space (taking either $ \C $ or $ \R $ as the underlying scalar field).\par
  We also can see that $ M_{m\times n}(\F) $ form a vector space with $ m $ rows and $ n $ columns.\\
  For any non-empty set $ X $, we denote $ \F^X $ as the space of functions from $ X $ to $ \F $ equipped with operations such that:
\begin{align*}
	f+g \text{ is given by } (f+g)(x)=f(x)+g(x)\\
	\lambda f \text{ is given by } (\lambda f)(x)=\lambda f(x)
\end{align*}
\begin{proposition}
  For all $ v\in V $ we have that $ 0\cdot v = \boldsymbol 0 $ and $ (-1)\cdot v=-v $ where $ -v $ denotes the additive inverse of $ v $.
\end{proposition}
\pf Trivial.
\begin{definition}
	(Subspace) A \textit{subspace} of a $ \F $-vector space $ V $ is a subset $ U\subseteq V $ which is a $ \F $-vector space itself under the same operations as $ V $. Equivalently, $ (U,+) $ is a subgroup of $ (V,+) $ and $ \forall \lambda\in \F $, $\forall u\in U $ we have that $ \lambda u \in U $.
\end{definition}
\begin{remark}
  Axioms (i)-(iv) are always automatically inherited into all subspaces.
\end{remark}
\begin{proposition}
	(Subspace test) Let $ V $ be a $ \F $-vector space and $ U\subseteq V $ then $ U $ is a subspace of $ V $ if and only if,
	\begin{enumerate}
		\item $ U $ is nonempty.
		\item $ \forall \lambda\in\F $ and $ \forall u,w\in U $ we have that $ u+\lambda w \in U $.
	\end{enumerate}
\end{proposition}
\pf If $ U $ is a subspace then $ U $ satisfies (i) and (ii) since it contains $ 0 $ and is closed. Conversely suppose that $ U\subseteq V $ satisfies (i) and (ii). Taking $ \lambda = -1 $ so $ \forall u,w\in V $, $ u-w\in U $ hence $ (U,+) $ is a subgroup of $ (V,+) $ by the subgroup test. Finally taking $ u=\boldsymbol 0 $ so we have that $ \forall w\in U,\forall\lambda\in \F $ we have that $ \lambda w\in U $. So $ U $ is a subspace of $ V $.\qed\par
We notate $ U $ by $ U\le V $.\par
For some examples
\begin{enumerate}
	\item \[
	\left\{\begin{pmatrix}
			x \\
			y \\
			z \\
	\end{pmatrix}\in \R^3:x+y+z=t\right\}\subseteq \R^3, 
\]
for fixed $ t\in \R $ is a subspace of $ \R^3 $ iff $ t = 0 $.\par
\item Take $ \R^\R $ as all the functions from $ \R $ to $ \R $ then the set of continuous functions is a subspace.
\item Also we have that $ C^\infty(\R) $, the set of infintely differentiable functions from $ \R $ to $ \R $ is a subspace of $ \R^\R $ and the subspace of continuous functions.
\item A further subspace of all of those subspaces is the set of polynomial functions.
\end{enumerate}
\begin{lemma}
  For $ U,W\le V $ we have that $ U\cap W\le V $.
\end{lemma}
\pf We'll use the subspace test. Both $ U,W $ are subspaces so they contain $ \boldsymbol 0 $ hence $ \boldsymbol 0\in U\cap W $ so $ U\cap W $ is nonempty. Secondly take $ x,y\in U\cap W $ with $ \lambda \in \F $. Then $ U\le V $ and $ x,y\in U $ so $ x+\lambda y\in U $. Similarly with $ W $ so $ x+\lambda y \in W $ hence we have that $ x+\lambda y \in U \cap W $ hence $ U\cap W\le V $\qed
\begin{remark}
  This does not apply for subspaces, in fact from IA Groups, we know it doesn't even hold for the underlying abelian group.
\end{remark}
\begin{definition}
	(Subspace sum) For $ U,W\le V $, the \textit{subspace sum} of $ U, W $ is
	\[
		U+W=\{u+w:u\in U, w\in W\}.
	\]
\end{definition}
\begin{lemma}
  If $ U, W\le V $ then $ U+W\le V $.
\end{lemma}
\pf Simple application of the subspace test.
\begin{remark}
  $ U+W $ is the smallest subgroup of $ U, W $ in terms of inclusion, i.e. if $ K $ is such that $ U\subseteq K $ and $ W\subseteq K $ then $ U+W\subseteq K $.
\end{remark}
\subsection{Linear maps, isomorphisms, and quotients}
\begin{definition}
	(Linear map) For $ V $, $ W $ $ \F $-vector spaces. A \textit{linear map} from $ V $ to $ W $ is a group homomorphism, $ \varphi $, from $ (V,+) $ to $ (W,+) $ such that $ \forall v\in V $
	\[
	  \varphi(\lambda v) = \lambda\varphi(v)
	\]
\end{definition}
Equivalently to show any function $\alpha: V\to W $ is a linear map we just need to show that $ \forall u,w\in V $, $ \forall \lambda \in\F $ we have
\[
  \alpha(u+\lambda w)=\alpha(u)+\lambda\alpha(w).
\]
For some examples of linear maps
\begin{enumerate}
	\item $ V=\F^n, W=\F^m $ $ A\in M_{m\times n}(\F) $. Then let $ \alpha:V\to W $ be given by $ \alpha(v)=Av $. Then $ \alpha $ is linear.
	\item $ \alpha:C^\infty(\R)\to C^\infty(\R) $ defined by taking the derivative.
	\item $ \alpha: C(\R)\to \R $ defined by taking the integral from $ 0 $ to $ 1 $.
	\item $ X $ any nonempty  set, $ x_0\in X $,
		\begin{align*}
		  \alpha:\F^X\to \F \\
		  f\to f(x_0)
		\end{align*}
	\item For any $ V,W $ the identity mapping from $ V $ to $ V $ is linear and so is the zero map from $ V $ to $ W $.
	\item The composition of two linear maps is linear.
	\item For a non-example squaring in $ \R $ is not linear. Similiarly adding constants is not linear, since linear maps preserve the zero vector.
\end{enumerate}
\begin{definition}
	(Isomorphism) A linear map $ \alpha:V\to W $ is an \textit{isomorphism} if it is bijective.\par
	We say that $ V $ and $ W $ are isomorphic, if there exists an isomorphism from $ V\to W $ and denote this by $ V\cong W $.
\end{definition}
An example is the vector space $ V=\F^4 $ and $ W=M_{2\times 2}(\F) $ we can define the map
\begin{align*}
	\alpha:V &\to W\\
	        \begin{pmatrix}
	         a\\b\\c\\d
	       \end{pmatrix}
		 &\to \begin{pmatrix}
		       a & b \\
		       c & d 
	       \end{pmatrix}
\end{align*}
Then $ \alpha $ is an isomorphism.
\begin{proposition}
  If $ \alpha: V\to W $ is an isomorphism then $ \inv\alpha:W\to V $ is also an isomorphism.
\end{proposition}
\pf Clearly $ \inv\alpha $ is a bijection. We need to prove that $ \inv\alpha $ is linear. Take $ w_1,w_2\in W $ and $ \lambda\in \F $. So we can write $ w_i=\alpha(v_i) $ for $ i=1,2 $. Then \[ \inv\alpha(w_1+\lambda w_2)=\inv \alpha(\alpha(v_1)+\lambda\alpha(v_2))=\inv\alpha(\alpha(v_1+\lambda v_2))=v_1+\lambda v_2=\inv \alpha(w_1)+\lambda\inv \alpha(w_2) \]. Hence $ \inv\alpha $ is linear, so $ \inv\alpha $ is an isomorphism.\qed 
\begin{definition}
	(Kernal) Let $ V,W $ be $ \F $-vector spaces. Then the \textit{kernal} of the linear map $ \alpha:V\to W $ is
	\[
		\ker(\alpha)=\{v\in V:\alpha(v)=\mathbf 0_W\}\subseteq V
	\]
\end{definition}
\begin{definition}
	(Image) Let $ V,W $ be $ \F $-vector spaces. Then the \textit{image} of a linear map $ \alpha:V\to W $ is
	\[
		\ima(\alpha)=\{\alpha(v):v\in V\}\subseteq W
	\]
\end{definition}
\begin{lemma}
  For a linear map $ \alpha:V\to W $ the following hold.
  \begin{enumerate}
	  \item $ \ker\alpha\le V $ and $ \ima \alpha \le W $
	  \item $ \alpha $ is surjective if and only if $ \ima \alpha =W $
	  \item $ \alpha $ is injective if and only if $ \ker\alpha=\{\mathbf 0_V\} $
  \end{enumerate}
\end{lemma}
\pf $ \mathbf 0_V+\mathbf 0_V=\mathbf 0_V $, so applying $ \alpha $ to both sides any using the fact that $ \alpha $ is linear gives that $ \alpha(\mathbf 0_V)=\mathbf 0_W $. So $ \ker\alpha $ is nonempty. The rest of the proof is a simple application of the subspace test.\\
The second statement is immediate from the definition.\\
For the final statement suppose $ \alpha $ injective. Suppose $ v\in \ker \alpha $. Then $ \alpha(v)=\mathbf 0_W=\alpha(\mathbf 0_w) $ so $ v=\mathbf 0_V $ by injectivity. Hence $ \ker\alpha $ is trivial.
Conversely suppose that $ \ker \alpha=\{0_V\} $ Let $ u,v\in V $ and suppose that $ \alpha(u)=\alpha(v) $. The $ \alpha(u-v)=\mathbf 0_W $, so $ u-v\in\ker\alpha $, so $ u=v $.\qed\par

For $ V $ a $ \F $-vector space, $ W\le V $ write
	\[
		\frac VW=\{v+W:v\in V\}
	\]
	as the left cosets of $ W $ in $ V $. Recall that two cosets $ v + V $ and $ u+W $ are the same coset if and only if $ v-u\in W $.
\begin{proposition}
  $ V/W $ is an $ \F $-vector space under operations
  \begin{align*}
   (u+W)+(v+W)&=(u+v)+W \\
   \lambda (v+W)&=(\lambda v)+W
\end{align*}
We call $ V/W $ the quotient space of $ V $ by $ W $.
\end{proposition}
\pf The proof is long and requires a lot of vector space axioms so we'll just sketch out the proof.\\
We check that operations are well-defined, so for $ u,\overline u, v,\overline v \in V $ and $ \lambda\in \F $ if
\[
  u+W=\overline u + W,\quad v+W=\overline v + W
\]
then
\[
	(u+v)+W=(\overline u+\overline w)+W
\]
and
\[
	(\lambda u) + W = (\lambda\overline u)+W
\]
The vector space axioms are inherited from $ V $.\qed
\begin{proposition}
	(Quotient map) The function $ \pi_W: V\to \frac VW $ called a \textit{quotient map} is given by
  \[
    \pi_W(v)=v+W
  \]
  is a well-defined, surjective, linear map with $ \ker\pi_W=W $.
\end{proposition}
\pf Surjectivity is clear. For linearity let $ u,v \in V $ and $ \lambda\in \F $. Then 
\begin{align*}
	\pi_W(u+\lambda v)&=(u+\lambda v) + W \\
			  &= (u+W)+(\lambda v+W)\\
			  &= (u+W)+\lambda(v+W) \\
			  &= \pi_W(u)+\lambda\pi_W(v)
\end{align*}
For $ v\in V $, we have that  $ v\in\ker \pi_W \iff \pi_W(v) = \mathbf 0_{V/W} $. So $ v+W=\mathbf 0_V+W $ so finally $ v=v-\mathbf 0_V\in W $.\qed
\begin{theorem}
	(First isomorphism theorem) Let $ V,W $ be $ \F $-vector spaces and $ \alpha:V\to W  $ linear. Then there is an isomorphism
	\[
		\overline\alpha:\frac{V}{\ker \alpha}\to\ima\alpha
	\]
	given by $ \overline\alpha(v+\ker\alpha)=\alpha(v) $
\end{theorem}
\pf For $ u,v\in V $,
\begin{align*}
	u+K=v=K &\iff u-v\in K
		&\iff \alpha(u-v)=\mathbf 0_W
		&\iff \alpha(u)=\alpha(v)
		&\iff \overline \alpha(u+\ker\alpha)=\overline\alpha(v+\ker\alpha)
\end{align*}
The forward direction shows that $ \overline\alpha $ is well-defined, and the converse shows that $ \overline\alpha $ is injective. For surjectivity given $ w\in\ima\alpha $, there exists some $ v\in V \st w=\alpha(v)$. Then $ w=\overline\alpha(v+\ker\alpha) $.\\
Finally for linearity given $ u,v\in V $, $ \lambda\in\F $,
\begin{align*}
	\overline\alpha((u+\ker\alpha)+\lambda(v+\ker\alpha))&=\overline\alpha((u+\lambda v)+\ker\alpha)\\
  &= \alpha(u+\lambda v)\\
  &= \alpha(u)+\lambda\alpha(v) \\
  &= \overline\alpha(u+\ker\alpha)+\lambda\overline\alpha(v+\ker\alpha)
\end{align*}
So $ \overline \alpha$ is linear hence is an isomorphism\qed
\subsection{Basis}
\begin{definition}
	(Span) Let $ V $ be a $ \F $-vector space. Then the \textit{span} of some subset $ S\subseteq V $ is
	\[
		\langle S\rangle = \left\{\sum_{s\in S}\lambda_s\cdot s: \lambda_s\in \F\right\}
	\]
	where $ \sum $ denotes finite sums. An expression the form above is called a \textit{linear combination} of $ S $.\\
	We say that $ S $ \textit{spans} $ V $ if $ \langle S\rangle =V $
\end{definition}
\begin{definition}
	(Finite-dimensional) For a vector space $ V $ we say that it is \textit{finite-dimensional} if there exists a finite spanning set.
\end{definition}
We'll give some simple remarks without proof.
  \begin{enumerate}
	  \item $ \langle S \rangle\le V $ and conversely if $ W\le V $ and $ S\subseteq W $ then $ \langle S\rangle \le W $.
	  \item If $ S,T\subseteq W $ and $ S $ spans $ V $ and $ S\subseteq \langle V\rangle $ then $ T $ spans $ V $.
	  \item By convention $ \langle\emptyset\rangle =\{\mathbf 0_V\} $.
	  \item $ \langle S\cup T\rangle = \langle S\rangle + \langle T\rangle $
  \end{enumerate}
For an example consider $ V=\R^3 $ and consider the sets
\begin{align*}
  S=\left\{\begin{pmatrix}
    1\\0\\0
  \end{pmatrix},\begin{pmatrix}
    1\\1\\2
\end{pmatrix}\right\}\\
T=\left\{\begin{pmatrix}
  2\\1\\2
\end{pmatrix},\begin{pmatrix}
  0\\1\\2
\end{pmatrix},\begin{pmatrix}
  -1\\2\\4
\end{pmatrix}\right\}
\end{align*}
Then $ \langle S\rangle = \langle T\rangle =\left\{\begin{pmatrix}
  x\\y\\2y
\end{pmatrix}:x,y\in \R\right\} \le \R^3.$\par
For a second example consider $ V=\R^\N $ and set $ T=\{\delta_n:n\in \N\} $. This is not a spanning set, since we require infinitely many elements from $ T $ to make an element in $ V $. In fact we can write that
\[
	\langle T\rangle =\{f\in \R^\N:f(n)=0\text{ for all but finitely many terms}\}.
\]
\begin{definition}
	(Linear Independence) A subset $ S\subseteq V $ is called \textit{linearly independent} if, for all finite linear combinations
\[
	\sum_{s\in S}\lambda_ss\quad \text{of S}
\]
if the sum is the zero vector in $ V $ the $ \lambda_s=0 $ for all $ s\in S $.
\end{definition}
If $ S $ is not linearly indepedent we say that $ S $ is linearly dependent.\par
We'll make some more remarks
\begin{enumerate}
	\item If $ \mathbf 0 \in S $ then $ S $ is not linearly independent.
	\item If we have a finite set, then to show linearly independent, we only need to consider the linear combination of all elements, not all finite lienar combinations.
	\item However is $ S $ is infinite, then we have to consider every possible finite subset of $ S $ and show it's linearly independent.
	\item Every subset of a linearly independent set is itself linearly indepedent.	
\end{enumerate}
\begin{definition}
	(Basis) A subset $ S\subseteq V $ is a \textit{basis} for $ V $ if $ S $ is linearly independent and a spanning set.
\end{definition}
For an example consider $ e_i\in \F^n $ be given by
\[
  e_i=\begin{pmatrix}
    0\\ \vdots \\ 0 \\ 1 \\ 0 \\\vdots \\ 0
\end{pmatrix}\quad \text{with the 1 in the } i\text{th entry}
\]
then the set $ \{e_i:1\le i\le n\} $ is the standard basis for $ \F^n $.\par
For $ P(\R) $ the set of real polynomial functions and let $ p_n\in P(\R) $ be given by $ p_n(x)=x^n $, then $ \{p_n:n\in\Z_{\ge 0}\} $ is a basis for $ P(\R) $.
\begin{proposition}
  If $ S\subseteq V $ is a finite spanning set, then there exists a subset $ S'\subseteq S $ such that $ S' $ is a basis.
\end{proposition}
\pf If $ S $ is linearly independent then we're done. Otherwise write $ S=\{v_1,\dots, v_n\} $. Then there exists $ \lambda_1,\dots, \lambda_n $ such that $ \lambda_1v_1+\cdots\lambda_nv_n=\mathbf 0 $ wlog suppose that $ \lambda_n $ is nonzero. Then
\begin{align*}
	v_n=-\frac 1{\lambda_n}\sum_{i=1}^{n-1}\lambda_iv_i
\end{align*}
so $ v_n $ is in the span of the other vectors. Hence $ S\setminus \{v_n\} $ is still a spanning set. Repeat which the set is linearly independent, must terminate since the set is finite and the empty set is not a spanning set.\qed
\begin{corollary}
  Every finite-dimensional vector space has a finite basis.
\end{corollary}
\pf Trivial application of the proposition\qed
\begin{theorem}
	(Steinitz Exchange Lemma) Let $ S,T\subseteq V $ finite with $ S $ linearly independent and $ T $ a spanning set of $ V $. Then
	\begin{enumerate}
		\item $ |S|\le |T| $,
		\item and there exists $ T'\subseteq T $ which has size $ |T'|=|T|-|S| $ and $ S\cup T' $ spans $ V $.
	\end{enumerate}
\end{theorem}
\pf To come later...\par
Let's look at some consequences of the lemma first.
\begin{corollary}
	For a finite-dimensional vector space $ V $,
  \begin{enumerate}
	  \item Every basis for $ V $ is finite.
	  \item All finite basis have the same size.
  \end{enumerate}
\end{corollary}
\pf $ V $ has a finite basis $ B $, suppose we have some other basis $ B' $ infinite. Let $ B''\subseteq B' $ with $ |B''|=|B|+1 $ then $ |B''| $ is linearly independent, so applying (i) of the Steinitz exchange lemma with $ S=B'' $ and $ T=B $ we get a contradiction.\par
For the second part, let $ B_1,B_2 $ be finite basis for $ V $ then apply Steinitz symmetrically since both are spanning set and linearly independent, so we get that $ |B_1|\ge |B_2| $ and $ |B_1|\ge |B_2| $ so $ |B_1|=|B_2| $.\qed
\begin{definition}
	(Dimension) For a vector space $ V $ the \textit{dimension} of $ V $ is the size of any basis. We write this as \dim V.
\end{definition}
This definition is well-defined by the previous corollary.\par
For an example $ \dim \F^n=n $ since we've shown the standard basis has size $ n $. As a complex vector space $ \C $ is one-dimensional as a complex vector space and two-dimension as a real vector space, with basis $ \{1\} $ and $ \{1,i\} $ repectively.
\begin{corollary}
  For a vector space $ V $ let $ S,T\subseteq V $ finite, with $ S $ linearly independent and $ T $ a spanning set, then
  \begin{align*}
	  |S|\le \dim V\le |T|
	  \end{align*}
	  with equality if and only if $ S $ spans or $ V $ is linearly independent respectively.
\end{corollary}
\pf The inequalities are immediate from Steinitz. If $ S $ is a basis then $ |S|=\dim V $ from the previous corollary. Conversely if $ |S|=\dim V $ and let $ B $ be a basis for $ V $ so we have that $ |B|=|S| $ so $ B $ is a spanning set. So we can apply Steinitz (ii) to $ B $ so there exists $ B'\subseteq B $ with $ |B'|=|B|-|S|=0 $ and $ S\cup B'=S\cup \emptyset $ spans $ V $. So $ S $ is a basis. Similiar we have a very similiar proof for equality in $ V $.\par We will not prove that every vector space has a basis, however some non-finitely dimensional vector spaces have an infinite basis, for example $ P(\R) $.










































\end{document}
