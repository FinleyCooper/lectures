\documentclass{article}
\usepackage{../header}
\title{Linear Algebra}
\author{Notes by Finley Cooper}
\newcommand{\F}{\mathbb{F}}
\newcommand{\n}{\mathrm{n}}
\newcommand{\rk}{\mathrm{rk}}
\newcommand{\Col}{\mathrm{Col}}
\newcommand{\Row}{\mathrm{Row}}
\newcommand{\sgn}{\mathrm{sgn}}
\begin{document}
  \maketitle
  \newpage
  \tableofcontents
  \newpage
  \section{Vector Spaces}
  \subsection{Definitions}
  For this lecture course, $ \F $ will always be field.
  \begin{definition}
	  (Vector Space) A $ \F $\textit{-vector space} (or a vector space over $ \F $) is an abelian group $ (V,+,\boldsymbol 0) $ equipped with a function
	  \begin{align*}
	    \F\times V\to V \\
	    (\lambda, v)\to \lamda v
	  \end{align*}
	  which we call scalar multiplication such that $ \forall v,w\in V, \forall \lambda,\mu\in\F $
	  \begin{enumerate} 
		  \item $ (\lambda + \mu)v=\lambda v + \mu v $
		  \item $ \lambda(v + w)=\lambda v + \lambda w $
		  \item $ \lambda(\mu v)=(\lambda \mu)v $
		  \item $ 1\cdot v = v\cdot 1 = v $
	  \end{enumerate}
  \end{definition}
  Remember that $ \boldsymbol 0 $ and $ 0 $ are not the same thing. $ 0 $ is an element in the field $ \F $ and $ \boldsymbol 0 $ is the additive identity in $ V $.\par
  For an example consider $ \F^n $ n-dimensional column vectors with entries in $ \F $. We also have the example of a vector space $ \C^n $ which is a complex vector space, but also a real vector space (taking either $ \C $ or $ \R $ as the underlying scalar field).\par
  We also can see that $ M_{m\times n}(\F) $ form a vector space with $ m $ rows and $ n $ columns.\\
  For any non-empty set $ X $, we denote $ \F^X $ as the space of functions from $ X $ to $ \F $ equipped with operations such that:
\begin{align*}
	f+g \text{ is given by } (f+g)(x)=f(x)+g(x)\\
	\lambda f \text{ is given by } (\lambda f)(x)=\lambda f(x)
\end{align*}
\begin{proposition}
  For all $ v\in V $ we have that $ 0\cdot v = \boldsymbol 0 $ and $ (-1)\cdot v=-v $ where $ -v $ denotes the additive inverse of $ v $.
\end{proposition}
\pf Trivial.
\begin{definition}
	(Subspace) A \textit{subspace} of a $ \F $-vector space $ V $ is a subset $ U\subseteq V $ which is a $ \F $-vector space itself under the same operations as $ V $. Equivalently, $ (U,+) $ is a subgroup of $ (V,+) $ and $ \forall \lambda\in \F $, $\forall u\in U $ we have that $ \lambda u \in U $.
\end{definition}
\begin{remark}
  Axioms (i)-(iv) are always automatically inherited into all subspaces.
\end{remark}
\begin{proposition}
	(Subspace test) Let $ V $ be a $ \F $-vector space and $ U\subseteq V $ then $ U $ is a subspace of $ V $ if and only if,
	\begin{enumerate}
		\item $ U $ is nonempty.
		\item $ \forall \lambda\in\F $ and $ \forall u,w\in U $ we have that $ u+\lambda w \in U $.
	\end{enumerate}
\end{proposition}
\pf If $ U $ is a subspace then $ U $ satisfies (i) and (ii) since it contains $ 0 $ and is closed. Conversely suppose that $ U\subseteq V $ satisfies (i) and (ii). Taking $ \lambda = -1 $ so $ \forall u,w\in V $, $ u-w\in U $ hence $ (U,+) $ is a subgroup of $ (V,+) $ by the subgroup test. Finally taking $ u=\boldsymbol 0 $ so we have that $ \forall w\in U,\forall\lambda\in \F $ we have that $ \lambda w\in U $. So $ U $ is a subspace of $ V $.\qed\par
We notate $ U $ by $ U\le V $.\par
For some examples
\begin{enumerate}
	\item \[
	\left\{\begin{pmatrix}
			x \\
			y \\
			z \\
	\end{pmatrix}\in \R^3:x+y+z=t\right\}\subseteq \R^3, 
\]
for fixed $ t\in \R $ is a subspace of $ \R^3 $ iff $ t = 0 $.\par
\item Take $ \R^\R $ as all the functions from $ \R $ to $ \R $ then the set of continuous functions is a subspace.
\item Also we have that $ C^\infty(\R) $, the set of infintely differentiable functions from $ \R $ to $ \R $ is a subspace of $ \R^\R $ and the subspace of continuous functions.
\item A further subspace of all of those subspaces is the set of polynomial functions.
\end{enumerate}
\begin{lemma}
  For $ U,W\le V $ we have that $ U\cap W\le V $.
\end{lemma}
\pf We'll use the subspace test. Both $ U,W $ are subspaces so they contain $ \boldsymbol 0 $ hence $ \boldsymbol 0\in U\cap W $ so $ U\cap W $ is nonempty. Secondly take $ x,y\in U\cap W $ with $ \lambda \in \F $. Then $ U\le V $ and $ x,y\in U $ so $ x+\lambda y\in U $. Similarly with $ W $ so $ x+\lambda y \in W $ hence we have that $ x+\lambda y \in U \cap W $ hence $ U\cap W\le V $\qed
\begin{remark}
  This does not apply for subspaces, in fact from IA Groups, we know it doesn't even hold for the underlying abelian group.
\end{remark}
\begin{definition}
	(Subspace sum) For $ U,W\le V $, the \textit{subspace sum} of $ U, W $ is
	\[
		U+W=\{u+w:u\in U, w\in W\}.
	\]
\end{definition}
\begin{lemma}
  If $ U, W\le V $ then $ U+W\le V $.
\end{lemma}
\pf Simple application of the subspace test.
\begin{remark}
  $ U+W $ is the smallest subgroup of $ U, W $ in terms of inclusion, i.e. if $ K $ is such that $ U\subseteq K $ and $ W\subseteq K $ then $ U+W\subseteq K $.
\end{remark}
\subsection{Linear maps, isomorphisms, and quotients}
\begin{definition}
	(Linear map) For $ V $, $ W $ $ \F $-vector spaces. A \textit{linear map} from $ V $ to $ W $ is a group homomorphism, $ \varphi $, from $ (V,+) $ to $ (W,+) $ such that $ \forall v\in V $
	\[
	  \varphi(\lambda v) = \lambda\varphi(v)
	\]
\end{definition}
Equivalently to show any function $\alpha: V\to W $ is a linear map we just need to show that $ \forall u,w\in V $, $ \forall \lambda \in\F $ we have
\[
  \alpha(u+\lambda w)=\alpha(u)+\lambda\alpha(w).
\]
For some examples of linear maps
\begin{enumerate}
	\item $ V=\F^n, W=\F^m $ $ A\in M_{m\times n}(\F) $. Then let $ \alpha:V\to W $ be given by $ \alpha(v)=Av $. Then $ \alpha $ is linear.
	\item $ \alpha:C^\infty(\R)\to C^\infty(\R) $ defined by taking the derivative.
	\item $ \alpha: C(\R)\to \R $ defined by taking the integral from $ 0 $ to $ 1 $.
	\item $ X $ any nonempty  set, $ x_0\in X $,
		\begin{align*}
		  \alpha:\F^X\to \F \\
		  f\to f(x_0)
		\end{align*}
	\item For any $ V,W $ the identity mapping from $ V $ to $ V $ is linear and so is the zero map from $ V $ to $ W $.
	\item The composition of two linear maps is linear.
	\item For a non-example squaring in $ \R $ is not linear. Similiarly adding constants is not linear, since linear maps preserve the zero vector.
\end{enumerate}
\begin{definition}
	(Isomorphism) A linear map $ \alpha:V\to W $ is an \textit{isomorphism} if it is bijective.\par
	We say that $ V $ and $ W $ are isomorphic, if there exists an isomorphism from $ V\to W $ and denote this by $ V\cong W $.
\end{definition}
An example is the vector space $ V=\F^4 $ and $ W=M_{2\times 2}(\F) $ we can define the map
\begin{align*}
	\alpha:V &\to W\\
	        \begin{pmatrix}
	         a\\b\\c\\d
	       \end{pmatrix}
		 &\to \begin{pmatrix}
		       a & b \\
		       c & d 
	       \end{pmatrix}
\end{align*}
Then $ \alpha $ is an isomorphism.
\begin{proposition}
  If $ \alpha: V\to W $ is an isomorphism then $ \inv\alpha:W\to V $ is also an isomorphism.
\end{proposition}
\pf Clearly $ \inv\alpha $ is a bijection. We need to prove that $ \inv\alpha $ is linear. Take $ w_1,w_2\in W $ and $ \lambda\in \F $. So we can write $ w_i=\alpha(v_i) $ for $ i=1,2 $. Then \[ \inv\alpha(w_1+\lambda w_2)=\inv \alpha(\alpha(v_1)+\lambda\alpha(v_2))=\inv\alpha(\alpha(v_1+\lambda v_2))=v_1+\lambda v_2=\inv \alpha(w_1)+\lambda\inv \alpha(w_2) \]. Hence $ \inv\alpha $ is linear, so $ \inv\alpha $ is an isomorphism.\qed 
\begin{definition}
	(Kernal) Let $ V,W $ be $ \F $-vector spaces. Then the \textit{kernel} of the linear map $ \alpha:V\to W $ is
	\[
		\ker(\alpha)=\{v\in V:\alpha(v)=\mathbf 0_W\}\subseteq V
	\]
\end{definition}
\begin{definition}
	(Image) Let $ V,W $ be $ \F $-vector spaces. Then the \textit{image} of a linear map $ \alpha:V\to W $ is
	\[
		\ima(\alpha)=\{\alpha(v):v\in V\}\subseteq W
	\]
\end{definition}
\begin{lemma}
  For a linear map $ \alpha:V\to W $ the following hold.
  \begin{enumerate}
	  \item $ \ker\alpha\le V $ and $ \ima \alpha \le W $
	  \item $ \alpha $ is surjective if and only if $ \ima \alpha =W $
	  \item $ \alpha $ is injective if and only if $ \ker\alpha=\{\mathbf 0_V\} $
  \end{enumerate}
\end{lemma}
\pf $ \mathbf 0_V+\mathbf 0_V=\mathbf 0_V $, so applying $ \alpha $ to both sides any using the fact that $ \alpha $ is linear gives that $ \alpha(\mathbf 0_V)=\mathbf 0_W $. So $ \ker\alpha $ is nonempty. The rest of the proof is a simple application of the subspace test.\\
The second statement is immediate from the definition.\\
For the final statement suppose $ \alpha $ injective. Suppose $ v\in \ker \alpha $. Then $ \alpha(v)=\mathbf 0_W=\alpha(\mathbf 0_w) $ so $ v=\mathbf 0_V $ by injectivity. Hence $ \ker\alpha $ is trivial.
Conversely suppose that $ \ker \alpha=\{0_V\} $ Let $ u,v\in V $ and suppose that $ \alpha(u)=\alpha(v) $. The $ \alpha(u-v)=\mathbf 0_W $, so $ u-v\in\ker\alpha $, so $ u=v $.\qed\par

For $ V $ a $ \F $-vector space, $ W\le V $ write
	\[
		\frac VW=\{v+W:v\in V\}
	\]
	as the left cosets of $ W $ in $ V $. Recall that two cosets $ v + V $ and $ u+W $ are the same coset if and only if $ v-u\in W $.
\begin{proposition}
  $ V/W $ is an $ \F $-vector space under operations
  \begin{align*}
   (u+W)+(v+W)&=(u+v)+W \\
   \lambda (v+W)&=(\lambda v)+W
\end{align*}
We call $ V/W $ the quotient space of $ V $ by $ W $.
\end{proposition}
\pf The proof is long and requires a lot of vector space axioms so we'll just sketch out the proof.\\
We check that operations are well-defined, so for $ u,\overline u, v,\overline v \in V $ and $ \lambda\in \F $ if
\[
  u+W=\overline u + W,\quad v+W=\overline v + W
\]
then
\[
	(u+v)+W=(\overline u+\overline w)+W
\]
and
\[
	(\lambda u) + W = (\lambda\overline u)+W
\]
The vector space axioms are inherited from $ V $.\qed
\begin{proposition}
	(Quotient map) The function $ \pi_W: V\to \frac VW $ called a \textit{quotient map} is given by
  \[
    \pi_W(v)=v+W
  \]
  is a well-defined, surjective, linear map with $ \ker\pi_W=W $.
\end{proposition}
\pf Surjectivity is clear. For linearity let $ u,v \in V $ and $ \lambda\in \F $. Then 
\begin{align*}
	\pi_W(u+\lambda v)&=(u+\lambda v) + W \\
			  &= (u+W)+(\lambda v+W)\\
			  &= (u+W)+\lambda(v+W) \\
			  &= \pi_W(u)+\lambda\pi_W(v)
\end{align*}
For $ v\in V $, we have that  $ v\in\ker \pi_W \iff \pi_W(v) = \mathbf 0_{V/W} $. So $ v+W=\mathbf 0_V+W $ so finally $ v=v-\mathbf 0_V\in W $.\qed
\begin{theorem}
	(First isomorphism theorem) Let $ V,W $ be $ \F $-vector spaces and $ \alpha:V\to W  $ linear. Then there is an isomorphism
	\[
		\overline\alpha:\frac{V}{\ker \alpha}\to\ima\alpha
	\]
	given by $ \overline\alpha(v+\ker\alpha)=\alpha(v) $
\end{theorem}
\pf For $ u,v\in V $,
\begin{align*}
	u+K=v=K &\iff u-v\in K
		&\iff \alpha(u-v)=\mathbf 0_W
		&\iff \alpha(u)=\alpha(v)
		&\iff \overline \alpha(u+\ker\alpha)=\overline\alpha(v+\ker\alpha)
\end{align*}
The forward direction shows that $ \overline\alpha $ is well-defined, and the converse shows that $ \overline\alpha $ is injective. For surjectivity given $ w\in\ima\alpha $, there exists some $ v\in V \st w=\alpha(v)$. Then $ w=\overline\alpha(v+\ker\alpha) $.\\
Finally for linearity given $ u,v\in V $, $ \lambda\in\F $,
\begin{align*}
	\overline\alpha((u+\ker\alpha)+\lambda(v+\ker\alpha))&=\overline\alpha((u+\lambda v)+\ker\alpha)\\
  &= \alpha(u+\lambda v)\\
  &= \alpha(u)+\lambda\alpha(v) \\
  &= \overline\alpha(u+\ker\alpha)+\lambda\overline\alpha(v+\ker\alpha)
\end{align*}
So $ \overline \alpha$ is linear hence is an isomorphism\qed
\subsection{Basis}
\begin{definition}
	(Span) Let $ V $ be a $ \F $-vector space. Then the \textit{span} of some subset $ S\subseteq V $ is
	\[
		\langle S\rangle = \left\{\sum_{s\in S}\lambda_s\cdot s: \lambda_s\in \F\right\}
	\]
	where $ \sum $ denotes finite sums. An expression the form above is called a \textit{linear combination} of $ S $.\\
	We say that $ S $ \textit{spans} $ V $ if $ \langle S\rangle =V $
\end{definition}
\begin{definition}
	(Finite-dimensional) For a vector space $ V $ we say that it is \textit{finite-dimensional} if there exists a finite spanning set.
\end{definition}
We'll give some simple remarks without proof.
  \begin{enumerate}
	  \item $ \langle S \rangle\le V $ and conversely if $ W\le V $ and $ S\subseteq W $ then $ \langle S\rangle \le W $.
	  \item If $ S,T\subseteq W $ and $ S $ spans $ V $ and $ S\subseteq \langle V\rangle $ then $ T $ spans $ V $.
	  \item By convention $ \langle\emptyset\rangle =\{\mathbf 0_V\} $.
	  \item $ \langle S\cup T\rangle = \langle S\rangle + \langle T\rangle $
  \end{enumerate}
For an example consider $ V=\R^3 $ and consider the sets
\begin{align*}
  S=\left\{\begin{pmatrix}
    1\\0\\0
  \end{pmatrix},\begin{pmatrix}
    1\\1\\2
\end{pmatrix}\right\}\\
T=\left\{\begin{pmatrix}
  2\\1\\2
\end{pmatrix},\begin{pmatrix}
  0\\1\\2
\end{pmatrix},\begin{pmatrix}
  -1\\2\\4
\end{pmatrix}\right\}
\end{align*}
Then $ \langle S\rangle = \langle T\rangle =\left\{\begin{pmatrix}
  x\\y\\2y
\end{pmatrix}:x,y\in \R\right\} \le \R^3.$\par
For a second example consider $ V=\R^\N $ and set $ T=\{\delta_n:n\in \N\} $. This is not a spanning set, since we require infinitely many elements from $ T $ to make an element in $ V $. In fact we can write that
\[
	\langle T\rangle =\{f\in \R^\N:f(n)=0\text{ for all but finitely many terms}\}.
\]
\begin{definition}
	(Linear Independence) A subset $ S\subseteq V $ is called \textit{linearly independent} if, for all finite linear combinations
\[
	\sum_{s\in S}\lambda_ss\quad \text{of S}
\]
if the sum is the zero vector in $ V $ the $ \lambda_s=0 $ for all $ s\in S $.
\end{definition}
If $ S $ is not linearly indepedent we say that $ S $ is linearly dependent.\par
We'll make some more remarks
\begin{enumerate}
	\item If $ \mathbf 0 \in S $ then $ S $ is not linearly independent.
	\item If we have a finite set, then to show linearly independent, we only need to consider the linear combination of all elements, not all finite lienar combinations.
	\item However is $ S $ is infinite, then we have to consider every possible finite subset of $ S $ and show it's linearly independent.
	\item Every subset of a linearly independent set is itself linearly indepedent.	
\end{enumerate}
\begin{definition}
	(Basis) A subset $ S\subseteq V $ is a \textit{basis} for $ V $ if $ S $ is linearly independent and a spanning set.
\end{definition}
For an example consider $ e_i\in \F^n $ be given by
\[
  e_i=\begin{pmatrix}
    0\\ \vdots \\ 0 \\ 1 \\ 0 \\\vdots \\ 0
\end{pmatrix}\quad \text{with the 1 in the } i\text{th entry}
\]
then the set $ \{e_i:1\le i\le n\} $ is the standard basis for $ \F^n $.\par
For $ P(\R) $ the set of real polynomial functions and let $ p_n\in P(\R) $ be given by $ p_n(x)=x^n $, then $ \{p_n:n\in\Z_{\ge 0}\} $ is a basis for $ P(\R) $.
\begin{proposition}
  If $ S\subseteq V $ is a finite spanning set, then there exists a subset $ S'\subseteq S $ such that $ S' $ is a basis.
\end{proposition}
\pf If $ S $ is linearly independent then we're done. Otherwise write $ S=\{v_1,\dots, v_n\} $. Then there exists $ \lambda_1,\dots, \lambda_n $ such that $ \lambda_1v_1+\cdots\lambda_nv_n=\mathbf 0 $ wlog suppose that $ \lambda_n $ is nonzero. Then
\begin{align*}
	v_n=-\frac 1{\lambda_n}\sum_{i=1}^{n-1}\lambda_iv_i
\end{align*}
so $ v_n $ is in the span of the other vectors. Hence $ S\setminus \{v_n\} $ is still a spanning set. Repeat which the set is linearly independent, must terminate since the set is finite and the empty set is not a spanning set.\qed
\begin{corollary}
  Every finite-dimensional vector space has a finite basis.
\end{corollary}
\pf Trivial application of the proposition\qed
\begin{theorem}
	(Steinitz Exchange Lemma) Let $ S,T\subseteq V $ finite with $ S $ linearly independent and $ T $ a spanning set of $ V $. Then
	\begin{enumerate}
		\item $ |S|\le |T| $,
		\item and there exists $ T'\subseteq T $ which has size $ |T'|=|T|-|S| $ and $ S\cup T' $ spans $ V $.
	\end{enumerate}
\end{theorem}
\pf To come later...\par
Let's look at some consequences of the lemma first.
\begin{corollary}
	For a finite-dimensional vector space $ V $,
  \begin{enumerate}
	  \item Every basis for $ V $ is finite.
	  \item All finite basis have the same size.
  \end{enumerate}
\end{corollary}
\pf $ V $ has a finite basis $ B $, suppose we have some other basis $ B' $ infinite. Let $ B''\subseteq B' $ with $ |B''|=|B|+1 $ then $ |B''| $ is linearly independent, so applying (i) of the Steinitz exchange lemma with $ S=B'' $ and $ T=B $ we get a contradiction.\par
For the second part, let $ B_1,B_2 $ be finite basis for $ V $ then apply Steinitz symmetrically since both are spanning set and linearly independent, so we get that $ |B_1|\ge |B_2| $ and $ |B_1|\ge |B_2| $ so $ |B_1|=|B_2| $.\qed
\begin{definition}
	(Dimension) For a vector space $ V $ the \textit{dimension} of $ V $ is the size of any basis. We write this as \dim V.
\end{definition}
This definition is well-defined by the previous corollary.\par
For an example $ \dim \F^n=n $ since we've shown the standard basis has size $ n $. As a complex vector space $ \C $ is one-dimensional as a complex vector space and two-dimension as a real vector space, with basis $ \{1\} $ and $ \{1,i\} $ repectively.
\begin{corollary}
  For a vector space $ V $ let $ S,T\subseteq V $ finite, with $ S $ linearly independent and $ T $ a spanning set, then
  \begin{align*}
	  |S|\le \dim V\le |T|
	  \end{align*}
	  with equality if and only if $ S $ spans or $ V $ is linearly independent respectively.
\end{corollary}
\pf The inequalities are immediate from Steinitz. If $ S $ is a basis then $ |S|=\dim V $ from the previous corollary. Conversely if $ |S|=\dim V $ and let $ B $ be a basis for $ V $ so we have that $ |B|=|S| $ so $ B $ is a spanning set. So we can apply Steinitz (ii) to $ B $ so there exists $ B'\subseteq B $ with $ |B'|=|B|-|S|=0 $ and $ S\cup B'=S\cup \emptyset $ spans $ V $. So $ S $ is a basis. Similiar we have a very similar proof for equality in $ V $.\qed\par We will not prove that every vector space has a basis, however some non-finitely dimensional vector spaces have an infinite basis, for example $ P(\R) $.
\begin{proposition}
  If $ V $ is a finite-dimensional vector space, then if $ U\le V $ then $ U $ is finite-dimensional, namely, $ \dim U \le \dim V $ with equality if and only if $ U=V $.
\end{proposition}
\pf If $ U=\{\mathbf 0\} $, we're done. Otherwise let $ \mathbf 0\ne u_1\in U $. Then $ \{u_1\}\subseteq U $ is linearly indepedent. Repeating, after repeating $ k $ times suppose we have $ \{u_1,\dots, u_k\} $ linearly indepedent with $ k\le \dim(V) $ by the previously corollary. If the set spans $ U $ we're done, if not we'll add another vector, $ u_{k+1} $ outside of the span of our space. If $ \{u_1,\dots, u_{k+1}\} $ is not linearly indepedent, we can write $ \mathbf 0 $ non-trivially, so
\[
	\sum_{i=1}^{k+1}\lambda_i u_i=\mathbf 0
\]
with $ \lambda_{k+1}\ne 0 $ since $ \{u_1,\dots, u_k\} $ linearly indepedent. Thus we have that
\[
	u_{k+1}=-\frac 1{\lambda_{k+1}}\left(\sum_{i=1}^k\lambda_iu_i\right)
\]
this process must terminate after at most $ \dim V $ many steps, by the previous corollary. If $ \dim U=\dim V $ apply the previous corollary with $ S $ being any basis for $ U $.\qed
\begin{proposition}
	(Extending a basis) Let $ U\le V $. For any basis $ B_U $ of $ U $ there exists a basis $ B_V $ of $ V $ such that $ B_U\subseteq B_V $.
\end{proposition}
\pf Apply the second result from Steinitz with $ S=B_U $ and $ T $ is any basis for $ V $. We obtain that $ T'\subseteq T\st $ 
\[
  |T'|=|T|-|S|=\dim V-\dim U
\]
and $ B_V=B_U\cup T' $ spans $ V $. But we have that
\[
  |B_V|\le |B_U|+|T'|=\dim V
\]
so by the previous corollary, $ B_V $ is a basis for $ V $.\qed\par
Now we'll finally prove the Steinitz exchange lemma.\par
\pf Let $ S=\{u_1,\dots, u_m\} $, $ T=\{v_1,\dots, v_n\} $ with $ |T|=m $ and $ |T|=n $. If $ S $ is empty then we're done. Otherwise there exists $ \lambda_i\in \F $ such that 
\[
	u_1=\sum_{i=1}^n\lambda_iv_i
\]
so by renumbering we can say that $ \lambda_1\ne 0 $. Then
\[
	v_1=\frac 1{\lambda_1}\left(u_1-\sum_{i=2}^n\lambda_iv_i\right)
\]
So $ \{u_1,v_2,\dots, v_n\} $ spans $ V $. After repeating $ k $ times with $ k<m $ suppose $ \{u_1,\dots, u_k,v_{k+1},\dots, v_n\} $ spans V, then there exists $ \lambda_i,\mu_j\in\F $ such that
\begin{align*}
	u_{k+1}=\sum_{j=1}^k\mu_ju_j+\sum_{i=k+1}^n\lambda_iv_i
\end{align*}
If for all $ \lambda_i= 0 $ then
\[
	\left(\sum_{j=1}^k\mu_ju_j\right) -u_{k+1}=\mathbf 0
\]
which is a contradiction since $ S $ is linearly independent. So by relabeling we have that $ \lambda_{k+1}\ne 0 $ such that 
\[
	v_{k+1}=\frac 1{\lambda_{k+1}}\left(u_{k+1}-\sum_{j=1}^k\mu_ju_j-\sum_{i=k+1}^n\lambda_iv_i\right)
\]
so $ (u_1,\dots, u_{k+1},v_{k+2},\dots, v_n\}$ spans $ V $. So we can conclude that $ m\ne n $ and $ \{u_1,\dots, u_m,v_{m+1},\dots, v_n\} $ spans $ V $ hence the set $ T'=\{v_{m+1},\dots, v_n\} $ exists as claimed. \qed
\begin{definition}
	(Nullity) For a linear map $ \alpha:V\to W $ we define the \textit{nullity} of $ \alpha $ as
	\[
	  \n(\alpha)=\dim\ker\alpha.
	\]
\end{definition}
\begin{definition}
	(Rank) For a linear map $ \alpha:V\to W $ we define the \textit{rank} of $ \alpha $ as
	\[
	  \rk(\alpha)=\dim\ima\alpha.
	\]
\end{definition}
\begin{theorem}
	(Rank-nullity theorem) If $ V $ is a finite dimensional $ \F $-vector space and $ W  $ is a $ \F $-vector space. Then if $ \alpha:V\to W $ is linear then $ \ima \alpha $ is finite dimensional and
	\[
	  \dim V=\n(\alpha)+\rk(\alpha).
	\]
\end{theorem}
\pf Recall the first isomorphism theorem so
\[
	\frac V{\ker \alpha}\cong \ima\alpha
\]
It is sufficient to prove the lemma
\begin{lemma}
  For $ U\le V $,
  \[
    \dim(V/U)=\dim V-\dim U
  \]
\end{lemma}
\pf Let $ B_U=\{u_1,\dots, u_m\} $ be a basis of $ U $. Extend to a basis $ B_V=\{u_1,\dots, u_m,v_{m+1},\dots, v_n\} $ of $ V $ where $ m=\dim U $ and $ n=\dim V $.\\
Set $ B_{V/U}=\{v_i+U:m+1\le i\le n $. The we claim that $ B_{V/U} $ is a basis for $ V/U $ of size $ n-m $. To show spanning, for $ v\in V $ write
	\[
	  v=\sum_i\lambda_iv_i+\sum_j\mu_jv_j
	\]
	Then $ v+U=\sum_i\lambda_i(v_i+U)\in \langle B_{V/U}\rangle $. For linear independence, suppose
	\[
	  \sum_i\lambda_i(v_i+U)=\mathbf 0+U
  \]
	hence 
	\begin{align*}
	  =\left(\sum_i\lambda_iv_i\right)+U \\
	  \sum_i\lambda_iv_i\in U\\
	  \sum_i\lambda_iv_i=\sum_j\mu_ju_j
	\end{align*}
	since $ B_V $ is linearly independent, we have that all $ \lambda_i $ and $ \mu_j $ are zero. Similiarly if $ v_i+U=v_j+U $ with $ i\ne j $ then we can write $ v_i-v_j=\sum_j\mu_ju_j $ which is a contradiction.\qed
	\begin{remark}
	   We can maek a direct proof without quotient spaces by rearranging some of the arguments of the proof.
	\end{remark}
\begin{corollary}
	(Linear Pigeonhole principle) If $ \dim V=\dim W=n $ and $ \alpha: V\to W $ then the following conditions are equivalent.
	\begin{enumerate}
		\item $ \alpha $ is injective,
		\item $ \alpha $ is surjective,
		\item $ \alpha $ is an isomorphism.
	\end{enumerate}
\end{corollary}
\pf If $ \alpha $ injective then $ \n(\alpha)=0 $ so by rank nullity we have that $ \rk(\alpha)=n $ so $ \alpha $ is surjective. If $ \alpha $ is surjective then $ \rk(\alpha)=n $ so by rank nullity, the dimension of the kernel is $ 0 $ hence the kernel is trivial, so $ \alpha $ injective, hence $ \alpha $ is an isomorphism. If $ \alpha $ is an isomorphism, clearly it's injective, so all equivalent.\qed
\begin{proposition}
  Suppose $ V $ is a vector space with a basis $ B $. For any vector space $ W	 $ and any function $ f:B\to W $ there is a unique linear map $ F:V\to W $ such that $ F(B)=W $.
\end{proposition}
\pf First we'll show existance. For $ v\in V $ write $ v=\sum_b\lambda_bb $ for a finite sum. Then define
\[
  F(v)=\sum_b \lambda_bf(b).
\]
This is well-defined, since $ B $ is a basis the $ \lambda_b $ are uniquely determined by $ v $. For $ u,v\in V $ and $ \lambda\in \F $ we write
\[
  u=\sum_b\mu_bb,\quad \sum_b\lambda_bb.
\]
Then \begin{align*}
	F(u+\lambda v)&=F(\sum_b(\mu_b+\lambda\lambda_b)f(b)\\
		     &= \sum_b\mu_bf(b)+\lambda\sum_b\lambda_bf(b)\\
		     &= F(u)+\lambda F(v).
\end{align*}
So $ F $ is linear. To show uniqueness $ \overline F:V\to W $ is another linear map extending $ f $ then,
\[
  \overline F\left(\sum_b\lambda bb\right)=\sum_b\lambda_b\overline F(b)
\]
which is the same as our definition for $ F $ hence they are the same function.
\begin{corollary}
	For a vector space, $ V $, with $ \dim V=n $ with a basis $ B=\{v_1,\dots, v_n\} $ for $ V $ then there is a unique isomorphism
  \begin{align*}
	  F_B:V&\to \F^n\\
	       \sum_{i=1}^n\lambda_iv_i &\to \begin{pmatrix}
	        \lambda_1\\
		\vdots \\
		\lambda_n
	      \end{pmatrix}
  \end{align*}
  \end{corollary}
  \pf Let $ E=\{e_1,\dots, e_n\} $ be the standard basis for $ \F^n $. Define
  \begin{align*}
	  f:B&\to W\\
	  v_i&\to e_i
  \end{align*}
  and let $ F_B $ be the unique linear extension of $ f $ to $ V $. We see that $ f $ defines a bijection from $ B\to E $. Let $ \bar F_B $ be the unique linear extension of $ \inv f:E\to B $. Then $ \bar F_B\cdot F_B $ is the composition of two linear maps, hence it's linear, moreover it is $ \mathrm{id}_B $. But also $ \mathrm{id}_V $ is also a linear extension of $ \mathrm{id}_B $, by the proposition, they are the same map so $ \bar F_B\cdot F_B=F_B\cdot \bar F_B=\mathrm{id}_B $. Hence $ F_B $ is bijective, so it is an isomorphism.\qed
  \begin{corollary}
    If $ V,W $ are finite dimensional $ \F $-vector spaces. Then
    \[
      V\cong W\iff \dim V=\dim W
    \]
  \end{corollary}
  \pf Trivial from the corollary using the transitivity of the isomorphism relation.\qed
\begin{definition}
	(Coordinate vector) $ F_B(v)=[v]_B $ is the \textit{coordinate vector} of $ v $ with respect to the basis $ B $
\end{definition}
For an example if $ V\cong \F^n $ and $ U\le V $ with $ U\cong \F^m $ then $ \dim(V/U)=n-m $, so $ \frac VU\cong \F^{n-m} $.
\subsection{Direct sums}
\begin{definition}
	(External direct sum) For $ \F $-vector spaces, $ V $ and $ W $, we dnote the \textit{external direct sum} of $ V $ and $ W $ as $ V\oplus W $ with underlying set $ V\times W $ with addition and scalar multiplication given in the obvious sense.
\end{definition}
We can similarly define
\[
	V_1\oplus \cdots \oplus V_n=\bigoplus_{i=1}^nV_i.
\]
\begin{lemma}
  For $ V,W $ finite dimensional vector spaces,
  \[
    \dim(V\oplus W)=\dim V+\dim W
  \]
\end{lemma}
\pf\\(First Proof) Let $ B,C $ be basis for $ V,W $ respectively. Set
\[
	D=(B\times\{\mathbf 0_W\})\cup (\{\mathbf 0_V\}\times C)
\]
it is straightfoward to check that $ D $ is basis of $ V\oplus W $ of the size $ \dim V + \dim W $.\qed\par
(Second Proof) Suppose $ V\cong \F^n $ and $ W\cong \F^m $ construct an isomorphism $ V\oplus W\cong\F^{n+m} $.\qed

\begin{proposition}
  Let $ V $ be a vector space with $ U,W\le V $. There is a surjective linear map
  \begin{align*}
	  \varphi: U\oplus W &\to U+W\\
	  (u,w) &\to u+w
  \end{align*}
  with $ \ker\varphi\cong U\cap W $.
\end{proposition}
\pf Surjectively and linearity are clear. Note for $ (u,w)\in U\oplus W $ then $ (u,w)\in\ker\varphi $ if and only if $ w=-u $. Hence
\[
	\ker\varphi=\{(x,-x):x\in U\cap W\}
\]
the map $ \psi:U\cap W\to \ker\varphi $ sending $ x\to(x,-x) $ is an isomorphism.
\begin{corollary}
	(Sum-Intersection Formula) If $ V $ is finite dimensional and $ U,W\le V $ then
	\[
	  \dim (U+W)=\dim U + \dim V-\dim(U\cup V)
	\]
\end{corollary}
Applying the rank-nullity theorem to the linear map $ \varphi $ in the proposition we get that
\begin{align*}
	\dim U + \dim W&=\dim (U\oplus V)\\
		       &=\dim(\ker\varphi)+\dim(\ima\varphi)\\
		       &=\dim(U+W)+\dim(U\cap W)\qed
\end{align*}
We can also give an explicit basis. Given a basis $ B $ for $ U\cap W $, extend $ B $ to a basis $ B_U $ for $ U $, and a basis $ B_W $ for $ W $. Then $ B_U\cap B_W $ spans $ U+W $ and
\[
  |B_U\cup B_W|\le |B_U|+|B_W|-|B|=\dim(U+V)
\]
hence $ B_U\cup B_W $ is linearly independent so it's a basis for $ U+W $.
\begin{remark}
We could also check directly that $ B_U\cup B_W $ is linearly independent of the size $ \dim(U+V) $ without assuming the sum-intersection formula, so this alos servers as an alternative proof of the sum-intersection formula.
\end{remark}
\begin{definition}
	(Internal direct sum) Suppose $ U,W\le V $ satisify 
	\begin{enumerate}
		\item $ U+W=V $,
		\item $ U\cap W=\{\mathbf 0_V\} $.
	\end{enumerate}
	Then 
	\begin{align*}
	  \varphi: U\oplus W\to V
	\end{align*}
	is an isomorphism, and we say that $ V $ is the \textit{internal direct sum} of $ U $ and $ W $, and we write that $ V=U\oplus W $.
\end{definition}
Alternatively, every element $ v\in V $ can be written \textit{uniquely} as $ v=u+w $ for $ u\in U, w\in W $.
\begin{definition}
	(Direct complement) For $ U\le V $ a \textit{direct complement} to $ U $ in $ V $ is a subspace $ W\le V $ satisfying $ V=U\oplus W $.
\end{definition}
\begin{proposition}
  If $ V $ is finite dimensional then every subspace has a direct complement.
\end{proposition}
\pf Let $ U\le V $ and let $ B_U $ be a basis for $ U $. Extend to a basis $ B_V $ for $ V $. Set $ W=\langleB_V\setminus B_U\rangle $. Then
\begin{align*}
	V=\langle B_V\rangle &= \langle B_U\cup (B_V\setminus B_U)\rangle\\
			     &= \langle B_U\rangle + \langle B_V\setminus B_U\rangle \\
			     &=U+W.
\end{align*}
Moreover using the sum-intersection formula
\begin{align*}
	\dim (U\cap W)=|B_V|+|B_U|-|B_V\setminus B_U|=0.
\end{align*}
Hence $ U\oplus W=V $.\qed\par
More generally for $ U_1,\dots, U_n\le V $ we say that $ V $ is the direct sum of the $ U_i $ and write that
\[
	V=U_1\oplus+\cdots +\oplus V_n=\bigoplus_{i=1}^n V_i
\]
if the map
\begin{align*}
	\varphi: U_1\oplus\cdots\oplus U_n &\to V\\
	(u_1,\dots, u_n) &\to u_1,\dots, u_n
\end{align*}
is an isomorphism. Equivalently every $ v\in V $ can be uniquely written as $ v=u_1+\dots +u_n $ for $ u_i\in U_i $.
\section{Matrices and Linear Maps}
\subsection{Vector spaces of linear maps}
\begin{definition}
  For $ V,W $ $ \F $-vector spaces we define
  \[
	  \mathcal L(V,W)=\{\alpha:V\to W:\alpha\text{ is linear} \}
  \]
  which forms a $ \F $-vector space under pointwise addition and obvious scalar multiplication.
\end{definition}
Recall that $ M_{m\times n} $ is the space of matrices over $ \F $ with $ m $ rows and $ n $ columns. For $ A\in M_{m\times n}(\F) $ we write $ A=(a_{ij}) $ where $ a_{ij}\in \F $ is the entry in the $ i $th row and the $ j $th column.\par
Let $ B=\{v_1,\dots, v_n\}, C=\{w_1,\dots, w_m\} $ are \textit{ordered} basis for $ V,W $.\par
Let $ \alpha\in\mathcal L(V,W) $. We can write
\begin{align*}
	\alpha(v_1)&=a_{11}w_1+a_{21}w_2+\cdots + a_{m1}w_m\\
	\alpha(v_2)&=a_{12}w_1+a_{22}w_2+\cdots + a_{m1}w_m\\
	^\vdots\\
	\alpha(v_n)&=a_{1n}w_1+a_{2n}w_2+\cdots +a_{mn}w_m
\end{align*}
\begin{definition}
	(Matrix) The \textit{matrix} of $ \alpha $ with respect to the ordered basis $ B,C $ is
	\[
		[\alpha]^B_C=(a_{ij})\in M_{m\times n}(\F)
	\]
\end{definition}
Recall we have a linear isomorphism
\begin{align*}
	\varepsilon_B:V &\to \F^n\\
	v=\sum_{i=1}^n\lambda_iv_i &\to (\lambda_i)_i=[v]_B
\end{align*}
where $ [v]_B $ is the coordinate vector of $ v $ with respect to $ B $.
\begin{theorem}
  For finite-dimensional vector spaces $ V,W $ with basis $ B,C $ respectively and $ \alpha:V\to W $ linear  then
  \begin{enumerate}
	  \item For all $ v\in V $
		  \[
			  [\alpha]^B_C[v]_B=[\alpha(v)]_C
		  \]
	  \item $ [\alpha]_C^B $ is the only matrix $ A\in M_{m\times n}(\F) $ satisfying $ A[v]_B =[\alpha(v)]_C $ for all $ v\in V $.
	  \item There is an isomorphism of $ \F $-vector spaces
		  \begin{align*}
			  \varepsilon_C^B:\mathcal L(V,W)&\to M_{m\times n}(\F)\\
			  \alpha &\to [\alpha]^B_C
		  \end{align*}
\end{theorem}
\pf Let $ v\in V $ write $ v=\sum_{j=1}^n\lambda_jv_j $. Then
\begin{align*}
	\alpha(v)&=\sum_{j=1}^n\lambda_j\alpha(v_j)\\
		 &=\sum_{j=1}^n\lambda_j\sum_{i=1}^ma_{ij}w_i\\
		 &=\sum_{i=1}^m\left(\sum_{j=1}^n\lambda_ja_{ij}\right)w_i.
\end{align*}
So
\begin{align*}
	[\alpha(v)]_C&=\left(\sum_{j=1}^n a_{ij}\lambda_j\right)_i\\
		     &=(a_{ij})\cdot(\lambda_j)\\
		     &=[\alpha]^B_C[v]_B.
\end{align*}
Hence (i) is proved. For (ii), take $ 1\le j\le n $, so $ [v_j]_B=e_j $. Hence for $ A\in M_{m\times n}(\F) $, $ A[v_j]_B $ is the $ j $th column of $ A $. But if $ A[v_j]_B=[\alpha(v_j)]_C=[\alpha]^B_C[v_j]_B=[\alpha]^B_Ce_j $, then $ A[v_j]_B $ is also the $ j $th column of $ [\alpha]^B_C $. Since this holds for all $ j $ in our range, they are the same matrix.\par
Now for part (iii), let $ \alpha,\beta\in\mathcal L(V,W) $ and $ \lambda\in \F $. Then
\begin{align*}
	[\alpha+\lambda\beta]^B_C[v]_B&=[(\alpha+\lambda\beta)(v)]_C\\
				      &=[\alpha(v)+\lambda\beta(v)]_C\\
				      &= [\alpha(v)]_C+\lambda[\beta(v)]_C\\
				      &= ([\alpha]^B_C+\lambda[\beta]^B_C)[v]_B
\end{align*}
for all $ v\in V $. Hence by (ii) we get that $ [\alpha+\lambda\beta]^b_c=[\alpha]_C^B+\lambda[\beta]^B_C $ so the map is linear. Let $ \alpha\in\ker(\varepsilon_C^B) $ so that $ [\alpha]^B_C=0\in M_{m\times n} (\F)$. Then by (i) we have that $ [\alpha(v)]_C=0 $ for all $ v\in V $. But $ \varepsilon:w\to [w]_C $ is an isomorphism so $ \alpha(v)=0 $ for all $ v\in V $ hence $ \alpha=0 $ and $ \alpha $ is injective. For surjectivity let $ A\in M_{m\times n}(\F) $ and define $ f:B\to W $ by $ f(v_j)=\sum_{i=1}^na_{ij}w_I $ and extend $ f $ to a linear map $ F:V\to W $. Then $ [F]_C^B=A $. So $ \varepsilon_C^B $ is an isomorphism.\qed
\begin{proposition}
  Let $ V,W,X $ be finite-dimensional $ \F $-vector spaces with basis $ B,C,D $ and $ \alpha\in\mathcal L(V,W) $ and $ \beta\in\mathcal L(W,X) $. Then
  \[
	  [\beta\circ \alpha]^B_D=[\beta]^C_D[\alpha]^B_C.
  \]
\end{proposition}
\pf By the theorem $ [\beta\circ\alpha]^B_D $ is the unique matrix $ A $ satisfying
\[
	A[v]_B=[\beta(\alpha(v))]_D,\quad \forall v\in V.
\]
But $ [\beta]^C_D[\alpha]^B_C[v]_B=[\beta]^C_D[\alpha(v)]_C=[\beta(\alpha(v))]_D $. So by (ii) of theorem they are equal.\qed
\begin{remark}
  For any basis $ B $ of $ V $,
  \[
	  [\mathrm{id}_V]^B_B=I_{\dim V}.
  \]
\end{remark}
\begin{definition}
	(Change of basis matrix) Let $ B,B' $ be basis for $ V $ and $ \dim V=n $. The \textit{change of basis matrix} from $ B $ to $ B' $ is given by
	\[
		P=[\mathrm{id}_V]^B_{B'}\in M_{m\times n}(\F)
	\]
\end{definition}
Equivalently letting $ B=\{v_i\}_{i=1}^n $ and $ B'=\{v_i'\}_{i=1}^n $, then
\[
	P=(p_{ij})\quad\text{where}\quad v_j=\sum_{i=1}^n p_{ij}v_i'
\]
so the $ j $th column of $ P $ is $ [v_j]_{B'} $.
\begin{proposition}
	For $ V,W $ finite-dimensional vector spaces,\smallskip
  \begin{enumerate}
	  \item $ [\mathrm{id}_V]^B_{B'}\in GL_n(\F) $ with inverse $ [\mathrm{id}_V]^{B'}_B $.
	  \item If $ \alpha\in\mathcal L(V,W) $ and $ B,B' $ basis for $ V $ and $ C,C' $ basis for $ W $, then
		  \[
			  [\alpha]^{B'}_{C'}=[\mathrm{id}_W]^C_{C'}[\alpha]^B_C[\mathrm{id}_V]^{B'}_B.
		  \]
  \end{enumerate}
\end{proposition}
\pf By the remark,
\begin{align*}
	I_n=[\mathrm{id}_V]^B_B=[\mathrm{id}_V]^{B'}_B[\mathrm{id}_V]^B_{B'}
\end{align*}
and symmetrically swapping $ B $ and $ B' $. For the second part the result is immediate from the proposition.
\begin{definition}
	(Equivalent matrices) Let $ A,A'\in M_{m\times n}(\F) $. We say that $ A $ and $ A' $ are \textit{equivalent} if $ \exists P\in GL_m(\F) $, $ Q\in GL_n(\F) $ such that $ A'=PAQ $. 
\end{definition}
\begin{remark}
  Certianly $ A $ is equivalent to itself by $ P=I_m $ and $ Q=I_n $.\\
  If $ A'=PAQ $ then $ A=\inv PA'\inv Q $.\\
  If $ A''=RA'S $ too, then $ A''=(RP)A(QS) $, so the equivalence of matrices is an equivance relation on $ M_{m\times n}(\F) $.
\end{remark}
\begin{theorem}
  Let $ V,W $ be finite-dimensional $ \F $-vector spaces. Let $ \dim V=n $, $ \dim W=m $ and let $ \alpha\in\mathcal L(V,W) $. Let $ r=\rk(\alpha) $. Then,
  \begin{enumerate}
	  \item There exists basis $ B,C $ for $ V,W $ respectively such that
		  \[
			  [\alpha]^B_C=\begin{pmatrix}
				  I_r & 0 \\
				  0 & 0
			  \end{pmatrix}\in M_{m\times n}(\F)
		  \]
		  where $ I_r $ is the identity matrix of size $ r $, and the zeros are block zero matrices.
	  \item If \[
			  [\alpha]^{B'}_{C'}=\begin{pmatrix}
				  I_{r'} & 0 \\
				  0 & 0
			  \end{pmatrix}\in M_{m\times n}(\F)
		  \]
		  for some basis $ B',C'$ of $ V,W $ respectively, then $ r'=r $
  \end{enumerate}
\end{theorem}
\pf By rank-nullity $ \n(\alpha)=n-r $. Let $ \{v_{r+1},\dots, v_n\} $ be a basis for $ \ker\alpha $. Extend to a basis $ B=\{v_1,\dots, v_r,v_{r+1},\dots, v_n\} $. Then $ \{\alpha(v_1),\dots, \alpha(v_r)\} $ spans the image, and has size at most $ \dim(\ima(\alpha)) $, so it's linearly independent, hence we can extend it to form a basis of $ W $.
\[
	C=\{w_1=\alpha(v_1),\dots, w_r=\alpha(v_r),w_{r+1},\cdots, w_m\}
\]
Then
\[
  \alpha(v_j)=\begin{cases}
	  w_j & 1\le j\le r \\
	  \mathbf 0 & \text{otherwise}
  \end{cases}
\]
hence we have that $ [\alpha]^B_C= \begin{pmatrix}
				  I_{r} & 0 \\
				  0 & 0
			  \end{pmatrix} $.\par
			  For the second part, if $ [\alpha]^{B'}_{C'}= \begin{pmatrix}
				  I_{r'} & 0 \\
				  0 & 0
			  \end{pmatrix} $ then
			  \[
			    \alpha(v_j')=\begin{cases}
	  w_j' & 1\le j\le r' \\
	  \mathbf 0 & \text{otherwise}
  \end{cases}.


			  \]
			  Hence $ w_1',\dots, w_{r'}' $ span $ \ima(\alpha) $ and are linearly independent. Hence $ \rk(\alpha)=r' $.\qed
\begin{definition}
	(Column-space) For $ A\in M_{m\times n}(\F) $ the \textit{column-space} $ \Col(A) $ is the subspace of $ \F^m $ spanned by the columns of $ A $. The dimension of the column-space is called the \textit{column-rank} of $ A $.
\end{definition}

\begin{definition}
	(Row-space) For $ A\in M_{m\times n}(\F) $ the \textit{row-space} $ \Row(A) $ is the subspace of $ \F^m $ spanned by the rows of $ A $ (when transposed as column vectors). The dimension of the row-space is called the \textit{row-rank} of $ A $.
\end{definition}
\begin{remark}
  \[
    \Row(A)=\Col(A^T)
  \]
  hence the row-rank of $ A $ is the same as the column-rank of $ A^T $.
\end{remark}
\begin{remark}
	Given a matrix $ A\in M_{m\times n}(F) $ we can define a linear map $ \alpha:\F^n\to \F^m $ by $ \alpha(v)=Av $. Then $ \ima(\alpha)=\Col(A) $, so the rank of $ \alpha $ is the same as the column-rank of $ A $. Moreover, $ A=[\alpha]^{E_n}_{E_m} $ where $ E_k $ are the standard basis for $ \F^k $.
\end{remark}
We may write $ \ima A,\ker A, \rk(A),\n(A) $ to refer to the corresponding concepts for $ \alpha $.
\begin{theorem}
	Let $ A, A' \in M_{m\times n}(\F) $, then
	\begin{enumerate}
		\item $ A $ is equivalent to
\[
  \begin{pmatrix}
	  I_r & 0 \\
	  0 & 0
  \end{pmatrix}
  \text{ where } r \text{ is the column-rank of } A
\]
\item $ A $ and $ A' $ are equivalent if and only if the have the same column-rank.

	\end{enumerate}
\end{theorem}
\pf We'll first prove a lemma.
\begin{lemma}
	For $ A\in M_{m\times n}(\F) $ and $ B\in M_{n\times p}(\F) $ then $ \rk(A\cdot B)\le \min(\rk(A),\rk(B)) $.
\end{lemma}
\pf We have that $ \ima(AB)\le \ima(A) $ so $ \rk(AB)\le \rk(A) $. If $ Bv=\mathbf 0 $ for $ v\in \F^p $, then $ ABv=\mathbf 0 $, so $ \n(B)\ge \n(AB) $, so applying rank-nullity, we get that
\[
  p-\rk(B)\le p-\rk(AB)\implies \rk(AB)\le \rk(B)\qed
\]
\par
Now we'll prove the first part of the theorem. Let $ \alpha $ the natural linear map corresponding to $ A $, so $ A=[\alpha]^{E_n}_{E_m} $. By the previous theorem, there exists matrices $ B,C $ of $ \F^n,\F^m $ such that
\[
  \begin{pmatrix}
	  I_r & 0\\ 0 & 0
  \end{pmatrix} = [\alpha]^B_C=[\mathrm{id}_{\F^m}]^{E_m}_C[\alpha]^{E_n}_{E_m}[\mathrm{id}_{\F^n}]^B_{E_n}=PAQ
\]
where $ r=\rk(\alpha) $ which we know is equal to the column-rank of $ A $.\par
If $ A' $ has column-rank $ r $ then both matrices are equivalent to $ \begin{pmatrix}
	I_r&0\\0 &0
\end{pmatrix} $, so by transitivity, $ A $ and $ A' $ are equivalent. Conversely suppose that $ A $ and $ A' $ are equivalent, so $ A'=PAQ $. By the lemma $ \rk(A')\ge \rk(AQ)\ge\rk(A) $ and symmetrically we get that $ \rk(A)\ge \rk(A') $, hence $ \rk(A')=\rk(A) $.\qed
\begin{theorem}
	For any $ A\in M_{m\times n}(\F) $, the row-rank of $ A $ is equal to the column-rank of $ A $.
\end{theorem}
\pf Note that if $ P $ is invertiable, then so it the tranpose with inverse $ (\inv P)^T $. Let $ r $ be the column-rank of $ A $. So there exists matrices $ P\in GL_m(\F) $ and $ Q\in GL_n(\F) $ such that $ PAQ=\begin{pmatrix}
	I_r&0\\ 0 & 0
\end{pmatrix}\in M_{m\times n}(\F) $. Then $ A^T $ is equivalent to $ Q^TA^TP^T=(PAQ)^T=\begin{pmatrix}
	I_r & 0 \\ 0 & 0
\end{pmatrix} \in M_{n\times m}(\F)$. By the previous theorem, the column-rank of $ A^T $ is $ r $ which also the row-rank of $ A $.\qed\par
Let $ V $ be a finite-dimensional vector space and $ B,B' $ be basis for $ V $. Now let $ \alpha\in\mathrm{End}(V)=\mathcal L(V,V) $. Then
\[
	[\alpha]^{B'}_{B'}=[\mathrm{id}_V]^B_{B'}[\alpha]^B_B[\mathrm{id}_V]^{B'}_B
\]
\begin{definition}
	(Similarity) For matrices $ A,A'\in M_{n\times m}(\F) $ are \textit{similar} if there exists $ P\in GL_n(\F) $ such that $ A'=\inv P AP $.
\end{definition}
\begin{remark}
We have some remarks showing the similarity and equivalence are not the same thing.
\begin{enumerate}
	\item Similarity is an equivalence relation on $ M_{n\times n}(\F) $.
	\item Similar matrices are equivalent but equivalent matrices need not be similar.
\end{enumerate}
For example every matrix in $ GL_n(\F) $ is equivalent to $ I_n $ but $ I_n $ forms its only single element equivalence class, when we think about similarity.
\end{remark}
\subsection{Elementary operations on matrices}
\begin{definition}
(Elementary row operations)
Let $ r_1,\dots, r_m $ be the rows of $ A $. We have three types of \textit{elementary row operations} on $ A $
\begin{enumerate}
	\item Swap $ r_i $ and $ r_j $ with $ i\ne j $.
	\item Replace $ r_i $ with $ \lambda r_i $ with $ 0\ne\lambda\in\F $.
	\item Replace $ r_i $ with $ r_i+\lambda r_j $ with $ \lambda\in \F $ and $ i\ne j $.
\end{enumerate}
\end{definition}
Similarly there are three types of elementary column operations.
\begin{remark}
  These are all reversable.
\end{remark}
Each elementary operation has a corresponding matrix representation repesentation. All corresponding matrices are invertiable.
\begin{lemma}
  If $ E $ is a matrix of type (i)-(iii) then $ EA $ is obtained from $ A $ by applying the corresponding ERO to $ A $.
\end{lemma}
\pf Direct matrix computation.
\begin{remark}
  Similarly $ AE $ is obtained by applying the corresponding ECO
\end{remark}
\begin{remark}
  EROs preserve $ \Row(A) $ (and ECOs preserve $ \Col(A) $).
\end{remark}
So both EROs and ECOs preserve the row-rank of a matrix, and therefore also the rank of the linear map corresponding to the matrix.
\begin{definition}
	(Row reduced echelon form) A matrix $ A\in M_{m\times n}(\F) $ is said to be in \textit{row reduced echelon form} (RRE) if
	\begin{enumerate}
		\item All non-zero rows of $ A $ appear above all zero rows.
		\item The leftmost non-zero element of a non-zero row is $ 1 $ (called the \textit{pivot entry}).
		\item If row $ r_i,r_j $ are non-zero rows with $ i<j $ then the index of the pivot entry of $ i $ is less than the index of the pivot entry of $ j $.
		\item In a column containing a pivot entry, every other entry is zero.
	\end{enumerate}
\end{definition}
For an example consider
\[
  M=\begin{pmatrix}
	  1 & a & 0 & 0 & b\\
	  0 & 0 & 1 & 0 & c\\
	  0 & 0 & 0 & 1 & d
  \end{pmatrix}
\]
which is in row reduced echelon form. Similarly we have column reduced echelon form, which have the exact same rules but transposed.
\begin{lemma}
  If $ A $ is in row reduced echelon form then the row rank of $ A $ is the number of non-zero rows of $ A $.
\end{lemma}
\pf Let $ r_1,\dots, r_k $ be the non-zero rows of $ A $ let $ j_i=P(v_i) $ be the pivot entry. Certainly $ r_1,\dots, r_k $ span $ \Row(A) $. Suppose that
\[
	v=\sum_{i=1}^k\lambda_ir_i=0\quad (\lambda_i\in\F).
\]
Then $ (v)_{j_i} =\lambda_i=0 $ so the non-zero rows are linearly independent so we're done.\qed
\begin{proposition}
	Every matrix $ A \in M_{m\times n}(\F) $ can be put into row reduced echelon form with elementary row operations.
\end{proposition}
\pf Proceed by induction on $ n $. Write that $ A=\left[c_1\mid \cdots \mid c_n\right] $. If $ c_1=0 $ apply induction to $ [c_2\mid \cdots \mid c_n] $, so suppose that $ c_1\ne 0 $, suppose that element in $ (i,1) $ is non-zero. Applying row operations (i) we can move it to $ (1,1) $.  Apply row operation (ii) to rescale it to be $ 1 $. Now we can clear the rest of the column by (iii). By induction we can use elementary row operation on rows $ 2$-$ m $ to reduce further. This is decreasing the dimension to the process terminates, hence the matrix can be put into row reduced echelon form.\qed
\begin{remark}
  Putting a matrix into RRE form preserves the row-space and the RRE of any matrix is unique. Also if $ A $ is a square matrix then $ A $ either has a zero row or is the identity. 
\end{remark}
\begin{theorem}
	For $ A\in M_{m\times n}(\F) $ the following are equivalent:
	\begin{enumerate}
		\item $ \rk (A)=n $.
		\item $ A $ is a product of elementary matrices.
		\item $ A $ is invertiable.
	\end{enumerate}
\end{theorem}
\pf Let's prove that (i) $ \implies $ (ii). By the proposition there exists elementary matrices $ E_i $ such that $ E_1\dots E_\ell A $ is in RRE form. By the remark this is $ I_n $ hence $ A=\inv E_\ell\cdots \inv E_1 $ which are also elementary. For (ii) $ \implies $ (iii) elementary matrix lie in $ GL_n(\F) $ which is a group, hence closed. Finially for (iii) $ \implies $ (i) suppose there exists $ B\in M_{m\times n}(\F) $ such that $ AB=I_n $. Then for $ v\in \F^n $ we have that $ v=(AB)v=A(Bv) $, so $ v\in \ima A $.
\section{Determinant and Traces}
\subsection{Determinant}
\begin{theorem}
	There exists a unique function $ F:M_{m\times n}\to \F $ satisfying
	\begin{enumerate}
		\item (Alternating) If $ c_i=c_j $ for some $ i\ne j $ then $ F(A)=0 $.
		\item (Multilinear in columns) For all $ 1\le i\le n $ and $ v_j\in \F^n $ the function 
			\begin{align*}
				\F^n&\to \F\\
				v&\to F(v_1\mid \cdots \mid v_{j-1} \mid v \mid v_{j+1} \mid \cdots \mid v_n )
			\end{align*}
			is linear.
		\item $ F(I_n)=1 $.
	\end{enumerate}
\end{theorem}
\begin{definition}
	(Determinant) We shall defined the $ F $ in the previous theorem as the $ n $\textit{-dimensional determinant}, written as $ F(A)=\det(A) $. A function satisfying conditions (i) and (ii) of the theorem is called an $ n $-\textit{-dimensional volumn form}.
\end{definition}

\begin{lemma}
	If $ F $ is an $ n $-dimensional volumn form, $ A\in M_{m\times n}(\F) $,
	\begin{enumerate}
		\item If $ A $ has a zero column then $ F(A)=0 $,
		\item $ F(AT_{ij})=-F(A) $,
		\item $ F(AM_{i,\lambda})=\lambda F(A) $,
		\item $ F(AC_{i,j,\lambda})=F(A) $.
	\end{enumerate}
\end{lemma}
\pf Let $ f_i:\F^n \to \F $ be given by $ v\to F(c_1\mid \cdots c_{i-1} \mid v\mid c_{i+1}\mid \cdots \mid c_n) $. So that $ f_i $ is linear. Then $ f_i(c_j)=\delta_{ij}F(A) $. iF $ c_i=0 $ then $ F(A)=f_i(c)=f_i(0)=0 $. For (ii), let $ \bar A $ be the matrix obtained from $ A $ by replacing both $ i $th and $ j $th columns of $ A $ by $ c_i+c_j $. Then $ 0=F(\bar A)=F(A)+f_i(c_j)+f_i(c_i)+F(AT_{ij}) $. For (iii), $ F(AM_{i,\lambda})=f_i(\lambda c_i)=\lambda f_i(c_i)=\lambda F(A) $. Now for (iv), $ F(AC_{i,j,\lambda})=f_j(c_j+\lambda c_i)=f_i(c_j)+\lambda f_j(c_i)=F(A) $.\qed\par
Now we're ready to prove the theorem.\\
\pf First we'll prove uniqueness. Let $ F $ be an $ n $-dimensional volumn form with $ F(I_n)=1 $. By the lemma, $ F(T_{i,j})=-1, F(M_{i,\lambda})=\lambda, F(C_{i,j,\lambda})=1, F(AE)=F(A)F(E) $ for $ E $ elementary. Let $ A\in M_{n\times n}(\F) $, so there exists elementary matrices $ E_1,\dots, E_\ell $ such that $ A'=AE_1\cdots E_\ell $ with $ A' $ in CRE form. Then $ F(A)=F(A')\inv {F(E_1)}\cdots \inv{F(E_\ell)} $, so either $ A'=I_n $ so $ F(A)=\inv{(F(E_1)}\cdots \inv{F(E_\ell)} $ or $ A' $ has a zero colun so by the lemma, $ F(A)=F(A')=0 $.\par
This also proves the corollary.
\begin{corollary}
  $ \det A\ne 0 $ if and only if $ A $ is invertible. In this case, $ A=E_1\cdots E_\ell $ then $ \det A=\det(E_1)\cdots \det(E_\ell) $.
\end{corollary}
Recall that from IA Groups that $ \sgn:S_n\to \{\pm 1 \} $ is the unique homomorphism satisfying $ \sgn(\tau)=-1 $ for all transpositions. Now we can define the determinant.
\begin{align*}
	\det:M_{n\times n}(\F)&\to \F \\
	a&\to \sum_{\sigma\in S_n}\sgn(\sigma)\prod_{i=1}^na_{\sigma(i)i}
\end{align*}
Clearly we have that $ \det I_n=1 $. Each product $ \prod_{i=1}^na_{\sigma(i)i} $ is multilinear in columns. Hence so is $ \det A $. Suppose that $ c_k=c_\ell $ for $ k\ne \ell $. Set $ \tau=(k\ \ell) $ so that $ a_{ij}=a_{i\tau(j)} $ for all $ i,j $. Then
\begin{align*}
	\det A&=\sum_{\sigma\in A_n}\sgn(\sigma)\prod_{i=1}^n a_{\sigma(i)i}+\sum_{\sigma\in A_n}\sgn(\sigma\tau)\prod_{i=1}^n a_{\sigma\tau(i)i}\\
	      &= \sum_{\sigma\in A_n}\prod_{i=1}^na_{\sigma(i)i}-\sum_{\sigma\in A_n}\prod_{i=1}^n a_{\sigma\tau(i)i}\\
	      &=\sum_\sigma\prod_ia_{\sigma(i)i}-\sum_\sigma\prod_ia_{\sigma\tau(i)\tau(i)}\\
	      &=\sum_\sigma\prod_ia_{\sigma(i)i}-\sum_\sigma\prod_ja_{\sigma(j)j}
\end{align*}\qed\par
Now we will observe some properties of the determinant.
\begin{lemma}
	For $ A\in M_{n\times n}(\F) $, we have that $ \det(A^T)=\det(A) $.
\end{lemma}
\pf 
\begin{align*}
	\det(A^T)&=\sum_{\sigma\in S_n}\sgn(\sigma)\prod_{i=1}^na_{i\sigma(i)}\\
		 &= \sum_{\sigma\in S_n}\sgn(\sigma)\prod_ja_{\inv{\sigma(j)}j}\\
		 &=\sum_{\sigma\in S_n}\sgn(\inv\sigma)\prod_ja_{\inv{\sigma(j)}j}\qed
\end{align*}
\begin{proposition}
	For all $ A,B\in M_{n\times n}(\F) $ we have that $ \det(AB)=\det(A)\det(B) $.
\end{proposition}
\pf Recall that $ \rk(AB)\le \min(\rk(A),\rk(b)) $, so if either $ A $ or $ B $ has rank less than $ n $ then so does $ AB $ so by the corollary above $ \det(AB)=0=\det(A)\det(B) $. If not, $ A $ and $ B $ are invertible and can be written as a product of elementary matrices, so $ A=E_1\cdots E_\ell $, $ B+E_1'\cdots E_k' $. So $ AB=E_1\cdots E_\ell E_1'\cdots E_k' $, hence $ \det(AB)=\det(E_1)\cdots \det(E_\ell)\det(E_1')\cdots \det(E_k')=\det(A)\det(B)$.\qed\par
Missed a lecture - 31.11.25
\begin{definition}
	(Trace) For a $ A\in M_{m\times n}(\F) $ the \textit{trace} of $ A $ is given by
	\[
		\tr(A)=\sum_{i=1}^na_{i,i}
	\]
\end{definition}
\begin{remark}
	$ \tr \in \mathcal L(M_{m\times n}(\F),\F) $
\end{remark}
\begin{lemma}
	For all $ A,B\in M_{m\times n}(\F) $ we have that
	\[
	  \tr(AB)=\tr(BA)
	\]
\end{lemma}
\pf 
\begin{align*}
	\tr(AB)&=\sum_{i=1}^n (AB)_{i,i}\\
	&= \sum_{i=1}^n\sum_{j=1}^na_{ij}b_{ji}\\
	&= \sum_{j=1}^n\sum_{i=1}^nb_{ji}a_{ij}\\
	&= \sum_{j=1}^n(BA)_{j,j}=\tr(BA)\qed
\end{align*}
But in general we don't have that $ \tr(AB)=\tr(A)\tr(B) $.
\begin{corollary}
  Similar matrices have the same trace.
\end{corollary}
\pf For $ P\in GL_n(\F) $,
\[
  \tr(PA\inv P)=\tr(\inv P(PA))=\tr(A)
\]
\begin{definition}
	For $ V $ a finite dimensional vector space and $ \alpha\in\mathcal L(V,V) $ define the \textit{trace} of $ \alpha $ by
	\[
		\tr(\alpha)=\tr([\alpha]_B^B)
	\]
	for $ B $ a basis of $ V $.
\end{definition}
\begin{proposition}
  This is independent of the basis $ B $.
\end{proposition}
\pf If $ B' $ is another basis of the vector space $ V $ then, $ [\alpha]_{B'}^{B'} $ and $ [\alpha]_B^B $ are similar matrices, so the result follows from the corollary.
\section{Dual Spaces}
\begin{definition}
	(Dual space) If $ V $ is a $ \F $-vector space, then the \textit{dual space} of $ V $ is
	\[
		V^*=\mathcal L(V,\F)=\{\theta:V\to\F: \theta\text{ is linear }\}
	\]
\end{definition}
For example we have that $ \theta:\R^3\to \R $ given by
\[
  \theta\left(\begin{pmatrix}
    x \\ y \\ z
  \end{pmatrix}\right) = x-2y+3z
\]
is an element in the dual space of $ \R^3 $.\par
$ \tr\in M_{n\times n}(\F)^* $\par
If $ V=C[0,1] $ then $ \theta:V\to R $ given by $ \theta(f)=\int_0^1f(t)e^{-t}\mathrm dt $ is in the dual space of $ V^* $.\par
An element $ \theta\in V^* $ is called a \textit{linear functional} on $ V $.\par
Suppose that $ B $ is a basis for $ V $. For $ b\in B $ define $ b^*\in V^* $ by
\[
	b^*\left(\sum_{c\in B}\lambda_cc\right)=\lambda_b
\]
i.e. $ b^*(c)=\delta_{bc} $. If we let $B^*=\{b^*:b\in B\} $ then:
\begin{proposition}
  For $ B^* $ defined above,
  \begin{enumerate}
	  \item $ B^* $ is linearly independent;
	  \item If $ V $ is finite dimensional then $ B^* $ is a basis for $ V^* $.
  \end{enumerate}
\end{proposition}
\begin{definition}
	(Dual basis) If $ V $ is finite dimensional, call $ B^* $ the \textit{dual basis} to $ B $.
\end{definition}
\pf Suppose that
\[
	\sum_{b\in B}\lambda_bb^*=0\quad\text{in } V^*.
\]
Then for $ c\in B $,
\[
	0=\left(\sum_{b\in B}\lambda_bb^*\right)(c)=\sum_{b\in B}\lambda_bb^*(c)=\lambda_c
\]
so $ \lambda_c=0 $ hence all coefficients are zero, so the set $ B^* $ is linearly independent.\\
For $ V,W $ finite dimensional we know that $ \dim(\mathcal L(V,W))=\dim(V)\dim(W) $ so for $ \dim(V^*)=\dim(\mathcal L(V,\F))=\dim V $. We know that $ B^* $ is a linearly independent subset of $ V $ of size $ \dim V $ hence it is a basis of $ V^* $.\qed\par
We can also offer a constructive proof of (ii)\par
Given $ \theta\in V^* $ and $ b\in B $ set $ \lambda_b=\theta(b)\in \F $ and let $ \theta=\sum_{b\in B}\lambda_bb^*\in V^* $. Then $ \bar\theta\in \langle B^*\rangle $ and for $ c\in B $ we have that
\[
  \bar\theta(c)=\lambda_c=\theta(c)
\]
so $ \theta=\bar\theta $ as they agree on a basis and hence we have that $ \theta\in\langle B^*\rangle $ so $ B^* $ spans $ V^* $.\par
However there are vector spaces, not finite dimensional such as $ P(\R)^*=\R^{\N} $.
\begin{corollary}
  For $ V $ finite dimensional $ V\cong V^* $
\end{corollary}
\pf Same dimension hence isomorphic.
Note that if $ V $ is finite dimensional, $ B=\{v_1,\dots, v_n\} $ is a basis of $ V $, for $ v\in V $ and $ \theta\in V^* $ we can write,
\[
	v=\sum_{i=1}^n\lambda_iv_i,\quad \theta=\sum_{j=1}^n\mu_jv_j^*.
\]
Then $ \theta(v) = \sum_{i,j}\lambda_i\mu_jv_j^*(v_i)=\sum_{i=1}^n\lambda_i\mu_i=([\theta]_{B*})^T\cdot [v]_B $
\begin{definition}
	(Annihilator) For $ V $ a finite dimensional $ \F $-vector space and $ S\subseteq V $, the \textit{annihilator} of $ S $ is
	\[
		S^0=\{\theta\in V^*:\forall s\in S,\theta(s)=0\}\subseteq V^*
	\]
\end{definition}
\begin{lemma}
  For and $ S,T\subseteq V $,
  \begin{enumerate}
	  \item $ S^0 \le V^* $;
	  \item If $ S\subseteq T $ then $ T^0\le S^0 $;
	  \item $ S^0=\langle S\rangle^0 $;
	  \item $ V^0=\{\mathbf 0_{V^*}\} $ and $ \{\mathbf 0_V\}^0=V^* $.
  \end{enumerate}
\end{lemma}
\pf A simple application of the subspace test proves (i). For (ii) is suffices to check that $ T^0\subseteq S^0 $. For $ \theta\in T^0 , s\in S	$ so $ s\in T $ hence $ \theta(s)=0 $. For (iii), $ S\subseteq \langle S\rangle $ so by (ii)
\[
  \langle S\rangle^0\le S^0.
\]
For the converse let $ \theta\in S^0 $ and $ v\in \langle S\rangle $. So we can write $ v=\sum_{s\in S}\lambda_s\cdot s $, so $ \theta(v)=\sum_{s\in S}\lamba_s\theta(s)=0 $, hence $ \theta\in\langle S\rangle^0 $ so $ S^0=\langle S\rangle^0 $. If $ \theta\in V^* $ and $ \forall v\in V $ we have that $ \theta(v)=0 $ then $ \theta $ must be the zero function, so $ V^0=\{\mathbf 0_{V^*}\} $. Secondly for $ \theta\in V^* $ we have that $ \theta(\mathbf 0_V)=0 $ so $ \{\mathbf 0_V\}^0=V^* $.\qed
\begin{proposition}
  For $ V $ finite dimensional with $ U\le V $, we have that
  \[
    \dim V=\dim U + \dim U^0
  \]
\end{proposition}
\pf Suppose that $ \dim V=n $ and $ \dim U=k $ and let $ B_U=\{v_1,\dots, v_k\} $ be a basis for $ U $ and extend to a bassi $ B_V=\{v_1,\dots, v_n\} $ for $ V $. Then $ B_v^*=\{v_1^*,\dots, v_n^*\} $ is a basis for $ V^* $. Suffices to prove the following claim.
\begin{claim}
	$ \{v^*_{k+1},\dots, v_n^* \}$ forms a basis for $ U^0 $
\end{claim} 
First we show that it's a subset of $ U^0 $. For $ i\le k $ and $ j\ge k+1 $,
\[
  v_j^*(v_i)=0
\]
so
\[
  v_j^*\in (B_U)^0=\langle B_u\rangle^0 = U^0.
\]
Linear indepedence is obvious since it's a subspace of $ B_V^* $. Let's check it's spanning. Let $ \theta\in U^0 $, so write $ \theta=\sum_{j=1}^n\lambda_jv_j^* $. Then for $ i\le k $, $ v_i\in U $, so $ 0=\theta(v_i)=\sum_j\lambda_jv_j^*(v_i)=\lambda_i $. Hence $ \theta=\sum_{j=k+1}^n\lambda_jv_j^*\in\langle v_{k+1}^*,\dots, v_n^*\rangle $.\qed
\begin{remark}
  If $ U,W\le V $ which are such that $ V=U\oplus W $ hence $ U^0\cong W^* $ is really what's going on behind the scenes.
\end{remark}








\end{document}


