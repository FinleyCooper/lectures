\documentclass{article}
\usepackage{../header}
\title{Linear Algebra}
\author{Notes by Finley Cooper}
\newcommand{\F}{\mathbb{F}}
\newcommand{\n}{\mathrm{n}}
\newcommand{\rk}{\mathrm{rk}}
\newcommand{\Col}{\mathrm{Col}}
\newcommand{\Row}{\mathrm{Row}}
\newcommand{\sgn}{\mathrm{sgn}}
\newcommand{\id}{\mathrm{id}}
\begin{document}
  \maketitle
  \newpage
  \tableofcontents
  \newpage
  \section{Vector Spaces}
  \subsection{Definitions}
  For this lecture course, $ \F $ will always be field.
  \begin{definition}
	  (Vector Space) A $ \F $\textit{-vector space} (or a vector space over $ \F $) is an abelian group $ (V,+,\boldsymbol 0) $ equipped with a function
	  \begin{align*}
	    \F\times V\to V \\
	    (\lambda, v)\to \lamda v
	  \end{align*}
	  which we call scalar multiplication such that $ \forall v,w\in V, \forall \lambda,\mu\in\F $
	  \begin{enumerate} 
		  \item $ (\lambda + \mu)v=\lambda v + \mu v $
		  \item $ \lambda(v + w)=\lambda v + \lambda w $
		  \item $ \lambda(\mu v)=(\lambda \mu)v $
		  \item $ 1\cdot v = v\cdot 1 = v $
	  \end{enumerate}
  \end{definition}
  Remember that $ \boldsymbol 0 $ and $ 0 $ are not the same thing. $ 0 $ is an element in the field $ \F $ and $ \boldsymbol 0 $ is the additive identity in $ V $.\par
  For an example consider $ \F^n $ n-dimensional column vectors with entries in $ \F $. We also have the example of a vector space $ \C^n $ which is a complex vector space, but also a real vector space (taking either $ \C $ or $ \R $ as the underlying scalar field).\par
  We also can see that $ M_{m\times n}(\F) $ form a vector space with $ m $ rows and $ n $ columns.\\
  For any non-empty set $ X $, we denote $ \F^X $ as the space of functions from $ X $ to $ \F $ equipped with operations such that:
\begin{align*}
	f+g \text{ is given by } (f+g)(x)=f(x)+g(x)\\
	\lambda f \text{ is given by } (\lambda f)(x)=\lambda f(x)
\end{align*}
\begin{proposition}
  For all $ v\in V $ we have that $ 0\cdot v = \boldsymbol 0 $ and $ (-1)\cdot v=-v $ where $ -v $ denotes the additive inverse of $ v $.
\end{proposition}
\pf Trivial.
\begin{definition}
	(Subspace) A \textit{subspace} of a $ \F $-vector space $ V $ is a subset $ U\subseteq V $ which is a $ \F $-vector space itself under the same operations as $ V $. Equivalently, $ (U,+) $ is a subgroup of $ (V,+) $ and $ \forall \lambda\in \F $, $\forall u\in U $ we have that $ \lambda u \in U $.
\end{definition}
\begin{remark}
  Axioms (i)-(iv) are always automatically inherited into all subspaces.
\end{remark}
\begin{proposition}
	(Subspace test) Let $ V $ be a $ \F $-vector space and $ U\subseteq V $ then $ U $ is a subspace of $ V $ if and only if,
	\begin{enumerate}
		\item $ U $ is nonempty.
		\item $ \forall \lambda\in\F $ and $ \forall u,w\in U $ we have that $ u+\lambda w \in U $.
	\end{enumerate}
\end{proposition}
\pf If $ U $ is a subspace then $ U $ satisfies (i) and (ii) since it contains $ 0 $ and is closed. Conversely suppose that $ U\subseteq V $ satisfies (i) and (ii). Taking $ \lambda = -1 $ so $ \forall u,w\in V $, $ u-w\in U $ hence $ (U,+) $ is a subgroup of $ (V,+) $ by the subgroup test. Finally taking $ u=\boldsymbol 0 $ so we have that $ \forall w\in U,\forall\lambda\in \F $ we have that $ \lambda w\in U $. So $ U $ is a subspace of $ V $.\qed\par
We notate $ U $ by $ U\le V $.\par
For some examples
\begin{enumerate}
	\item \[
	\left\{\begin{pmatrix}
			x \\
			y \\
			z \\
	\end{pmatrix}\in \R^3:x+y+z=t\right\}\subseteq \R^3, 
\]
for fixed $ t\in \R $ is a subspace of $ \R^3 $ iff $ t = 0 $.\par
\item Take $ \R^\R $ as all the functions from $ \R $ to $ \R $ then the set of continuous functions is a subspace.
\item Also we have that $ C^\infty(\R) $, the set of infintely differentiable functions from $ \R $ to $ \R $ is a subspace of $ \R^\R $ and the subspace of continuous functions.
\item A further subspace of all of those subspaces is the set of polynomial functions.
\end{enumerate}
\begin{lemma}
  For $ U,W\le V $ we have that $ U\cap W\le V $.
\end{lemma}
\pf We'll use the subspace test. Both $ U,W $ are subspaces so they contain $ \boldsymbol 0 $ hence $ \boldsymbol 0\in U\cap W $ so $ U\cap W $ is nonempty. Secondly take $ x,y\in U\cap W $ with $ \lambda \in \F $. Then $ U\le V $ and $ x,y\in U $ so $ x+\lambda y\in U $. Similarly with $ W $ so $ x+\lambda y \in W $ hence we have that $ x+\lambda y \in U \cap W $ hence $ U\cap W\le V $\qed
\begin{remark}
  This does not apply for subspaces, in fact from IA Groups, we know it doesn't even hold for the underlying abelian group.
\end{remark}
\begin{definition}
	(Subspace sum) For $ U,W\le V $, the \textit{subspace sum} of $ U, W $ is
	\[
		U+W=\{u+w:u\in U, w\in W\}.
	\]
\end{definition}
\begin{lemma}
  If $ U, W\le V $ then $ U+W\le V $.
\end{lemma}
\pf Simple application of the subspace test.
\begin{remark}
  $ U+W $ is the smallest subgroup of $ U, W $ in terms of inclusion, i.e. if $ K $ is such that $ U\subseteq K $ and $ W\subseteq K $ then $ U+W\subseteq K $.
\end{remark}
\subsection{Linear maps, isomorphisms, and quotients}
\begin{definition}
	(Linear map) For $ V $, $ W $ $ \F $-vector spaces. A \textit{linear map} from $ V $ to $ W $ is a group homomorphism, $ \varphi $, from $ (V,+) $ to $ (W,+) $ such that $ \forall v\in V $
	\[
	  \varphi(\lambda v) = \lambda\varphi(v)
	\]
\end{definition}
Equivalently to show any function $\alpha: V\to W $ is a linear map we just need to show that $ \forall u,w\in V $, $ \forall \lambda \in\F $ we have
\[
  \alpha(u+\lambda w)=\alpha(u)+\lambda\alpha(w).
\]
For some examples of linear maps
\begin{enumerate}
	\item $ V=\F^n, W=\F^m $ $ A\in M_{m\times n}(\F) $. Then let $ \alpha:V\to W $ be given by $ \alpha(v)=Av $. Then $ \alpha $ is linear.
	\item $ \alpha:C^\infty(\R)\to C^\infty(\R) $ defined by taking the derivative.
	\item $ \alpha: C(\R)\to \R $ defined by taking the integral from $ 0 $ to $ 1 $.
	\item $ X $ any nonempty  set, $ x_0\in X $,
		\begin{align*}
		  \alpha:\F^X\to \F \\
		  f\to f(x_0)
		\end{align*}
	\item For any $ V,W $ the identity mapping from $ V $ to $ V $ is linear and so is the zero map from $ V $ to $ W $.
	\item The composition of two linear maps is linear.
	\item For a non-example squaring in $ \R $ is not linear. Similiarly adding constants is not linear, since linear maps preserve the zero vector.
\end{enumerate}
\begin{definition}
	(Isomorphism) A linear map $ \alpha:V\to W $ is an \textit{isomorphism} if it is bijective.\par
	We say that $ V $ and $ W $ are isomorphic, if there exists an isomorphism from $ V\to W $ and denote this by $ V\cong W $.
\end{definition}
An example is the vector space $ V=\F^4 $ and $ W=M_{2\times 2}(\F) $ we can define the map
\begin{align*}
	\alpha:V &\to W\\
	        \begin{pmatrix}
	         a\\b\\c\\d
	       \end{pmatrix}
		 &\to \begin{pmatrix}
		       a & b \\
		       c & d 
	       \end{pmatrix}
\end{align*}
Then $ \alpha $ is an isomorphism.
\begin{proposition}
  If $ \alpha: V\to W $ is an isomorphism then $ \inv\alpha:W\to V $ is also an isomorphism.
\end{proposition}
\pf Clearly $ \inv\alpha $ is a bijection. We need to prove that $ \inv\alpha $ is linear. Take $ w_1,w_2\in W $ and $ \lambda\in \F $. So we can write $ w_i=\alpha(v_i) $ for $ i=1,2 $. Then \[ \inv\alpha(w_1+\lambda w_2)=\inv \alpha(\alpha(v_1)+\lambda\alpha(v_2))=\inv\alpha(\alpha(v_1+\lambda v_2))=v_1+\lambda v_2=\inv \alpha(w_1)+\lambda\inv \alpha(w_2) \]. Hence $ \inv\alpha $ is linear, so $ \inv\alpha $ is an isomorphism.\qed 
\begin{definition}
	(Kernal) Let $ V,W $ be $ \F $-vector spaces. Then the \textit{kernel} of the linear map $ \alpha:V\to W $ is
	\[
		\ker(\alpha)=\{v\in V:\alpha(v)=\mathbf 0_W\}\subseteq V
	\]
\end{definition}
\begin{definition}
	(Image) Let $ V,W $ be $ \F $-vector spaces. Then the \textit{image} of a linear map $ \alpha:V\to W $ is
	\[
		\ima(\alpha)=\{\alpha(v):v\in V\}\subseteq W
	\]
\end{definition}
\begin{lemma}
  For a linear map $ \alpha:V\to W $ the following hold.
  \begin{enumerate}
	  \item $ \ker\alpha\le V $ and $ \ima \alpha \le W $
	  \item $ \alpha $ is surjective if and only if $ \ima \alpha =W $
	  \item $ \alpha $ is injective if and only if $ \ker\alpha=\{\mathbf 0_V\} $
  \end{enumerate}
\end{lemma}
\pf $ \mathbf 0_V+\mathbf 0_V=\mathbf 0_V $, so applying $ \alpha $ to both sides any using the fact that $ \alpha $ is linear gives that $ \alpha(\mathbf 0_V)=\mathbf 0_W $. So $ \ker\alpha $ is nonempty. The rest of the proof is a simple application of the subspace test.\\
The second statement is immediate from the definition.\\
For the final statement suppose $ \alpha $ injective. Suppose $ v\in \ker \alpha $. Then $ \alpha(v)=\mathbf 0_W=\alpha(\mathbf 0_w) $ so $ v=\mathbf 0_V $ by injectivity. Hence $ \ker\alpha $ is trivial.
Conversely suppose that $ \ker \alpha=\{0_V\} $ Let $ u,v\in V $ and suppose that $ \alpha(u)=\alpha(v) $. The $ \alpha(u-v)=\mathbf 0_W $, so $ u-v\in\ker\alpha $, so $ u=v $.\qed\par

For $ V $ a $ \F $-vector space, $ W\le V $ write
	\[
		\frac VW=\{v+W:v\in V\}
	\]
	as the left cosets of $ W $ in $ V $. Recall that two cosets $ v + V $ and $ u+W $ are the same coset if and only if $ v-u\in W $.
\begin{proposition}
  $ V/W $ is an $ \F $-vector space under operations
  \begin{align*}
   (u+W)+(v+W)&=(u+v)+W \\
   \lambda (v+W)&=(\lambda v)+W
\end{align*}
We call $ V/W $ the quotient space of $ V $ by $ W $.
\end{proposition}
\pf The proof is long and requires a lot of vector space axioms so we'll just sketch out the proof.\\
We check that operations are well-defined, so for $ u,\overline u, v,\overline v \in V $ and $ \lambda\in \F $ if
\[
  u+W=\overline u + W,\quad v+W=\overline v + W
\]
then
\[
	(u+v)+W=(\overline u+\overline w)+W
\]
and
\[
	(\lambda u) + W = (\lambda\overline u)+W
\]
The vector space axioms are inherited from $ V $.\qed
\begin{proposition}
	(Quotient map) The function $ \pi_W: V\to \frac VW $ called a \textit{quotient map} is given by
  \[
    \pi_W(v)=v+W
  \]
  is a well-defined, surjective, linear map with $ \ker\pi_W=W $.
\end{proposition}
\pf Surjectivity is clear. For linearity let $ u,v \in V $ and $ \lambda\in \F $. Then 
\begin{align*}
	\pi_W(u+\lambda v)&=(u+\lambda v) + W \\
			  &= (u+W)+(\lambda v+W)\\
			  &= (u+W)+\lambda(v+W) \\
			  &= \pi_W(u)+\lambda\pi_W(v)
\end{align*}
For $ v\in V $, we have that  $ v\in\ker \pi_W \iff \pi_W(v) = \mathbf 0_{V/W} $. So $ v+W=\mathbf 0_V+W $ so finally $ v=v-\mathbf 0_V\in W $.\qed
\begin{theorem}
	(First isomorphism theorem) Let $ V,W $ be $ \F $-vector spaces and $ \alpha:V\to W  $ linear. Then there is an isomorphism
	\[
		\overline\alpha:\frac{V}{\ker \alpha}\to\ima\alpha
	\]
	given by $ \overline\alpha(v+\ker\alpha)=\alpha(v) $
\end{theorem}
\pf For $ u,v\in V $,
\begin{align*}
	u+K=v=K &\iff u-v\in K
		&\iff \alpha(u-v)=\mathbf 0_W
		&\iff \alpha(u)=\alpha(v)
		&\iff \overline \alpha(u+\ker\alpha)=\overline\alpha(v+\ker\alpha)
\end{align*}
The forward direction shows that $ \overline\alpha $ is well-defined, and the converse shows that $ \overline\alpha $ is injective. For surjectivity given $ w\in\ima\alpha $, there exists some $ v\in V \st w=\alpha(v)$. Then $ w=\overline\alpha(v+\ker\alpha) $.\\
Finally for linearity given $ u,v\in V $, $ \lambda\in\F $,
\begin{align*}
	\overline\alpha((u+\ker\alpha)+\lambda(v+\ker\alpha))&=\overline\alpha((u+\lambda v)+\ker\alpha)\\
  &= \alpha(u+\lambda v)\\
  &= \alpha(u)+\lambda\alpha(v) \\
  &= \overline\alpha(u+\ker\alpha)+\lambda\overline\alpha(v+\ker\alpha)
\end{align*}
So $ \overline \alpha$ is linear hence is an isomorphism\qed
\subsection{Basis}
\begin{definition}
	(Span) Let $ V $ be a $ \F $-vector space. Then the \textit{span} of some subset $ S\subseteq V $ is
	\[
		\langle S\rangle = \left\{\sum_{s\in S}\lambda_s\cdot s: \lambda_s\in \F\right\}
	\]
	where $ \sum $ denotes finite sums. An expression the form above is called a \textit{linear combination} of $ S $.\\
	We say that $ S $ \textit{spans} $ V $ if $ \langle S\rangle =V $
\end{definition}
\begin{definition}
	(Finite-dimensional) For a vector space $ V $ we say that it is \textit{finite-dimensional} if there exists a finite spanning set.
\end{definition}
We'll give some simple remarks without proof.
  \begin{enumerate}
	  \item $ \langle S \rangle\le V $ and conversely if $ W\le V $ and $ S\subseteq W $ then $ \langle S\rangle \le W $.
	  \item If $ S,T\subseteq W $ and $ S $ spans $ V $ and $ S\subseteq \langle V\rangle $ then $ T $ spans $ V $.
	  \item By convention $ \langle\emptyset\rangle =\{\mathbf 0_V\} $.
	  \item $ \langle S\cup T\rangle = \langle S\rangle + \langle T\rangle $
  \end{enumerate}
For an example consider $ V=\R^3 $ and consider the sets
\begin{align*}
  S=\left\{\begin{pmatrix}
    1\\0\\0
  \end{pmatrix},\begin{pmatrix}
    1\\1\\2
\end{pmatrix}\right\}\\
T=\left\{\begin{pmatrix}
  2\\1\\2
\end{pmatrix},\begin{pmatrix}
  0\\1\\2
\end{pmatrix},\begin{pmatrix}
  -1\\2\\4
\end{pmatrix}\right\}
\end{align*}
Then $ \langle S\rangle = \langle T\rangle =\left\{\begin{pmatrix}
  x\\y\\2y
\end{pmatrix}:x,y\in \R\right\} \le \R^3.$\par
For a second example consider $ V=\R^\N $ and set $ T=\{\delta_n:n\in \N\} $. This is not a spanning set, since we require infinitely many elements from $ T $ to make an element in $ V $. In fact we can write that
\[
	\langle T\rangle =\{f\in \R^\N:f(n)=0\text{ for all but finitely many terms}\}.
\]
\begin{definition}
	(Linear Independence) A subset $ S\subseteq V $ is called \textit{linearly independent} if, for all finite linear combinations
\[
	\sum_{s\in S}\lambda_ss\quad \text{of S}
\]
if the sum is the zero vector in $ V $ the $ \lambda_s=0 $ for all $ s\in S $.
\end{definition}
If $ S $ is not linearly indepedent we say that $ S $ is linearly dependent.\par
We'll make some more remarks
\begin{enumerate}
	\item If $ \mathbf 0 \in S $ then $ S $ is not linearly independent.
	\item If we have a finite set, then to show linearly independent, we only need to consider the linear combination of all elements, not all finite lienar combinations.
	\item However is $ S $ is infinite, then we have to consider every possible finite subset of $ S $ and show it's linearly independent.
	\item Every subset of a linearly independent set is itself linearly indepedent.	
\end{enumerate}
\begin{definition}
	(Basis) A subset $ S\subseteq V $ is a \textit{basis} for $ V $ if $ S $ is linearly independent and a spanning set.
\end{definition}
For an example consider $ e_i\in \F^n $ be given by
\[
  e_i=\begin{pmatrix}
    0\\ \vdots \\ 0 \\ 1 \\ 0 \\\vdots \\ 0
\end{pmatrix}\quad \text{with the 1 in the } i\text{th entry}
\]
then the set $ \{e_i:1\le i\le n\} $ is the standard basis for $ \F^n $.\par
For $ P(\R) $ the set of real polynomial functions and let $ p_n\in P(\R) $ be given by $ p_n(x)=x^n $, then $ \{p_n:n\in\Z_{\ge 0}\} $ is a basis for $ P(\R) $.
\begin{proposition}
  If $ S\subseteq V $ is a finite spanning set, then there exists a subset $ S'\subseteq S $ such that $ S' $ is a basis.
\end{proposition}
\pf If $ S $ is linearly independent then we're done. Otherwise write $ S=\{v_1,\dots, v_n\} $. Then there exists $ \lambda_1,\dots, \lambda_n $ such that $ \lambda_1v_1+\cdots\lambda_nv_n=\mathbf 0 $ wlog suppose that $ \lambda_n $ is nonzero. Then
\begin{align*}
	v_n=-\frac 1{\lambda_n}\sum_{i=1}^{n-1}\lambda_iv_i
\end{align*}
so $ v_n $ is in the span of the other vectors. Hence $ S\setminus \{v_n\} $ is still a spanning set. Repeat which the set is linearly independent, must terminate since the set is finite and the empty set is not a spanning set.\qed
\begin{corollary}
  Every finite-dimensional vector space has a finite basis.
\end{corollary}
\pf Trivial application of the proposition\qed
\begin{theorem}
	(Steinitz Exchange Lemma) Let $ S,T\subseteq V $ finite with $ S $ linearly independent and $ T $ a spanning set of $ V $. Then
	\begin{enumerate}
		\item $ |S|\le |T| $,
		\item and there exists $ T'\subseteq T $ which has size $ |T'|=|T|-|S| $ and $ S\cup T' $ spans $ V $.
	\end{enumerate}
\end{theorem}
\pf To come later...\par
Let's look at some consequences of the lemma first.
\begin{corollary}
	For a finite-dimensional vector space $ V $,
  \begin{enumerate}
	  \item Every basis for $ V $ is finite.
	  \item All finite basis have the same size.
  \end{enumerate}
\end{corollary}
\pf $ V $ has a finite basis $ B $, suppose we have some other basis $ B' $ infinite. Let $ B''\subseteq B' $ with $ |B''|=|B|+1 $ then $ |B''| $ is linearly independent, so applying (i) of the Steinitz exchange lemma with $ S=B'' $ and $ T=B $ we get a contradiction.\par
For the second part, let $ B_1,B_2 $ be finite basis for $ V $ then apply Steinitz symmetrically since both are spanning set and linearly independent, so we get that $ |B_1|\ge |B_2| $ and $ |B_1|\ge |B_2| $ so $ |B_1|=|B_2| $.\qed
\begin{definition}
	(Dimension) For a vector space $ V $ the \textit{dimension} of $ V $ is the size of any basis. We write this as \dim V.
\end{definition}
This definition is well-defined by the previous corollary.\par
For an example $ \dim \F^n=n $ since we've shown the standard basis has size $ n $. As a complex vector space $ \C $ is one-dimensional as a complex vector space and two-dimension as a real vector space, with basis $ \{1\} $ and $ \{1,i\} $ repectively.
\begin{corollary}
  For a vector space $ V $ let $ S,T\subseteq V $ finite, with $ S $ linearly independent and $ T $ a spanning set, then
  \begin{align*}
	  |S|\le \dim V\le |T|
	  \end{align*}
	  with equality if and only if $ S $ spans or $ V $ is linearly independent respectively.
\end{corollary}
\pf The inequalities are immediate from Steinitz. If $ S $ is a basis then $ |S|=\dim V $ from the previous corollary. Conversely if $ |S|=\dim V $ and let $ B $ be a basis for $ V $ so we have that $ |B|=|S| $ so $ B $ is a spanning set. So we can apply Steinitz (ii) to $ B $ so there exists $ B'\subseteq B $ with $ |B'|=|B|-|S|=0 $ and $ S\cup B'=S\cup \emptyset $ spans $ V $. So $ S $ is a basis. Similiar we have a very similar proof for equality in $ V $.\qed\par We will not prove that every vector space has a basis, however some non-finitely dimensional vector spaces have an infinite basis, for example $ P(\R) $.
\begin{proposition}
  If $ V $ is a finite-dimensional vector space, then if $ U\le V $ then $ U $ is finite-dimensional, namely, $ \dim U \le \dim V $ with equality if and only if $ U=V $.
\end{proposition}
\pf If $ U=\{\mathbf 0\} $, we're done. Otherwise let $ \mathbf 0\ne u_1\in U $. Then $ \{u_1\}\subseteq U $ is linearly indepedent. Repeating, after repeating $ k $ times suppose we have $ \{u_1,\dots, u_k\} $ linearly indepedent with $ k\le \dim(V) $ by the previously corollary. If the set spans $ U $ we're done, if not we'll add another vector, $ u_{k+1} $ outside of the span of our space. If $ \{u_1,\dots, u_{k+1}\} $ is not linearly indepedent, we can write $ \mathbf 0 $ non-trivially, so
\[
	\sum_{i=1}^{k+1}\lambda_i u_i=\mathbf 0
\]
with $ \lambda_{k+1}\ne 0 $ since $ \{u_1,\dots, u_k\} $ linearly indepedent. Thus we have that
\[
	u_{k+1}=-\frac 1{\lambda_{k+1}}\left(\sum_{i=1}^k\lambda_iu_i\right)
\]
this process must terminate after at most $ \dim V $ many steps, by the previous corollary. If $ \dim U=\dim V $ apply the previous corollary with $ S $ being any basis for $ U $.\qed
\begin{proposition}
	(Extending a basis) Let $ U\le V $. For any basis $ B_U $ of $ U $ there exists a basis $ B_V $ of $ V $ such that $ B_U\subseteq B_V $.
\end{proposition}
\pf Apply the second result from Steinitz with $ S=B_U $ and $ T $ is any basis for $ V $. We obtain that $ T'\subseteq T\st $ 
\[
  |T'|=|T|-|S|=\dim V-\dim U
\]
and $ B_V=B_U\cup T' $ spans $ V $. But we have that
\[
  |B_V|\le |B_U|+|T'|=\dim V
\]
so by the previous corollary, $ B_V $ is a basis for $ V $.\qed\par
Now we'll finally prove the Steinitz exchange lemma.\par
\pf Let $ S=\{u_1,\dots, u_m\} $, $ T=\{v_1,\dots, v_n\} $ with $ |T|=m $ and $ |T|=n $. If $ S $ is empty then we're done. Otherwise there exists $ \lambda_i\in \F $ such that 
\[
	u_1=\sum_{i=1}^n\lambda_iv_i
\]
so by renumbering we can say that $ \lambda_1\ne 0 $. Then
\[
	v_1=\frac 1{\lambda_1}\left(u_1-\sum_{i=2}^n\lambda_iv_i\right)
\]
So $ \{u_1,v_2,\dots, v_n\} $ spans $ V $. After repeating $ k $ times with $ k<m $ suppose $ \{u_1,\dots, u_k,v_{k+1},\dots, v_n\} $ spans V, then there exists $ \lambda_i,\mu_j\in\F $ such that
\begin{align*}
	u_{k+1}=\sum_{j=1}^k\mu_ju_j+\sum_{i=k+1}^n\lambda_iv_i
\end{align*}
If for all $ \lambda_i= 0 $ then
\[
	\left(\sum_{j=1}^k\mu_ju_j\right) -u_{k+1}=\mathbf 0
\]
which is a contradiction since $ S $ is linearly independent. So by relabeling we have that $ \lambda_{k+1}\ne 0 $ such that 
\[
	v_{k+1}=\frac 1{\lambda_{k+1}}\left(u_{k+1}-\sum_{j=1}^k\mu_ju_j-\sum_{i=k+1}^n\lambda_iv_i\right)
\]
so $ (u_1,\dots, u_{k+1},v_{k+2},\dots, v_n\}$ spans $ V $. So we can conclude that $ m\ne n $ and $ \{u_1,\dots, u_m,v_{m+1},\dots, v_n\} $ spans $ V $ hence the set $ T'=\{v_{m+1},\dots, v_n\} $ exists as claimed. \qed
\begin{definition}
	(Nullity) For a linear map $ \alpha:V\to W $ we define the \textit{nullity} of $ \alpha $ as
	\[
	  \n(\alpha)=\dim\ker\alpha.
	\]
\end{definition}
\begin{definition}
	(Rank) For a linear map $ \alpha:V\to W $ we define the \textit{rank} of $ \alpha $ as
	\[
	  \rk(\alpha)=\dim\ima\alpha.
	\]
\end{definition}
\begin{theorem}
	(Rank-nullity theorem) If $ V $ is a finite dimensional $ \F $-vector space and $ W  $ is a $ \F $-vector space. Then if $ \alpha:V\to W $ is linear then $ \ima \alpha $ is finite dimensional and
	\[
	  \dim V=\n(\alpha)+\rk(\alpha).
	\]
\end{theorem}
\pf Recall the first isomorphism theorem so
\[
	\frac V{\ker \alpha}\cong \ima\alpha
\]
It is sufficient to prove the lemma
\begin{lemma}
  For $ U\le V $,
  \[
    \dim(V/U)=\dim V-\dim U
  \]
\end{lemma}
\pf Let $ B_U=\{u_1,\dots, u_m\} $ be a basis of $ U $. Extend to a basis $ B_V=\{u_1,\dots, u_m,v_{m+1},\dots, v_n\} $ of $ V $ where $ m=\dim U $ and $ n=\dim V $.\\
Set $ B_{V/U}=\{v_i+U:m+1\le i\le n $. The we claim that $ B_{V/U} $ is a basis for $ V/U $ of size $ n-m $. To show spanning, for $ v\in V $ write
	\[
	  v=\sum_i\lambda_iv_i+\sum_j\mu_jv_j
	\]
	Then $ v+U=\sum_i\lambda_i(v_i+U)\in \langle B_{V/U}\rangle $. For linear independence, suppose
	\[
	  \sum_i\lambda_i(v_i+U)=\mathbf 0+U
  \]
	hence 
	\begin{align*}
	  =\left(\sum_i\lambda_iv_i\right)+U \\
	  \sum_i\lambda_iv_i\in U\\
	  \sum_i\lambda_iv_i=\sum_j\mu_ju_j
	\end{align*}
	since $ B_V $ is linearly independent, we have that all $ \lambda_i $ and $ \mu_j $ are zero. Similiarly if $ v_i+U=v_j+U $ with $ i\ne j $ then we can write $ v_i-v_j=\sum_j\mu_ju_j $ which is a contradiction.\qed
	\begin{remark}
	   We can maek a direct proof without quotient spaces by rearranging some of the arguments of the proof.
	\end{remark}
\begin{corollary}
	(Linear Pigeonhole principle) If $ \dim V=\dim W=n $ and $ \alpha: V\to W $ then the following conditions are equivalent.
	\begin{enumerate}
		\item $ \alpha $ is injective,
		\item $ \alpha $ is surjective,
		\item $ \alpha $ is an isomorphism.
	\end{enumerate}
\end{corollary}
\pf If $ \alpha $ injective then $ \n(\alpha)=0 $ so by rank nullity we have that $ \rk(\alpha)=n $ so $ \alpha $ is surjective. If $ \alpha $ is surjective then $ \rk(\alpha)=n $ so by rank nullity, the dimension of the kernel is $ 0 $ hence the kernel is trivial, so $ \alpha $ injective, hence $ \alpha $ is an isomorphism. If $ \alpha $ is an isomorphism, clearly it's injective, so all equivalent.\qed
\begin{proposition}
  Suppose $ V $ is a vector space with a basis $ B $. For any vector space $ W	 $ and any function $ f:B\to W $ there is a unique linear map $ F:V\to W $ such that $ F(B)=W $.
\end{proposition}
\pf First we'll show existance. For $ v\in V $ write $ v=\sum_b\lambda_bb $ for a finite sum. Then define
\[
  F(v)=\sum_b \lambda_bf(b).
\]
This is well-defined, since $ B $ is a basis the $ \lambda_b $ are uniquely determined by $ v $. For $ u,v\in V $ and $ \lambda\in \F $ we write
\[
  u=\sum_b\mu_bb,\quad \sum_b\lambda_bb.
\]
Then \begin{align*}
	F(u+\lambda v)&=F(\sum_b(\mu_b+\lambda\lambda_b)f(b)\\
		     &= \sum_b\mu_bf(b)+\lambda\sum_b\lambda_bf(b)\\
		     &= F(u)+\lambda F(v).
\end{align*}
So $ F $ is linear. To show uniqueness $ \overline F:V\to W $ is another linear map extending $ f $ then,
\[
  \overline F\left(\sum_b\lambda bb\right)=\sum_b\lambda_b\overline F(b)
\]
which is the same as our definition for $ F $ hence they are the same function.
\begin{corollary}
	For a vector space, $ V $, with $ \dim V=n $ with a basis $ B=\{v_1,\dots, v_n\} $ for $ V $ then there is a unique isomorphism
  \begin{align*}
	  F_B:V&\to \F^n\\
	       \sum_{i=1}^n\lambda_iv_i &\to \begin{pmatrix}
	        \lambda_1\\
		\vdots \\
		\lambda_n
	      \end{pmatrix}
  \end{align*}
  \end{corollary}
  \pf Let $ E=\{e_1,\dots, e_n\} $ be the standard basis for $ \F^n $. Define
  \begin{align*}
	  f:B&\to W\\
	  v_i&\to e_i
  \end{align*}
  and let $ F_B $ be the unique linear extension of $ f $ to $ V $. We see that $ f $ defines a bijection from $ B\to E $. Let $ \bar F_B $ be the unique linear extension of $ \inv f:E\to B $. Then $ \bar F_B\cdot F_B $ is the composition of two linear maps, hence it's linear, moreover it is $ \mathrm{id}_B $. But also $ \mathrm{id}_V $ is also a linear extension of $ \mathrm{id}_B $, by the proposition, they are the same map so $ \bar F_B\cdot F_B=F_B\cdot \bar F_B=\mathrm{id}_B $. Hence $ F_B $ is bijective, so it is an isomorphism.\qed
  \begin{corollary}
    If $ V,W $ are finite dimensional $ \F $-vector spaces. Then
    \[
      V\cong W\iff \dim V=\dim W
    \]
  \end{corollary}
  \pf Trivial from the corollary using the transitivity of the isomorphism relation.\qed
\begin{definition}
	(Coordinate vector) $ F_B(v)=[v]_B $ is the \textit{coordinate vector} of $ v $ with respect to the basis $ B $
\end{definition}
For an example if $ V\cong \F^n $ and $ U\le V $ with $ U\cong \F^m $ then $ \dim(V/U)=n-m $, so $ \frac VU\cong \F^{n-m} $.
\subsection{Direct sums}
\begin{definition}
	(External direct sum) For $ \F $-vector spaces, $ V $ and $ W $, we dnote the \textit{external direct sum} of $ V $ and $ W $ as $ V\oplus W $ with underlying set $ V\times W $ with addition and scalar multiplication given in the obvious sense.
\end{definition}
We can similarly define
\[
	V_1\oplus \cdots \oplus V_n=\bigoplus_{i=1}^nV_i.
\]
\begin{lemma}
  For $ V,W $ finite dimensional vector spaces,
  \[
    \dim(V\oplus W)=\dim V+\dim W
  \]
\end{lemma}
\pf\\(First Proof) Let $ B,C $ be basis for $ V,W $ respectively. Set
\[
	D=(B\times\{\mathbf 0_W\})\cup (\{\mathbf 0_V\}\times C)
\]
it is straightfoward to check that $ D $ is basis of $ V\oplus W $ of the size $ \dim V + \dim W $.\qed\par
(Second Proof) Suppose $ V\cong \F^n $ and $ W\cong \F^m $ construct an isomorphism $ V\oplus W\cong\F^{n+m} $.\qed

\begin{proposition}
  Let $ V $ be a vector space with $ U,W\le V $. There is a surjective linear map
  \begin{align*}
	  \varphi: U\oplus W &\to U+W\\
	  (u,w) &\to u+w
  \end{align*}
  with $ \ker\varphi\cong U\cap W $.
\end{proposition}
\pf Surjectively and linearity are clear. Note for $ (u,w)\in U\oplus W $ then $ (u,w)\in\ker\varphi $ if and only if $ w=-u $. Hence
\[
	\ker\varphi=\{(x,-x):x\in U\cap W\}
\]
the map $ \psi:U\cap W\to \ker\varphi $ sending $ x\to(x,-x) $ is an isomorphism.
\begin{corollary}
	(Sum-Intersection Formula) If $ V $ is finite dimensional and $ U,W\le V $ then
	\[
	  \dim (U+W)=\dim U + \dim V-\dim(U\cup V)
	\]
\end{corollary}
Applying the rank-nullity theorem to the linear map $ \varphi $ in the proposition we get that
\begin{align*}
	\dim U + \dim W&=\dim (U\oplus V)\\
		       &=\dim(\ker\varphi)+\dim(\ima\varphi)\\
		       &=\dim(U+W)+\dim(U\cap W)\qed
\end{align*}
We can also give an explicit basis. Given a basis $ B $ for $ U\cap W $, extend $ B $ to a basis $ B_U $ for $ U $, and a basis $ B_W $ for $ W $. Then $ B_U\cap B_W $ spans $ U+W $ and
\[
  |B_U\cup B_W|\le |B_U|+|B_W|-|B|=\dim(U+V)
\]
hence $ B_U\cup B_W $ is linearly independent so it's a basis for $ U+W $.
\begin{remark}
We could also check directly that $ B_U\cup B_W $ is linearly independent of the size $ \dim(U+V) $ without assuming the sum-intersection formula, so this alos servers as an alternative proof of the sum-intersection formula.
\end{remark}
\begin{definition}
	(Internal direct sum) Suppose $ U,W\le V $ satisify 
	\begin{enumerate}
		\item $ U+W=V $,
		\item $ U\cap W=\{\mathbf 0_V\} $.
	\end{enumerate}
	Then 
	\begin{align*}
	  \varphi: U\oplus W\to V
	\end{align*}
	is an isomorphism, and we say that $ V $ is the \textit{internal direct sum} of $ U $ and $ W $, and we write that $ V=U\oplus W $.
\end{definition}
Alternatively, every element $ v\in V $ can be written \textit{uniquely} as $ v=u+w $ for $ u\in U, w\in W $.
\begin{definition}
	(Direct complement) For $ U\le V $ a \textit{direct complement} to $ U $ in $ V $ is a subspace $ W\le V $ satisfying $ V=U\oplus W $.
\end{definition}
\begin{proposition}
  If $ V $ is finite dimensional then every subspace has a direct complement.
\end{proposition}
\pf Let $ U\le V $ and let $ B_U $ be a basis for $ U $. Extend to a basis $ B_V $ for $ V $. Set $ W=\langleB_V\setminus B_U\rangle $. Then
\begin{align*}
	V=\langle B_V\rangle &= \langle B_U\cup (B_V\setminus B_U)\rangle\\
			     &= \langle B_U\rangle + \langle B_V\setminus B_U\rangle \\
			     &=U+W.
\end{align*}
Moreover using the sum-intersection formula
\begin{align*}
	\dim (U\cap W)=|B_V|+|B_U|-|B_V\setminus B_U|=0.
\end{align*}
Hence $ U\oplus W=V $.\qed\par
More generally for $ U_1,\dots, U_n\le V $ we say that $ V $ is the direct sum of the $ U_i $ and write that
\[
	V=U_1\oplus+\cdots +\oplus V_n=\bigoplus_{i=1}^n V_i
\]
if the map
\begin{align*}
	\varphi: U_1\oplus\cdots\oplus U_n &\to V\\
	(u_1,\dots, u_n) &\to u_1,\dots, u_n
\end{align*}
is an isomorphism. Equivalently every $ v\in V $ can be uniquely written as $ v=u_1+\dots +u_n $ for $ u_i\in U_i $.
\section{Matrices and Linear Maps}
\subsection{Vector spaces of linear maps}
\begin{definition}
  For $ V,W $ $ \F $-vector spaces we define
  \[
	  \mathcal L(V,W)=\{\alpha:V\to W:\alpha\text{ is linear} \}
  \]
  which forms a $ \F $-vector space under pointwise addition and obvious scalar multiplication.
\end{definition}
Recall that $ M_{m\times n} $ is the space of matrices over $ \F $ with $ m $ rows and $ n $ columns. For $ A\in M_{m\times n}(\F) $ we write $ A=(a_{ij}) $ where $ a_{ij}\in \F $ is the entry in the $ i $th row and the $ j $th column.\par
Let $ B=\{v_1,\dots, v_n\}, C=\{w_1,\dots, w_m\} $ are \textit{ordered} basis for $ V,W $.\par
Let $ \alpha\in\mathcal L(V,W) $. We can write
\begin{align*}
	\alpha(v_1)&=a_{11}w_1+a_{21}w_2+\cdots + a_{m1}w_m\\
	\alpha(v_2)&=a_{12}w_1+a_{22}w_2+\cdots + a_{m1}w_m\\
	^\vdots\\
	\alpha(v_n)&=a_{1n}w_1+a_{2n}w_2+\cdots +a_{mn}w_m
\end{align*}
\begin{definition}
	(Matrix) The \textit{matrix} of $ \alpha $ with respect to the ordered basis $ B,C $ is
	\[
		[\alpha]^B_C=(a_{ij})\in M_{m\times n}(\F)
	\]
\end{definition}
Recall we have a linear isomorphism
\begin{align*}
	\varepsilon_B:V &\to \F^n\\
	v=\sum_{i=1}^n\lambda_iv_i &\to (\lambda_i)_i=[v]_B
\end{align*}
where $ [v]_B $ is the coordinate vector of $ v $ with respect to $ B $.
\begin{theorem}
  For finite-dimensional vector spaces $ V,W $ with basis $ B,C $ respectively and $ \alpha:V\to W $ linear  then
  \begin{enumerate}
	  \item For all $ v\in V $
		  \[
			  [\alpha]^B_C[v]_B=[\alpha(v)]_C
		  \]
	  \item $ [\alpha]_C^B $ is the only matrix $ A\in M_{m\times n}(\F) $ satisfying $ A[v]_B =[\alpha(v)]_C $ for all $ v\in V $.
	  \item There is an isomorphism of $ \F $-vector spaces
		  \begin{align*}
			  \varepsilon_C^B:\mathcal L(V,W)&\to M_{m\times n}(\F)\\
			  \alpha &\to [\alpha]^B_C
		  \end{align*}
\end{theorem}
\pf Let $ v\in V $ write $ v=\sum_{j=1}^n\lambda_jv_j $. Then
\begin{align*}
	\alpha(v)&=\sum_{j=1}^n\lambda_j\alpha(v_j)\\
		 &=\sum_{j=1}^n\lambda_j\sum_{i=1}^ma_{ij}w_i\\
		 &=\sum_{i=1}^m\left(\sum_{j=1}^n\lambda_ja_{ij}\right)w_i.
\end{align*}
So
\begin{align*}
	[\alpha(v)]_C&=\left(\sum_{j=1}^n a_{ij}\lambda_j\right)_i\\
		     &=(a_{ij})\cdot(\lambda_j)\\
		     &=[\alpha]^B_C[v]_B.
\end{align*}
Hence (i) is proved. For (ii), take $ 1\le j\le n $, so $ [v_j]_B=e_j $. Hence for $ A\in M_{m\times n}(\F) $, $ A[v_j]_B $ is the $ j $th column of $ A $. But if $ A[v_j]_B=[\alpha(v_j)]_C=[\alpha]^B_C[v_j]_B=[\alpha]^B_Ce_j $, then $ A[v_j]_B $ is also the $ j $th column of $ [\alpha]^B_C $. Since this holds for all $ j $ in our range, they are the same matrix.\par
Now for part (iii), let $ \alpha,\beta\in\mathcal L(V,W) $ and $ \lambda\in \F $. Then
\begin{align*}
	[\alpha+\lambda\beta]^B_C[v]_B&=[(\alpha+\lambda\beta)(v)]_C\\
				      &=[\alpha(v)+\lambda\beta(v)]_C\\
				      &= [\alpha(v)]_C+\lambda[\beta(v)]_C\\
				      &= ([\alpha]^B_C+\lambda[\beta]^B_C)[v]_B
\end{align*}
for all $ v\in V $. Hence by (ii) we get that $ [\alpha+\lambda\beta]^b_c=[\alpha]_C^B+\lambda[\beta]^B_C $ so the map is linear. Let $ \alpha\in\ker(\varepsilon_C^B) $ so that $ [\alpha]^B_C=0\in M_{m\times n} (\F)$. Then by (i) we have that $ [\alpha(v)]_C=0 $ for all $ v\in V $. But $ \varepsilon:w\to [w]_C $ is an isomorphism so $ \alpha(v)=0 $ for all $ v\in V $ hence $ \alpha=0 $ and $ \alpha $ is injective. For surjectivity let $ A\in M_{m\times n}(\F) $ and define $ f:B\to W $ by $ f(v_j)=\sum_{i=1}^na_{ij}w_I $ and extend $ f $ to a linear map $ F:V\to W $. Then $ [F]_C^B=A $. So $ \varepsilon_C^B $ is an isomorphism.\qed
\begin{proposition}
  Let $ V,W,X $ be finite-dimensional $ \F $-vector spaces with basis $ B,C,D $ and $ \alpha\in\mathcal L(V,W) $ and $ \beta\in\mathcal L(W,X) $. Then
  \[
	  [\beta\circ \alpha]^B_D=[\beta]^C_D[\alpha]^B_C.
  \]
\end{proposition}
\pf By the theorem $ [\beta\circ\alpha]^B_D $ is the unique matrix $ A $ satisfying
\[
	A[v]_B=[\beta(\alpha(v))]_D,\quad \forall v\in V.
\]
But $ [\beta]^C_D[\alpha]^B_C[v]_B=[\beta]^C_D[\alpha(v)]_C=[\beta(\alpha(v))]_D $. So by (ii) of theorem they are equal.\qed
\begin{remark}
  For any basis $ B $ of $ V $,
  \[
	  [\mathrm{id}_V]^B_B=I_{\dim V}.
  \]
\end{remark}
\begin{definition}
	(Change of basis matrix) Let $ B,B' $ be basis for $ V $ and $ \dim V=n $. The \textit{change of basis matrix} from $ B $ to $ B' $ is given by
	\[
		P=[\mathrm{id}_V]^B_{B'}\in M_{m\times n}(\F)
	\]
\end{definition}
Equivalently letting $ B=\{v_i\}_{i=1}^n $ and $ B'=\{v_i'\}_{i=1}^n $, then
\[
	P=(p_{ij})\quad\text{where}\quad v_j=\sum_{i=1}^n p_{ij}v_i'
\]
so the $ j $th column of $ P $ is $ [v_j]_{B'} $.
\begin{proposition}
	For $ V,W $ finite-dimensional vector spaces,\smallskip
  \begin{enumerate}
	  \item $ [\mathrm{id}_V]^B_{B'}\in GL_n(\F) $ with inverse $ [\mathrm{id}_V]^{B'}_B $.
	  \item If $ \alpha\in\mathcal L(V,W) $ and $ B,B' $ basis for $ V $ and $ C,C' $ basis for $ W $, then
		  \[
			  [\alpha]^{B'}_{C'}=[\mathrm{id}_W]^C_{C'}[\alpha]^B_C[\mathrm{id}_V]^{B'}_B.
		  \]
  \end{enumerate}
\end{proposition}
\pf By the remark,
\begin{align*}
	I_n=[\mathrm{id}_V]^B_B=[\mathrm{id}_V]^{B'}_B[\mathrm{id}_V]^B_{B'}
\end{align*}
and symmetrically swapping $ B $ and $ B' $. For the second part the result is immediate from the proposition.
\begin{definition}
	(Equivalent matrices) Let $ A,A'\in M_{m\times n}(\F) $. We say that $ A $ and $ A' $ are \textit{equivalent} if $ \exists P\in GL_m(\F) $, $ Q\in GL_n(\F) $ such that $ A'=PAQ $. 
\end{definition}
\begin{remark}
  Certianly $ A $ is equivalent to itself by $ P=I_m $ and $ Q=I_n $.\\
  If $ A'=PAQ $ then $ A=\inv PA'\inv Q $.\\
  If $ A''=RA'S $ too, then $ A''=(RP)A(QS) $, so the equivalence of matrices is an equivance relation on $ M_{m\times n}(\F) $.
\end{remark}
\begin{theorem}
  Let $ V,W $ be finite-dimensional $ \F $-vector spaces. Let $ \dim V=n $, $ \dim W=m $ and let $ \alpha\in\mathcal L(V,W) $. Let $ r=\rk(\alpha) $. Then,
  \begin{enumerate}
	  \item There exists basis $ B,C $ for $ V,W $ respectively such that
		  \[
			  [\alpha]^B_C=\begin{pmatrix}
				  I_r & 0 \\
				  0 & 0
			  \end{pmatrix}\in M_{m\times n}(\F)
		  \]
		  where $ I_r $ is the identity matrix of size $ r $, and the zeros are block zero matrices.
	  \item If \[
			  [\alpha]^{B'}_{C'}=\begin{pmatrix}
				  I_{r'} & 0 \\
				  0 & 0
			  \end{pmatrix}\in M_{m\times n}(\F)
		  \]
		  for some basis $ B',C'$ of $ V,W $ respectively, then $ r'=r $
  \end{enumerate}
\end{theorem}
\pf By rank-nullity $ \n(\alpha)=n-r $. Let $ \{v_{r+1},\dots, v_n\} $ be a basis for $ \ker\alpha $. Extend to a basis $ B=\{v_1,\dots, v_r,v_{r+1},\dots, v_n\} $. Then $ \{\alpha(v_1),\dots, \alpha(v_r)\} $ spans the image, and has size at most $ \dim(\ima(\alpha)) $, so it's linearly independent, hence we can extend it to form a basis of $ W $.
\[
	C=\{w_1=\alpha(v_1),\dots, w_r=\alpha(v_r),w_{r+1},\cdots, w_m\}
\]
Then
\[
  \alpha(v_j)=\begin{cases}
	  w_j & 1\le j\le r \\
	  \mathbf 0 & \text{otherwise}
  \end{cases}
\]
hence we have that $ [\alpha]^B_C= \begin{pmatrix}
				  I_{r} & 0 \\
				  0 & 0
			  \end{pmatrix} $.\par
			  For the second part, if $ [\alpha]^{B'}_{C'}= \begin{pmatrix}
				  I_{r'} & 0 \\
				  0 & 0
			  \end{pmatrix} $ then
			  \[
			    \alpha(v_j')=\begin{cases}
	  w_j' & 1\le j\le r' \\
	  \mathbf 0 & \text{otherwise}
  \end{cases}.


			  \]
			  Hence $ w_1',\dots, w_{r'}' $ span $ \ima(\alpha) $ and are linearly independent. Hence $ \rk(\alpha)=r' $.\qed
\begin{definition}
	(Column-space) For $ A\in M_{m\times n}(\F) $ the \textit{column-space} $ \Col(A) $ is the subspace of $ \F^m $ spanned by the columns of $ A $. The dimension of the column-space is called the \textit{column-rank} of $ A $.
\end{definition}

\begin{definition}
	(Row-space) For $ A\in M_{m\times n}(\F) $ the \textit{row-space} $ \Row(A) $ is the subspace of $ \F^m $ spanned by the rows of $ A $ (when transposed as column vectors). The dimension of the row-space is called the \textit{row-rank} of $ A $.
\end{definition}
\begin{remark}
  \[
    \Row(A)=\Col(A^T)
  \]
  hence the row-rank of $ A $ is the same as the column-rank of $ A^T $.
\end{remark}
\begin{remark}
	Given a matrix $ A\in M_{m\times n}(F) $ we can define a linear map $ \alpha:\F^n\to \F^m $ by $ \alpha(v)=Av $. Then $ \ima(\alpha)=\Col(A) $, so the rank of $ \alpha $ is the same as the column-rank of $ A $. Moreover, $ A=[\alpha]^{E_n}_{E_m} $ where $ E_k $ are the standard basis for $ \F^k $.
\end{remark}
We may write $ \ima A,\ker A, \rk(A),\n(A) $ to refer to the corresponding concepts for $ \alpha $.
\begin{theorem}
	Let $ A, A' \in M_{m\times n}(\F) $, then
	\begin{enumerate}
		\item $ A $ is equivalent to
\[
  \begin{pmatrix}
	  I_r & 0 \\
	  0 & 0
  \end{pmatrix}
  \text{ where } r \text{ is the column-rank of } A
\]
\item $ A $ and $ A' $ are equivalent if and only if the have the same column-rank.

	\end{enumerate}
\end{theorem}
\pf We'll first prove a lemma.
\begin{lemma}
	For $ A\in M_{m\times n}(\F) $ and $ B\in M_{n\times p}(\F) $ then $ \rk(A\cdot B)\le \min(\rk(A),\rk(B)) $.
\end{lemma}
\pf We have that $ \ima(AB)\le \ima(A) $ so $ \rk(AB)\le \rk(A) $. If $ Bv=\mathbf 0 $ for $ v\in \F^p $, then $ ABv=\mathbf 0 $, so $ \n(B)\ge \n(AB) $, so applying rank-nullity, we get that
\[
  p-\rk(B)\le p-\rk(AB)\implies \rk(AB)\le \rk(B)\qed
\]
\par
Now we'll prove the first part of the theorem. Let $ \alpha $ the natural linear map corresponding to $ A $, so $ A=[\alpha]^{E_n}_{E_m} $. By the previous theorem, there exists matrices $ B,C $ of $ \F^n,\F^m $ such that
\[
  \begin{pmatrix}
	  I_r & 0\\ 0 & 0
  \end{pmatrix} = [\alpha]^B_C=[\mathrm{id}_{\F^m}]^{E_m}_C[\alpha]^{E_n}_{E_m}[\mathrm{id}_{\F^n}]^B_{E_n}=PAQ
\]
where $ r=\rk(\alpha) $ which we know is equal to the column-rank of $ A $.\par
If $ A' $ has column-rank $ r $ then both matrices are equivalent to $ \begin{pmatrix}
	I_r&0\\0 &0
\end{pmatrix} $, so by transitivity, $ A $ and $ A' $ are equivalent. Conversely suppose that $ A $ and $ A' $ are equivalent, so $ A'=PAQ $. By the lemma $ \rk(A')\ge \rk(AQ)\ge\rk(A) $ and symmetrically we get that $ \rk(A)\ge \rk(A') $, hence $ \rk(A')=\rk(A) $.\qed
\begin{theorem}
	For any $ A\in M_{m\times n}(\F) $, the row-rank of $ A $ is equal to the column-rank of $ A $.
\end{theorem}
\pf Note that if $ P $ is invertiable, then so it the tranpose with inverse $ (\inv P)^T $. Let $ r $ be the column-rank of $ A $. So there exists matrices $ P\in GL_m(\F) $ and $ Q\in GL_n(\F) $ such that $ PAQ=\begin{pmatrix}
	I_r&0\\ 0 & 0
\end{pmatrix}\in M_{m\times n}(\F) $. Then $ A^T $ is equivalent to $ Q^TA^TP^T=(PAQ)^T=\begin{pmatrix}
	I_r & 0 \\ 0 & 0
\end{pmatrix} \in M_{n\times m}(\F)$. By the previous theorem, the column-rank of $ A^T $ is $ r $ which also the row-rank of $ A $.\qed\par
Let $ V $ be a finite-dimensional vector space and $ B,B' $ be basis for $ V $. Now let $ \alpha\in\mathrm{End}(V)=\mathcal L(V,V) $. Then
\[
	[\alpha]^{B'}_{B'}=[\mathrm{id}_V]^B_{B'}[\alpha]^B_B[\mathrm{id}_V]^{B'}_B
\]
\begin{definition}
	(Similarity) For matrices $ A,A'\in M_{n\times m}(\F) $ are \textit{similar} if there exists $ P\in GL_n(\F) $ such that $ A'=\inv P AP $.
\end{definition}
\begin{remark}
We have some remarks showing the similarity and equivalence are not the same thing.
\begin{enumerate}
	\item Similarity is an equivalence relation on $ M_{n\times n}(\F) $.
	\item Similar matrices are equivalent but equivalent matrices need not be similar.
\end{enumerate}
For example every matrix in $ GL_n(\F) $ is equivalent to $ I_n $ but $ I_n $ forms its only single element equivalence class, when we think about similarity.
\end{remark}
\subsection{Elementary operations on matrices}
\begin{definition}
(Elementary row operations)
Let $ r_1,\dots, r_m $ be the rows of $ A $. We have three types of \textit{elementary row operations} on $ A $
\begin{enumerate}
	\item Swap $ r_i $ and $ r_j $ with $ i\ne j $.
	\item Replace $ r_i $ with $ \lambda r_i $ with $ 0\ne\lambda\in\F $.
	\item Replace $ r_i $ with $ r_i+\lambda r_j $ with $ \lambda\in \F $ and $ i\ne j $.
\end{enumerate}
\end{definition}
Similarly there are three types of elementary column operations.
\begin{remark}
  These are all reversable.
\end{remark}
Each elementary operation has a corresponding matrix representation repesentation. All corresponding matrices are invertiable.
\begin{lemma}
  If $ E $ is a matrix of type (i)-(iii) then $ EA $ is obtained from $ A $ by applying the corresponding ERO to $ A $.
\end{lemma}
\pf Direct matrix computation.
\begin{remark}
  Similarly $ AE $ is obtained by applying the corresponding ECO
\end{remark}
\begin{remark}
  EROs preserve $ \Row(A) $ (and ECOs preserve $ \Col(A) $).
\end{remark}
So both EROs and ECOs preserve the row-rank of a matrix, and therefore also the rank of the linear map corresponding to the matrix.
\begin{definition}
	(Row reduced echelon form) A matrix $ A\in M_{m\times n}(\F) $ is said to be in \textit{row reduced echelon form} (RRE) if
	\begin{enumerate}
		\item All non-zero rows of $ A $ appear above all zero rows.
		\item The leftmost non-zero element of a non-zero row is $ 1 $ (called the \textit{pivot entry}).
		\item If row $ r_i,r_j $ are non-zero rows with $ i<j $ then the index of the pivot entry of $ i $ is less than the index of the pivot entry of $ j $.
		\item In a column containing a pivot entry, every other entry is zero.
	\end{enumerate}
\end{definition}
For an example consider
\[
  M=\begin{pmatrix}
	  1 & a & 0 & 0 & b\\
	  0 & 0 & 1 & 0 & c\\
	  0 & 0 & 0 & 1 & d
  \end{pmatrix}
\]
which is in row reduced echelon form. Similarly we have column reduced echelon form, which have the exact same rules but transposed.
\begin{lemma}
  If $ A $ is in row reduced echelon form then the row rank of $ A $ is the number of non-zero rows of $ A $.
\end{lemma}
\pf Let $ r_1,\dots, r_k $ be the non-zero rows of $ A $ let $ j_i=P(v_i) $ be the pivot entry. Certainly $ r_1,\dots, r_k $ span $ \Row(A) $. Suppose that
\[
	v=\sum_{i=1}^k\lambda_ir_i=0\quad (\lambda_i\in\F).
\]
Then $ (v)_{j_i} =\lambda_i=0 $ so the non-zero rows are linearly independent so we're done.\qed
\begin{proposition}
	Every matrix $ A \in M_{m\times n}(\F) $ can be put into row reduced echelon form with elementary row operations.
\end{proposition}
\pf Proceed by induction on $ n $. Write that $ A=\left[c_1\mid \cdots \mid c_n\right] $. If $ c_1=0 $ apply induction to $ [c_2\mid \cdots \mid c_n] $, so suppose that $ c_1\ne 0 $, suppose that element in $ (i,1) $ is non-zero. Applying row operations (i) we can move it to $ (1,1) $.  Apply row operation (ii) to rescale it to be $ 1 $. Now we can clear the rest of the column by (iii). By induction we can use elementary row operation on rows $ 2$-$ m $ to reduce further. This is decreasing the dimension to the process terminates, hence the matrix can be put into row reduced echelon form.\qed
\begin{remark}
  Putting a matrix into RRE form preserves the row-space and the RRE of any matrix is unique. Also if $ A $ is a square matrix then $ A $ either has a zero row or is the identity. 
\end{remark}
\begin{theorem}
	For $ A\in M_{m\times n}(\F) $ the following are equivalent:
	\begin{enumerate}
		\item $ \rk (A)=n $.
		\item $ A $ is a product of elementary matrices.
		\item $ A $ is invertiable.
	\end{enumerate}
\end{theorem}
\pf Let's prove that (i) $ \implies $ (ii). By the proposition there exists elementary matrices $ E_i $ such that $ E_1\dots E_\ell A $ is in RRE form. By the remark this is $ I_n $ hence $ A=\inv E_\ell\cdots \inv E_1 $ which are also elementary. For (ii) $ \implies $ (iii) elementary matrix lie in $ GL_n(\F) $ which is a group, hence closed. Finially for (iii) $ \implies $ (i) suppose there exists $ B\in M_{m\times n}(\F) $ such that $ AB=I_n $. Then for $ v\in \F^n $ we have that $ v=(AB)v=A(Bv) $, so $ v\in \ima A $.
\section{Determinant and Traces}
\subsection{Determinant}
\begin{theorem}
	There exists a unique function $ F:M_{m\times n}\to \F $ satisfying
	\begin{enumerate}
		\item (Alternating) If $ c_i=c_j $ for some $ i\ne j $ then $ F(A)=0 $.
		\item (Multilinear in columns) For all $ 1\le i\le n $ and $ v_j\in \F^n $ the function 
			\begin{align*}
				\F^n&\to \F\\
				v&\to F(v_1\mid \cdots \mid v_{j-1} \mid v \mid v_{j+1} \mid \cdots \mid v_n )
			\end{align*}
			is linear.
		\item $ F(I_n)=1 $.
	\end{enumerate}
\end{theorem}
\begin{definition}
	(Determinant) We shall defined the $ F $ in the previous theorem as the $ n $\textit{-dimensional determinant}, written as $ F(A)=\det(A) $. A function satisfying conditions (i) and (ii) of the theorem is called an $ n $-\textit{-dimensional volumn form}.
\end{definition}

\begin{lemma}
	If $ F $ is an $ n $-dimensional volumn form, $ A\in M_{m\times n}(\F) $,
	\begin{enumerate}
		\item If $ A $ has a zero column then $ F(A)=0 $,
		\item $ F(AT_{ij})=-F(A) $,
		\item $ F(AM_{i,\lambda})=\lambda F(A) $,
		\item $ F(AC_{i,j,\lambda})=F(A) $.
	\end{enumerate}
\end{lemma}
\pf Let $ f_i:\F^n \to \F $ be given by $ v\to F(c_1\mid \cdots c_{i-1} \mid v\mid c_{i+1}\mid \cdots \mid c_n) $. So that $ f_i $ is linear. Then $ f_i(c_j)=\delta_{ij}F(A) $. iF $ c_i=0 $ then $ F(A)=f_i(c)=f_i(0)=0 $. For (ii), let $ \bar A $ be the matrix obtained from $ A $ by replacing both $ i $th and $ j $th columns of $ A $ by $ c_i+c_j $. Then $ 0=F(\bar A)=F(A)+f_i(c_j)+f_i(c_i)+F(AT_{ij}) $. For (iii), $ F(AM_{i,\lambda})=f_i(\lambda c_i)=\lambda f_i(c_i)=\lambda F(A) $. Now for (iv), $ F(AC_{i,j,\lambda})=f_j(c_j+\lambda c_i)=f_i(c_j)+\lambda f_j(c_i)=F(A) $.\qed\par
Now we're ready to prove the theorem.\\
\pf First we'll prove uniqueness. Let $ F $ be an $ n $-dimensional volumn form with $ F(I_n)=1 $. By the lemma, $ F(T_{i,j})=-1, F(M_{i,\lambda})=\lambda, F(C_{i,j,\lambda})=1, F(AE)=F(A)F(E) $ for $ E $ elementary. Let $ A\in M_{n\times n}(\F) $, so there exists elementary matrices $ E_1,\dots, E_\ell $ such that $ A'=AE_1\cdots E_\ell $ with $ A' $ in CRE form. Then $ F(A)=F(A')\inv {F(E_1)}\cdots \inv{F(E_\ell)} $, so either $ A'=I_n $ so $ F(A)=\inv{(F(E_1)}\cdots \inv{F(E_\ell)} $ or $ A' $ has a zero colun so by the lemma, $ F(A)=F(A')=0 $.\par
This also proves the corollary.
\begin{corollary}
  $ \det A\ne 0 $ if and only if $ A $ is invertible. In this case, $ A=E_1\cdots E_\ell $ then $ \det A=\det(E_1)\cdots \det(E_\ell) $.
\end{corollary}
Recall that from IA Groups that $ \sgn:S_n\to \{\pm 1 \} $ is the unique homomorphism satisfying $ \sgn(\tau)=-1 $ for all transpositions. Now we can define the determinant.
\begin{align*}
	\det:M_{n\times n}(\F)&\to \F \\
	a&\to \sum_{\sigma\in S_n}\sgn(\sigma)\prod_{i=1}^na_{\sigma(i)i}
\end{align*}
Clearly we have that $ \det I_n=1 $. Each product $ \prod_{i=1}^na_{\sigma(i)i} $ is multilinear in columns. Hence so is $ \det A $. Suppose that $ c_k=c_\ell $ for $ k\ne \ell $. Set $ \tau=(k\ \ell) $ so that $ a_{ij}=a_{i\tau(j)} $ for all $ i,j $. Then
\begin{align*}
	\det A&=\sum_{\sigma\in A_n}\sgn(\sigma)\prod_{i=1}^n a_{\sigma(i)i}+\sum_{\sigma\in A_n}\sgn(\sigma\tau)\prod_{i=1}^n a_{\sigma\tau(i)i}\\
	      &= \sum_{\sigma\in A_n}\prod_{i=1}^na_{\sigma(i)i}-\sum_{\sigma\in A_n}\prod_{i=1}^n a_{\sigma\tau(i)i}\\
	      &=\sum_\sigma\prod_ia_{\sigma(i)i}-\sum_\sigma\prod_ia_{\sigma\tau(i)\tau(i)}\\
	      &=\sum_\sigma\prod_ia_{\sigma(i)i}-\sum_\sigma\prod_ja_{\sigma(j)j}
\end{align*}\qed\par
Now we will observe some properties of the determinant.
\begin{lemma}
	For $ A\in M_{n\times n}(\F) $, we have that $ \det(A^T)=\det(A) $.
\end{lemma}
\pf 
\begin{align*}
	\det(A^T)&=\sum_{\sigma\in S_n}\sgn(\sigma)\prod_{i=1}^na_{i\sigma(i)}\\
		 &= \sum_{\sigma\in S_n}\sgn(\sigma)\prod_ja_{\inv{\sigma(j)}j}\\
		 &=\sum_{\sigma\in S_n}\sgn(\inv\sigma)\prod_ja_{\inv{\sigma(j)}j}\qed
\end{align*}
\begin{proposition}
	For all $ A,B\in M_{n\times n}(\F) $ we have that $ \det(AB)=\det(A)\det(B) $.
\end{proposition}
\pf Recall that $ \rk(AB)\le \min(\rk(A),\rk(b)) $, so if either $ A $ or $ B $ has rank less than $ n $ then so does $ AB $ so by the corollary above $ \det(AB)=0=\det(A)\det(B) $. If not, $ A $ and $ B $ are invertible and can be written as a product of elementary matrices, so $ A=E_1\cdots E_\ell $, $ B+E_1'\cdots E_k' $. So $ AB=E_1\cdots E_\ell E_1'\cdots E_k' $, hence $ \det(AB)=\det(E_1)\cdots \det(E_\ell)\det(E_1')\cdots \det(E_k')=\det(A)\det(B)$.\qed\par
Missed a lecture - 31.11.25
\begin{definition}
	(Trace) For a $ A\in M_{m\times n}(\F) $ the \textit{trace} of $ A $ is given by
	\[
		\tr(A)=\sum_{i=1}^na_{i,i}
	\]
\end{definition}
\begin{remark}
	$ \tr \in \mathcal L(M_{m\times n}(\F),\F) $
\end{remark}
\begin{lemma}
	For all $ A,B\in M_{m\times n}(\F) $ we have that
	\[
	  \tr(AB)=\tr(BA)
	\]
\end{lemma}
\pf 
\begin{align*}
	\tr(AB)&=\sum_{i=1}^n (AB)_{i,i}\\
	&= \sum_{i=1}^n\sum_{j=1}^na_{ij}b_{ji}\\
	&= \sum_{j=1}^n\sum_{i=1}^nb_{ji}a_{ij}\\
	&= \sum_{j=1}^n(BA)_{j,j}=\tr(BA)\qed
\end{align*}
But in general we don't have that $ \tr(AB)=\tr(A)\tr(B) $.
\begin{corollary}
  Similar matrices have the same trace.
\end{corollary}
\pf For $ P\in GL_n(\F) $,
\[
  \tr(PA\inv P)=\tr(\inv P(PA))=\tr(A)
\]
\begin{definition}
	For $ V $ a finite dimensional vector space and $ \alpha\in\mathcal L(V,V) $ define the \textit{trace} of $ \alpha $ by
	\[
		\tr(\alpha)=\tr([\alpha]_B^B)
	\]
	for $ B $ a basis of $ V $.
\end{definition}
\begin{proposition}
  This is independent of the basis $ B $.
\end{proposition}
\pf If $ B' $ is another basis of the vector space $ V $ then, $ [\alpha]_{B'}^{B'} $ and $ [\alpha]_B^B $ are similar matrices, so the result follows from the corollary.
\section{Dual Spaces}
\begin{definition}
	(Dual space) If $ V $ is a $ \F $-vector space, then the \textit{dual space} of $ V $ is
	\[
		V^*=\mathcal L(V,\F)=\{\theta:V\to\F: \theta\text{ is linear }\}
	\]
\end{definition}
For example we have that $ \theta:\R^3\to \R $ given by
\[
  \theta\left(\begin{pmatrix}
    x \\ y \\ z
  \end{pmatrix}\right) = x-2y+3z
\]
is an element in the dual space of $ \R^3 $.\par
$ \tr\in M_{n\times n}(\F)^* $\par
If $ V=C[0,1] $ then $ \theta:V\to R $ given by $ \theta(f)=\int_0^1f(t)e^{-t}\mathrm dt $ is in the dual space of $ V^* $.\par
An element $ \theta\in V^* $ is called a \textit{linear functional} on $ V $.\par
Suppose that $ B $ is a basis for $ V $. For $ b\in B $ define $ b^*\in V^* $ by
\[
	b^*\left(\sum_{c\in B}\lambda_cc\right)=\lambda_b
\]
i.e. $ b^*(c)=\delta_{bc} $. If we let $B^*=\{b^*:b\in B\} $ then:
\begin{proposition}
  For $ B^* $ defined above,
  \begin{enumerate}
	  \item $ B^* $ is linearly independent;
	  \item If $ V $ is finite dimensional then $ B^* $ is a basis for $ V^* $.
  \end{enumerate}
\end{proposition}
\begin{definition}
	(Dual basis) If $ V $ is finite dimensional, call $ B^* $ the \textit{dual basis} to $ B $.
\end{definition}
\pf Suppose that
\[
	\sum_{b\in B}\lambda_bb^*=0\quad\text{in } V^*.
\]
Then for $ c\in B $,
\[
	0=\left(\sum_{b\in B}\lambda_bb^*\right)(c)=\sum_{b\in B}\lambda_bb^*(c)=\lambda_c
\]
so $ \lambda_c=0 $ hence all coefficients are zero, so the set $ B^* $ is linearly independent.\\
For $ V,W $ finite dimensional we know that $ \dim(\mathcal L(V,W))=\dim(V)\dim(W) $ so for $ \dim(V^*)=\dim(\mathcal L(V,\F))=\dim V $. We know that $ B^* $ is a linearly independent subset of $ V $ of size $ \dim V $ hence it is a basis of $ V^* $.\qed\par
We can also offer a constructive proof of (ii)\par
Given $ \theta\in V^* $ and $ b\in B $ set $ \lambda_b=\theta(b)\in \F $ and let $ \theta=\sum_{b\in B}\lambda_bb^*\in V^* $. Then $ \bar\theta\in \langle B^*\rangle $ and for $ c\in B $ we have that
\[
  \bar\theta(c)=\lambda_c=\theta(c)
\]
so $ \theta=\bar\theta $ as they agree on a basis and hence we have that $ \theta\in\langle B^*\rangle $ so $ B^* $ spans $ V^* $.\par
However there are vector spaces, not finite dimensional such as $ P(\R)^*=\R^{\N} $.
\begin{corollary}
  For $ V $ finite dimensional $ V\cong V^* $
\end{corollary}
\pf Same dimension hence isomorphic.
Note that if $ V $ is finite dimensional, $ B=\{v_1,\dots, v_n\} $ is a basis of $ V $, for $ v\in V $ and $ \theta\in V^* $ we can write,
\[
	v=\sum_{i=1}^n\lambda_iv_i,\quad \theta=\sum_{j=1}^n\mu_jv_j^*.
\]
Then $ \theta(v) = \sum_{i,j}\lambda_i\mu_jv_j^*(v_i)=\sum_{i=1}^n\lambda_i\mu_i=([\theta]_{B*})^T\cdot [v]_B $
\begin{definition}
	(Annihilator) For $ V $ a finite dimensional $ \F $-vector space and $ S\subseteq V $, the \textit{annihilator} of $ S $ is
	\[
		S^0=\{\theta\in V^*:\forall s\in S,\theta(s)=0\}\subseteq V^*
	\]
\end{definition}
\begin{lemma}
  For and $ S,T\subseteq V $,
  \begin{enumerate}
	  \item $ S^0 \le V^* $;
	  \item If $ S\subseteq T $ then $ T^0\le S^0 $;
	  \item $ S^0=\langle S\rangle^0 $;
	  \item $ V^0=\{\mathbf 0_{V^*}\} $ and $ \{\mathbf 0_V\}^0=V^* $.
  \end{enumerate}
\end{lemma}
\pf A simple application of the subspace test proves (i). For (ii) is suffices to check that $ T^0\subseteq S^0 $. For $ \theta\in T^0 , s\in S	$ so $ s\in T $ hence $ \theta(s)=0 $. For (iii), $ S\subseteq \langle S\rangle $ so by (ii)
\[
  \langle S\rangle^0\le S^0.
\]
For the converse let $ \theta\in S^0 $ and $ v\in \langle S\rangle $. So we can write $ v=\sum_{s\in S}\lambda_s\cdot s $, so $ \theta(v)=\sum_{s\in S}\lamba_s\theta(s)=0 $, hence $ \theta\in\langle S\rangle^0 $ so $ S^0=\langle S\rangle^0 $. If $ \theta\in V^* $ and $ \forall v\in V $ we have that $ \theta(v)=0 $ then $ \theta $ must be the zero function, so $ V^0=\{\mathbf 0_{V^*}\} $. Secondly for $ \theta\in V^* $ we have that $ \theta(\mathbf 0_V)=0 $ so $ \{\mathbf 0_V\}^0=V^* $.\qed
\begin{proposition}
  For $ V $ finite dimensional with $ U\le V $, we have that
  \[
    \dim V=\dim U + \dim U^0
  \]
\end{proposition}
\pf Suppose that $ \dim V=n $ and $ \dim U=k $ and let $ B_U=\{v_1,\dots, v_k\} $ be a basis for $ U $ and extend to a bassi $ B_V=\{v_1,\dots, v_n\} $ for $ V $. Then $ B_v^*=\{v_1^*,\dots, v_n^*\} $ is a basis for $ V^* $. Suffices to prove the following claim.
\begin{claim}
	$ \{v^*_{k+1},\dots, v_n^* \}$ forms a basis for $ U^0 $
\end{claim} 
First we show that it's a subset of $ U^0 $. For $ i\le k $ and $ j\ge k+1 $,
\[
  v_j^*(v_i)=0
\]
so
\[
  v_j^*\in (B_U)^0=\langle B_u\rangle^0 = U^0.
\]
Linear indepedence is obvious since it's a subspace of $ B_V^* $. Let's check it's spanning. Let $ \theta\in U^0 $, so write $ \theta=\sum_{j=1}^n\lambda_jv_j^* $. Then for $ i\le k $, $ v_i\in U $, so $ 0=\theta(v_i)=\sum_j\lambda_jv_j^*(v_i)=\lambda_i $. Hence $ \theta=\sum_{j=k+1}^n\lambda_jv_j^*\in\langle v_{k+1}^*,\dots, v_n^*\rangle $.\qed
\begin{remark}
  If $ U,W\le V $ which are such that $ V=U\oplus W $ hence $ U^0\cong W^* $ is really what's going on behind the scenes.
\end{remark}
\begin{proposition}
  If $ V $ is a $ \F $-vector space  and $ U,W\le V $ then
  \begin{enumerate}
	  \item $ U^0\cap W^0=(U+W)^0 $;
	  \item $ U^0+W^0\le (U\cap W)^0 $;
	  \item If $ V $ is finite dimensional then we have equality in (ii).
  \end{enumerate}
\end{proposition}
\pf First we prove (i). For $ \theta\in V^* $ we have that $ \theta\in(U+W)^0 \iff \forall u\in U, w\in W $ we have that $ \theta(u+w)=0 $. This is equivalent to $ \forall u,\forall w, \theta(u)=\theta(w)=0 $ so $ \theta\in U^0\cap W^0 $. So we have equality.\\
Now for (ii) we have that
\[
  U\cap W\le U,W
\]
so by the previous lemma we have that
\begin{align*}
	U^0,W^0&\le (U\cap W)^0\\
	U^0+W^0&\le (U\cap W)^0\\
\end{align*}
Now for the finial part if we let $ n =\dim V $ then we have that \[
  \dim(U^0+W^0)=\dim(U^0)+\dim(W^0)-\dim(U^0\cap W^0)
\]
which using the fact that $ \dim(U^0\cap W^0)=\dim((U+W)^0) $ we get that
\begin{align*}
	&=(n-\dim U)+(n-\dim W)-(n-\dim(U+W))\\
	&=n-\dim(U\cap W)\quad\text{(by the Sum-Intersection formula)}\\
	&= \dim((U\cap W)^0) \quad\text{(by the proposition)}
\end{align*}
\begin{definition}
	(Dual map) If $ \alpha\in\mathcal L(V,W) $ then the \textit{dual map} of $ \alpha $ is $ \alpha^*:W^*\to V^* $ given by
	\[
	  \alpha^*(\theta)=\theta\circ \alpha
	\]
\end{definition}
\begin{lemma}
  If $ \alpha,\beta\in \mathcal L(V,W) $ and $ \gamma\in\mathcal L(U,V) $ and $ \lambda \in \F $ then:
  \begin{enumerate}
	  \item $ \alpha^* $ is linear;
	  \item $ (\alpha+\lambda\beta)^*=\alpha^*+\lambda\beta^* $;
	  \item $ (\alpha\circ \gamma)^*=\gamma^*\alpha^* $;
	  \item If $ \beta $ is an isomorphism then so is $ \beta^* $ and $\inv{(\beta^*)}=(\inv \beta)^* $.
  \end{enumerate}
\end{lemma}
\pf Let $ \beta, \eta \in W^* $ and $ \mu \in \F $. Then for $ v\in V $ we have that
\begin{align*}
	\alpha^*(\theta+\mu\eta(v))&=(\theta+\mu\eta)(\alpha(v))\\
				   &= \theta(\alpha(v))+\mu\eta(\alpha(v))\\
				   &= \alpha^*(\theta)(v)+\mu\alpha^*(\eta)(v)\\
				   &=(\alpha^*(\theta)+\mu\alpha^*(\eta))(v)
\end{align*}
Which holds for all $ v $ so we must have that
\[
  \alpha^*(\theta+\mu\eta)=\alpha^*(\theta)+\mu\alpha^*(\eta)
\]
For (ii) take $ \theta\in W^* $ and $ v\in V $ and consider,
\begin{align*}
	(\alpha+\lambda\beta)^*(\theta)(v)&=\theta((\alpha+\lambda\beta)(v))\\
					  &=\theta(\alpha(v)+\lambda\beta(v))\\
					  &=\theta(\alpha(v))+\lambda\theta(\beta(v))\\
					  &=\alpha^*(\theta)(v)+\lambda\beta^*(\theta)(v)\\
					  &=(\alpha^*(\theta)+\lambda\beta^*(\theta))(v).
\end{align*}
This is true for all $ v $ so we have that
\[
	(\alpha+\lambda\beta)^*(\theta)=\alpha^*(\theta)+\lambda\beta^*(\theta)=(\alpha^*+\lambda\beta^*)(\theta).
\]
Which now is true for $ \theta $ so that
\[
	(\alpha+\lambda\beta)^*=\alpha^*+\lambda\beta^*.
\]
For (iii) take $ \theta\in W^* $, so
\begin{align*}
	(\alpha\circ \gamma)^*(\theta)&=\theta\circ(\alpha\circ \gamma)\\
				      &=\alpha^*(\theta)\circ\gamma\\
				      &=\gamma^*(\alpha^*(\theta))\\
				      &=(\gamma^*\circ \alpha^*)(\theta).
\end{align*}
And again these maps both agree at all values of $ \theta $, so they are the same map.
\[
	(\alpha\circ\gamma)^*=\gamma^*\circ\alpha^*
\]
Lastly for (iv), note that for all $ \theta\in V^* $ we have,
\[
	(\id_V)^*(\theta)=\theta\circ\id_V=\theta,
\]
so $ (\id_V)^*=\id_{V^*} $. Thus $ \id_{V^*}=(\inv \beta\circ\beta)^*=\beta^*\circ (\inv\beta)^* $ and symmetrically we have that $ \id_{W^*}=(\inv\beta)^*\circ\beta^* $. Hence $ \beta^* $ is an isomorphism with our required inverse.\qed

\begin{proposition}
  Let $ V,W $ be finite dimensional vector spaces and $ \alpha\in \mathcal L(V,W) $. Let $ B,C $ be basis for $ V,W $ Then
  \[
	  [\alpha^*]_{B^*}^{C^*}=\left([\alpha]^B_C\right)^T.
  \]
\end{proposition}
\pf Let $ n=\dim V $ and $ m=\dim W $. Let $ B=\{b_1,\dots, b_n\} $ and $ C=\{c_1,\dots, c_m\} $. Let $ A=[\alpha]_C^B $ so
\[
	\alpha(b_i)=\sum_{k=1}^ma_{ki}c_k.
\]
Now let $ A'=[\alpha^*]_{B^*}^{C^*} $, so
\[
	\alpha^*(c_j^*)=\sum_{\ell =1}^na_{\ell i}'b_\ell^*.
\]
Then
\begin{align*}
	a_{i,j}'&=\alpha^*(c_j^*)(b_i) \\
		&=c_j^*(\alpha(b_i)\\
		&=c_j^*\left(\sum_ka_{kc}c_k\right)=a_{j,i}.\qed
\end{align*}
For an example let $ B,C $ be basis of $ V $ finite dimensional. The setting $ P[\id_V]_C^B $ and $ [\id_{V^*}]_{B^*}^{C^*}=P^T $ so we have that
\[
	[\id_{V^*}]^{B^*}_{C^*}=\inv{(P^T)}
\]
\begin{corollary}
  Let $ V,W $ and $ \alpha$ be as in the previous proposition. Then
  \[
    \rk(\alpha^*)=\rk(\alpha)
  \]
\end{corollary}
\pf Set $ A=[\alpha]_C^B $, and recall that the column rank of a matrix is the same as the rank of the corresponding linear transformation. Hence
\begin{align*}
	\text{c-rk}(A)&=\text{r-rk}(A)\\
		      &=\text{c-rk}(A^T)\\
		      &=\rk(\alpha^*)\qed
\end{align*}
\begin{proposition}
  Let $ V,W $ be $ \F $-vector spaces. Then if $ \alpha\in\mathcal L(V,W) $,
  \begin{enumerate}
	  \item $ \ker(\alpha^*)=\ima(\alpha)^0 $;
	  \item $ \ima(\alpha^*)\le (\ker(\alpha))^0 $;
	  \item If $ V,W $ are finite dimensional, then we have equality in (ii).
  \end{enumerate}
\end{proposition}
\pf For $ \theta\in W^* $, then
\begin{align*}
	\theta\in \ker(\alpha^*)&\iff \theta\circ\alpha=\mathbf 0 \text{ in } V^*\\
				&\iff \forall v\in V,\ \theta(\alpha(v)) =0\\
				&\iff \theta\in\ima(\alpha)^0.
\end{align*}
And for (ii), let $ \eta\in V^* $,
\begin{align*}
	\eta\in\ima(\alpha^*)&\implies\exists\theta\in W^*\st \eta=\theta\circ\alpha\\
			     &\implies \forall v\in \ker(\alpha),\ \eta(v)=\theta(\alpha(v))=\theta(\mathbf 0)=0\\
			     &\implies \eta\in(\ker(\alpha))^0.
\end{align*}
Now if we assume that $ V,W $ are finite dimensional, by the corollary we have that
\begin{align*}
	\rk(\alpha^*)=\rk(\alpha)&=\dim V-\dim(\ker\alpha)\\
				 &=\dim(\ker(\alpha)^0)\qed
\end{align*}
\subsection{The double dual}
We denote the \textit{double dual} as $ V^{**}=(V^*)^* $. If $ V $ is finite dimensional then $ V^{**}\cong V $ so they have the same dimension. But we can construct a much nicer isomorphism.
\begin{theorem}
	If $ V $ is a $ \F $-vector space then there is a linear map $ \mathcal E:V\to V^{**} $ given by
	\[
		\mathcal E(v)(\theta)=\theta(v)\quad\text{for }v\in V, \theta\in V^*
	\]
	where if $ V $ is finite dimensional then $ \varepsilon $ is an isomorphism.
\end{theorem}
\pf First we prove linearity. Take $ v,w\in V;\ \lambda\in \F;\ \theta\in V^*$. Then
\begin{align*}
	\mathcal E(v+\lambda w)(\theta)=\theta(v+\lambda w)&=\theta(v)+\lambda\theta(w)\\
							    &= \mathcal E(v)(\theta)+\lambda\mathcal E(w)(\theta)\\
							    &=(\mathcal E(v)+\lambda\mathcal E(w))(\theta)
\end{align*}
This is true for all $ \theta $ so $ \mathcal E(v+\lambda w)=\mathcal E(v)+\lambda \mathcal E(w) $.\par
Now suppose that $ V $ is finite dimensional. Now we prove injectivity. Take $ \mathbf 0\ne v\in V $ with $ \mathcal E(v)=\mathbf 0 $. i.e. $ \forall \theta\in V^*,\ \mathcal E(v)(\theta)=0 $. Extend to a basis $ \{v_1=v,v_2,\dots, v_n\} $ for $ V $. Let $ B^* =\{ v_1^*,\dots, v_n^*\} $ be a dual basis. Set $ \theta=v_1^* $, then $ 0=\theta(v)=v_1^*(v_1)=1 $, which is a contradiction. Surjectivity follows from the linear pigeonhole principle.
	\begin{definition}
		We call $ \mathcal E:V\to V^{**} $ the \textit{evaluation map} or the \textit{natural isomorphism}.
	\end{definition}
	\begin{remark}
	  For $ V $ finite dimensional we also have that $ V\cong V^* $, but any isomorphism requires a change of basis.
	\end{remark}
Note that if we let $ V,W $ be finite dimensional with basis $ B,C $ respectively, and let $ \alpha\in\mathcal L(V,W) $.
\begin{align*}
	\alpha^*(\theta)(v)&=\left([\alpha^*(\theta)]_{B^*}\right)^*\cdot[v]_B\\
			   &=(A^T[\theta]_{C^*})^T\cdot[v]_B\quad \text{where } A=[\alpha]^B_C\\
			   &= ([\theta]_{C*})^T(A[v]_B)\\
			   &= ([\theta]_{C^*})^T[\alpha(v)]_C
\end{align*}
For the rest of the chapter we assume that $ V $ is a finite dimensional $ \F $-vector space
\begin{proposition}
  Every basis $ C $ for $ V^* $ is the dual basis to some basis of $ V $.
\end{proposition}
\pf Let $ C=\{\theta_1,\dots, \theta_n\} $ and let $ C^*=\{\theta_1^*,\dots, \theta_n^*\} \subseteq V^{**}$ be the dual basis. Let $ \mathcal E:V\to V^{**} $ be the natural isomorphism. Set $ v_i=\inv{\mathcal E}(\theta_i^*)\in V $. Then $ B=\{v_1,\dots, v_n\} $ is a basis of $ V $ since the image of the basis $ C^* $ is a basis under an isomorphism. Then for all $ i,j $
\begin{align*}
	\theta_i(v_j)=\mathcal E(v_j)(\theta_i)&=\theta_j^*(\theta_i)\\
					       &=\delta_{ij}.
\end{align*}
Hence $ C $ is the dual basis to $ B $.\qed
\begin{proposition}
  For $ U\le V $,
  \[
	  \mathcal E(U)=(U^0)^0\le V^{**}
  \]
\end{proposition}
\pf For $ u\in U $ and $ \theta\in U^0 $
\begin{align*}
	\mathcal E(u)(\theta) &= \theta(u)=0
\end{align*}
True for all such $ \theta $, hence $ \mathcal E(U)\le (U^0)^0 $. But $ \dim((U^0)^0)=\dim V^*-\dim U^0=\dim(V)-(\dim V-\dim U)=\dim(\mathcal E(U)) $ hence we have that $ \mathcal E(u)=(U^0)^0 $.\qed
\begin{remark}
	It is common to identify $ V $ with $ V^{**} $ under $ \mathcal E $. Under this identification for $ U\le V $, $ U=U^{00} $.\\
	For $ X\subset V^* $, $ X^0\le V^{**} $ is identified with 
	\[
		X_0=\{v\in V:\theta(v)=0\ \forall \theta\in X\}=\bigcap_{\theta\in X}\ker\theta\le V
	\]
	Then $ \dim(X_0)=n-\dim(\langle X\rangle) $ and every $ U\le V $ is $ X_0 $ for some $ X\le V^* $, namely $ X=U^0 $.
\end{remark}
\section{Bilinear forms}
\subsection{Polynomials}
Let $ \F $ be a field.
\begin{definition}
	(Polynomial) A \textit{polynomial} $ f $ over $ \F $ is a formal expression:
	\[
		f(t)=\sum_{i=0}^na_it^i\quad n\in \Z_{\ge 0},\ a_i\in \F
	\]
\end{definition}
Then we say that $ \F[t] $ is the $ \F $ vector space of all polynomials, with a basis $ \{1,t,t^2,\dots\} $.
\begin{definition}
	(Degree) The \textit{degree} of $ f $, written, $ \deg f $ is the largest $ i $ such that $ a_i\ne 0 $. We also say that $ \deg 0 = -\infty $.
\end{definition}
We say that $ a_i $ is the \textit{leading coefficent} of the polynomial and if $ a_i=1 $, we say that the polynomial is monic.\par
We can perform addition and multiplication of polynomials in the usual sense. We have a multiplicative and additive identity ($ 1 $ and $ 0 $ respectively) and additive inverses. We can also distribute over these operations, which means that $ \F[t] $ forms a \textit{ring} (See IB Groups, Rings and Modules).\par
Note that $ \deg(f+g)\le \max(\deg(f),\deg(g)) $ and $ \deg(fg)=\deg f + \deg g $. We can write $ f\mid g $ if $ \exists h\in \F[t] $ such that $ g=fh $.\par
For $ \lambda\in \F $ write $ f(\lambda)=\sum_{i=0}^na_i\lambda^i \in \F $. We can see that evaluation resepctives addition and multiplication. We distinguish between $ \F[t] $ and the space of polynomial maps $ \F\to \F $ since if $ \F $ is finite, then $ \F[t] $ is not finite dimensional, but the space of polynomial maps is a subspace of $ \F^\F $ which is a finite dimensional vector space, so they're not even isomorphic spaces. For example if we're in the field $ \F^4 $ we can construct the polynomial
\[
	t(t-1)(t-2)(t-3)\in \F[t]
\]
which is not zero in $ \F[t] $, but when viewed as a function from $ \F^4\to\F^4 $, induces the zero map.
\begin{proposition}                                                                  (Euclidean algorithm for polynomials in $ X $) Let $ K $ be a field and $ f,g\in K[X] $. Then there exists polynomials $ r,q\in K[X] $ such that $ f=gq+r $ with $ \mathrm{deg}(r)<\mathrm{deg}(g) $.                          \end{proposition}
\textit{(From IB Groups, Rings and Modules)}\par
\pf Let $ n $ be the degree of $ f $. So $ f=\sum_{i=0}^na_iX^i $ with $ a_i\in K,a_n\ne 0 $. Similarly $ g=\sum_{i=0}^mb_iX^i $ with $ b_i\in K $ and $ b_m\ne 0 $.\par                                                               If $ n< m $ set $ q=0 $ and $ r=f $ so we're finished.\par                   If instead $ n\ge m $, proceed by induction on the degree. Let $ f_1=f-a_n\inv b_mX^{n-m}g $. Observe that $ \mathrm{deg}(f_1)<n $. If $ n=m $ then $ \mathrm{deg}(f_1)<n=m $. So write $ f=(a_{\inv b_m}X^{n-m})g+f_1 $, so we're done. Otherwise if $ n>m $, then because $ \mathrm{deg}(f_1)<n $, by induction we can write $ f_1=gq_1+r_1 $ where $ \mathrm{deg}(r_1)<\mathrm{deg}(g)=m $. Then $ f=(a_n\inv b_m)X^{n-m}g+q_1g+r_1=(a_n\inv b_mX^{n-m}+q_1)g+r_1 $\qed
\begin{corollary}
	(Bezout's Lemma) If $ f_1,\dots, f_n\in \F[t] $ have no common divisor of degree $ \ge $ 1 (i.e the $ \gcd $ is a unit) then $ \exists g_1,\dots, g_n\in \F[t] $ such that
	\[
		\sum_{i=1}^nf_ig_i=1\quad\in \F[t].
	\]
\end{corollary}
\pf Same as in $ \Z $.
\begin{lemma}
  For $ \lambda\in \F $,
  \[
    f(\lambda)=0\iff (t-\lambda)\mid f(t).
  \]
\end{lemma}
\pf Apply the Euclidian algorithm to $ f(t) $ and $ g(t)=t-\lambda $.
\begin{definition}
	(Root) $ \lambda\in \F $ is a \textit{root} of $ f\in \F[t] $ of \textit{multiplicity} greater than $ e $ if
	\[
		(t-\lambda)^e\mid f(t).
	\]
\end{definition}
\begin{corollary}
  If $ \deg f = n\ge 0 $, then $ f $ has at most $ n $ roots counted with multiplicity.
\end{corollary}
\begin{corollary}
  If $ \deg f,\deg g < n $, and there exists $ \lambda_1,\dots, \lambda_n\in \F $ distinct such that $ f(\lambda_i)=g(\lambda_i) $ for $ 1\le i\le n $, then $ f=g $.
\end{corollary}
\begin{theorem}
	(Fundamental Theorem of Algebra) Every $ f\in \C[t] $ of $ \deg f\ge 1 $ has exactly $ n $ roots counting multiplicity. i.e. $ f $ is a product of polynomials of degree $ 1 $.
\end{theorem}
\pf IB Complex Analysis

\section{Eigenspaces}
\begin{definition}
	(Diagonalisable) Let $ V $ be a finite dimensional vector space, and $ \alpha\in\mathcal L(V,V) $. Then $ \alpha $ is \textit{diagonalisable} if there exists a basis $ B $ of $ V $ such that $ [\alpha]^B_B $ is a diagonal matrix.
\end{definition}
\begin{definition}
	(Triangularisable) Let $ V $ be a finite dimensional vector space, and $ \alpha\in\mathcal L(V,V) $. Then $ \alpha $ is \textit{triangularisable} if there exists a basis $ B $ of $ V $ such that $ [\alpha]^B_B $ is an upper-triangular matrix.
\end{definition}
A matrix $ A\in M_{n\times n}(\F) $ is diagonalisable or triangularisable if $ A $ is similar to a diagonal or upper triangular matrix respectively.
\begin{remark}
	By change of basis, $ \alpha $ is diagonalisable or triangularisable if and only if for any basis $ B $ of $ V $, $ [\alpha]^B_B $ is a diagonalisable or respectively triangularisable matrix.
\end{remark}
\begin{enumerate}
	\item A triangularisable matrix has a very easy to compute determinate; we can just take the product of the entries in the leading diagonal.
	\item A diagonalisable matrix makes it easier to understand its similarity class.
\end{enumerate}
\begin{remark}
	If $ B=\{v_1,v_2,\dots, v_n\} $ is such that
	\[
		[\alpha]_B^B=\begin{pmatrix}
			\lambda_1 & & \\
				  & \ddots & \\
				  & & \lambda_n
		\end{pmatrix}
	\]
	then $ \alpha(v_i)=\lambda_iv_i $ for all $ 1\le i\le n $.
\end{remark}
\begin{definition}
	(Eigenvalues, Eigenvectors, and Eigensapces) Let $ V $ be a $ \F $-vector space and $ \alpha\in\mathcal L(v,v) $. An element $ \lambda\in \F $ is an \textit{eigenvalue} of $ \alpha $ if there exists some $ v\in\V $ non-zero such that $ \alpha(v)=\lambda v $. Such a vector is a $ \lambda $-\textit{eigenvector} of $ \alpha $.
	\[
		V_\lambda=\{v\in V:\alpha(v)=\lambda (v)\}
	\]
	is the $ \lambda $-\textit{eigenspace} of $ \alpha $.
\end{definition}
\begin{remark}
	We can make some remarks from this definition.
  \begin{enumerate}
	  \item $ V_\lambda=\ker(\alpha-v\id_V)\le V $;
	  \item For $ V $ finite dimensional, $ \alpha $ is diagonalisable if and only if $ V $ has a basis of eigenvectors of $ \alpha $.
	  \item If $ v\in V_\mu $ for some $ \lambda\ne\mu $,
		  \[
			  (\alpha-\lambda\id_V)(v)=(\mu-\lambda)(v).
		  \]
		  Thus $ (\alpha-\lambda\id_V) $ preserves $ V_\mu $ and $ (\alpha-\lambda\id_V)|_{V\mu} $ is invertiable with inverse $ v\to \inv{(\mu-\lambda)}v $.
  \end{enumerate}
\end{remark}
\begin{lemma}
	If $ \lambda_1,\dots, \lambda_k\in\F $ are distinct and $ \mathbf 0 \ne v_i\in V_{\lambda_i} $ then $ \{v_1,\dots, v_k\} $ is linearly independent.
\end{lemma}
\pf Suppose that $ \{v_1,\dots, v_k\} $ is linearly dependent. Let
\[
	\sum_{i=0}^k\mu_iv_i=\mathbf 0\quad\text{in } V
\]
and \textit{wlog} we have that $ \mu_1\ne 0 $. Let
\[
	\beta=\prod_{i=2}^k(\alpha-\mu_i\id_V).
\]
Then by remark (iii) we have that
\[
	\beta(v_j)=\left(\prod_{i=2}^k(\lambda_j-\lambda_i)\right)\cdot v_j\ne \mathbf 0
\]
if and only if $ j=1 $. Thus,
\[
	\mathbf 0 = \beta\left(\sum_{i=1}^n\mu_iv_i\right)=\left(\mu_1\prod_{i=2}^k(\lambda_1-\lambda_i)\right)v_1
\]
so $ \mu_1=0 $, which is a contradiction.\qed
\begin{corollary}
  If $ V $ is a finite dimensional vector space, then every $ \alpha\in\mathcal L(V,V) $ has only finitely many eigenvalues.
\end{corollary}
\begin{proposition}
  Let $ V $ be a finite dimensional $ \F $-vector space, and let $ \alpha\in\mathcal L(V,V) $. Let $ \lambda_1,\dots, \lambda_k\in\F $ be all the eigenvalues of $ \alpha $. Then,
  \begin{enumerate}
	  \item $ \langle V_{\lambda_1}\cup \cdots \cup V_{\lambda_k}\rangle= V_{\lambda_1}\oplus \cdots \oplus V_{\lambda_k} $;
	  \item $ \alpha $ is diagonalisable if and only if $ V=V_{\lambda_1}\oplus \cdots \oplus V_{\lambda_k} $.
  \end{enumerate}
\end{proposition}
\pf 
\begin{enumerate}
	\item If not, then $ \exists v_j\in V_{\lambda_j} $ not all zero such that
		\[
		  \sum_jv_j=\mathbf 0
		\]
		which is a contradiction from the lemma.
	\item Suppose that $ \alpha $ is diagonalisable. By the remark $ V $ has a basis of eigenvectors, so the $ V_{\lambda_i} $ spans $ V $. Conversely, suppose that $ V=V_{\lambda_1}\oplus \cdots \oplus V_{\lambda_k} $. Let $ B_i $ be a basis of $ V_{\lambda_i} $. Then by Example Sheet 1 we have that
		\[
			\bigcup_{i=1}^k B_i=B
		\]
		is a basis for $ V $, hence $ V $ has a basis of eigenvectors, so $ \alpha $ is diagonalisable by the remark.\qed
\end{enumerate}
Recall that if $ V $ is a finite dimensional vector space, then any $ \beta\in\mathcal L(V,V) $ we have that
\[
	\det(B)=0\iff \ker(B)\ne \{\mathbf 0_V\}.
\]
Thus $ \lambda\in \F $ is an eigenvalue of $ \alpha\in\mathcal L(v,v) $ if and only if
\[
  \det(\alpha-\lambda\id_V)=0.
\]
\begin{definition}
	(Characteristic polynomial) For $ A\in M_{n\times n}(\F) $, the \textit{characteristic polynomial} of $ A $ is
	\[
		\chi_A(t)=\det(tI_n-A)\in \F[t].
	\]
\end{definition}
Similarly for $ \alpha\in\mathcal L(V,V) $ with $ V $ finite dimensional we can define
\[
  \chi_\alpha(t)=\det(t\cdot\id_V-\alpha)
\]
as the characteristic polynomial of $ \alpha $.
\begin{lemma}
  For $ V $ a finite dimensional vector space, and $ \alpha\in\mathcal L(,V) $, we have that the set of roots of $ \chi_A(t) $ are exactly the set eigenvalues of $ \alpha $.
\end{lemma}
\begin{remark}
  We can make some remarks about the characteristic polynomial.
  \begin{enumerate}
	  \item $ \chi_\alpha $ is monic of degree $ \dim V $.
	  \item Similar matrices have the same characteristic polynomial.
	  \item  By the Leibniz formula, if $ \chi_\alpha(t)=t^n+C_{n-1}t^{n-1}+\cdots + C_1t+C_0 $ we have that
		  \begin{enumerate}
			  \item $ C_{n-1}=\tr(\alpha) $,
				  $ C_0=(-1)^n\det \alpha=\chi_\alpha(0) $.
		  \end{enumerate}
  \end{enumerate}
\end{remark}
\begin{proposition}
  If $ \alpha $ is triangularisable then $ \chi_\alpha(t) $ can be written as aproduct of linear factors.
\end{proposition}
\pf Let $ B $ be a basis for $ V $ such that
\[
	A=[\alpha]^B_B=\begin{pmatrix}
		a_{11} & a_{12} & \cdots & a_{1n} \\
		0 & a_{22} & \cdots & a_{2n} \\
		\vdots & \vdots & \ddots & \vdots \\
		0 & \cdots & 0 & a_{nn}
	\end{pmatrix}.
\]
Then
\begin{align*}
	\chi_\alpha(t) = \prod_{i=1}^n(t-a_i).
\end{align*}
For example the matrix,
\[
  A=\begin{pmatrix}
	  \cos\theta & -\sin\theta \\
	  \sin\theta & \cos\theta
  \end{pmatrix}.
\]
Then $ \chi_A(t)=t^2-(2\cos\theta)t+1 $ is irreducible for $ \theta\in(0,\pi) $, hence $ A $ is not triangularisable in $ M_{2\times 2}(\R) $.
	\begin{theorem}
		Every $ A\in M_{n\times n}(\C) $ is triangularisable.`
	\end{theorem}
	\pf By the fundamental theorem of algebra, $ \chi_A(t) $ has a root $ \lambda\in \C $, so there exists a $ v_1 $, non-zero in $ \C^n $ such that $ Av_1=\lambda v_1 $. Extend to a basis $ B=\{v_1,\dots, v_n\} $ of $ \C^n $. So up to a change of basis, we can assume that
	\[
	  A=\begin{pmatrix}
		  \lambda_1 & a_{12} & \cdots & a_{1n} \\
		  0 &  & & \\
		  \vdots &  & C\\
		  0 & & & 
	  \end{pmatrix}
	\]
	where $ C $ is a $ (n-1)\times (n-1) $ matrix. By induction there exists a $ \bar P\in GL_{n-1}(\C) $ such that $ \bar PC\inv{\bar P} $ is upper triangular. So set
	\[
	  P=\begin{pmatrix}
		  \lambda_1 & 0 & \cdots & 0 \\
		  0 &  & & \\
		  \vdots &  & \bar P & \\
		  0 & & & 
	  \end{pmatrix}.
	\]
Then $ PA\inv P $ is upper-triangular.
	\begin{corollary}
	  Let $ V $ be a finite dimensional $ \C $-vector space. Then every $ \alpha\in \mathcal L(V,V) $ is triangularisable.
	\end{corollary}
\begin{remark}
	The same proof yields the result for any field $ \F$, $ A\in M_{n\times n}(\F) $ is triangularisable if and only if $ \chi_A(t) $ is a product of linear factors.
\end{remark}
From now on we'll assume all vector spaces are finite dimensional and $ \alpha\in \mathcal L(V,V) $.\par
Let's introduce some notation. For a polynomial
\[
	p(t)=\sum_{k=0}^n\mu_it^i
\]
we can introduce the linear map
\[
	p(\alpha)=\sum_{i=0}^n\mu_i\alpha^i\in\mathcal L(V,V).
\]
Similarly for $ A\in M_{k\times k}(\F) $ we have that $ p(A)=\sum_{i=0}^n\mu_iA^i $.
\begin{remark}
  We have some remarks about these new linear maps.
  \begin{enumerate}
	  \item If $ B $ is a basis for $ V $ then
		  \[
			  [p(\alpha)]^B_B=p([\alpha]^B_B).
		  \]
	  \item If $ \dim V=n $ then since we know that $ \dim(\mathcal L(V,V))=n^2 $, so $ \{\id_V,\alpha^1,\alpha^2,\dots, \alpha^{n^2}\} $ is linearly dependent. So there exists $ \mu_i\in\F $ such that,
		  \[
			  \sum_{i=0}^{n^2}\mu_i\alpha^i=0=p(\alpha)
		  \]
		  where $ p(t)=\sum_{i=0}^{n^2}\mu_it^i\ne 0 $ in $ \F[t] $.
  \end{enumerate}
\end{remark}
Let
\[
I_\alpha=\{p(t)\in \F[t]: p(\alpha)=0 \}\triangleleft \F[t]. 
\]
So by the remark we know that $ I_\alpha\ne \{0\} $. Let $ m_\alpha\in I_\alpha $ be a non-zero element of minimal degree. Rescaling we can assume that $ m_\alpha $ is monic.
\begin{definition}
(Minimal polynomial) We call $ m_\alpha $ the \textit{minimial polynomial}.
\end{definition}
\begin{proposition}
  The minimial polynomial is unique.
\end{proposition}
First we have to prove a lemma.
\begin{lemma}
  For any $ f\in I_\alpha $, we have that $ m\alpha\mid f $.
\end{lemma}
\pf Apply the Euclidean algorithm to $ \F[t] $. So there exists $ r,q\in \F[t] $ such that $ \deg r<\deg m_\alpha $, and $ f=qm_\alpha+r $. Then $ r(\alpha)+f(\alpha)-q(\alpha)m_\alpha(\alpha)=0 $. Hence $ r\in I_\alpha $, so since it has a degree less than $ m_\alpha $ we must have that $ r=0 $. Or we could prove the statement since $ \F[t] $ is a principal ideal domain hence we can show that $ I=(m_\alpha) $.\qed
\par
Now we can prove the proposition\par
\pf Let $ \bar m_\alpha $ be another non-zero element of minimial degree in $ I_\alpha $, monic. By the lemma we have that $ m_\alpha\mid \bar m_\alpha $, but $ \deg(m_\alpha)=\deg(\bar m_\alpha) $ so there exists a $ \lambda\in \F $ such that $ \bar m_\alpha=\lambda m_\alpha $. Hence $ \lambda=1 $, so they are the same polynomial.\qed
\begin{theorem}
	(Cayley-Hamilton Theorem) For $ \alpha\in \mathcal L(V,V) $ we have that
	\[
	  \chi_\alpha(\alpha)=0.
	\]
\end{theorem}
Let's give a proof valid for $ \F=\C $. The general proof for an arbitrary proof is a bonus exercise on Example Sheet 3.\par
\pf We know that there is a basis $ B $ for $ V $ such that when $ \alpha $ is written as a matrix in $ B $ it is upper-triangular. So we have that
\[
	\chi_\alpha(t)=\prod_{i=1}^n(t-\lambda_i)
\]
where $ \lambda_i $ is the element in the $ i $th row of the leading diagonal. Set $ U_j=\langle v_1,\dots, v_j\rangle $, so we have that $ U_0=\{\mathbf 0_V\} $, and $ U_n=V $.\\
Then for $ 1\le j\le n $ we have that $ (\alpha_\lambda_j\id_V)(U_j)\le U_{j-1} $ thus
\begin{align*}
	\chi_\alpha(\alpha)(V)&=(\alpha-\lambda_i\id_V)\circ \dots \circ (\alpha_\lambda_n\id_V)(U_n)\\&
	=U_0=\{\mathbf 0_V\}.
\end{align*}
so $ \chi_\alpha(\alpha) $.\qed
\par
Our proof is valid for $ \alpha $ triangularisable over every field.
\begin{corollary}
  $ m_\alpha\mid \chi_\alpha $.
\end{corollary}
\begin{definition}
	(Algebraic multiplicity) Let $ \lambda\in \F $ be an eigenvalue of $ \alpha $. The \textit{algebraic multiplicity} $ a_\lambda $ of $ \alpha $ is the multiplicity of $ \lambda $ as a root of $ \chi_\alpha(t) $.
\end{definition}
\begin{definition}
	(Geometric multiplicity) Let $ \lambda\in \F $ be an eigenvalue of $ \alpha $. The \textit{geometric multiplicity} $ g_\lambda $ of $ \lambda $ is $ \dim(V_\lambda) $.
\end{definition}
\begin{proposition}
	Let $ \lambda\in\F $ be an eigenvalue of $ \alpha $ then we have that 
	\[
	  g_\lambda\le a_\lambda.
	\]
\end{proposition}
\pf Let $ \{v_1,\dots, v_k\} $ be a basis for $ V_\lambda $. Extend to a basis $ B=\{v_1,\dots, v_n\} $ for $ V $. Then,
\[
	[\alpha]_B^B=\begin{pmatrix}
		\lambda I_k & B \\
		0 & C
	\end{pmatrix}
\]
is a block triangular matrix, so $ \chi_\alpha(t)=(t-\lambda)^k\chi_t(t) $. Hence we have that $ a_\lambda \ge k=g_\lambda $.\qed
\begin{lemma}
  Let the eigenvalues of $ \alpha $ be $ \lambda_1,\dots, \lambda_k\in \F $.
  \begin{enumerate}
	  \item \[
			  \sum_{i=1}^k g_{\lambda_i}\le \dim V
	  \]
	  with equality if and only if $ \alpha $ is diagonalisable.
  \item
	  \[
		  \sum_{i=1}^ka_{\lambda_i}\le \dim V
	  \]
	  with equality if and only if $ \alpha $ is triangularisable.
  \end{enumerate}
\end{lemma}
\pf 
\begin{enumerate}
	\item We have that $ V_{\lambda_1}\oplus \cdots \oplus V_{\lambda_k}\le V $ with equality if and only if $ \alpha $ is diagonalisable. hence the result is clear.
	\item We write $ \chi_\alpha(t)=f(t) \prod_{i=1}^k(t-\lambda_i)^{a_{\lambda_i}} $. Then $ f(t) $ has no linear factors, since a root of $ \chi_\alpha $ is an eigenvalue. Moreover 
		\begin{align*}
			\dim(V)&=\dim(\chi_\alpha)\\
			       &=\deg(f)+\sum_{i=1}^ka_{\lambda_i}\\
			       &\le \sum_{i=1}^ka_{\lambda_i}.
		\end{align*}
		with equality if and only if $ \chi_\alpha $ is a product of linear factors if and only if $ \alpha $ is triangularisable.\qed
\end{enumerate}
\begin{proposition}
  For $ \F=\C $, $ \alpha $ is diagonalisable if and only if $ a_\lambda=g_\lambda $ for every eigenvalue $ \lambda $ of $ \alpha $.
\end{proposition}
\pf Since we're working over $ \C $ we know that $ \alpha $ is triangularisable. Hence
\[
	\sum_{i=1}^ka_{\lambda_i}=\dim V.
\]
By the above proposition we have that
\[
	\sum_{i=1}^k g_{\lambda_i}\le \sum_{i=1}^k a_{\lambda_i}=\dim V.
\]
with equality if and only if $ a_{\lambda_i}=g_{\lambda_i} $ for all $ i $. But this occurs if and only if $ \alpha $ is diagonalisable.\qed
\begin{remark}
	For $ p(t)=\sum_{i=1}^n\mu_it^i $, with $ v\in V_{\lambda} $.
	\[
		p(\alpha)(v)=\sum_{i=1}^n\mu_i\alpha^i(v)=\sum_{i=1}^n\mu_i\lambda^iv=p(\lambda)v.
	\]
\end{remark}
\begin{lemma}
  For any $ \lambda \in \F $, $ \lambda $ is a root of $ m_\alpha $ is and only if it is a root of $ \chi_\alpha $.
\end{lemma}
\pf For the forward direction if $ \lambda $ is a root of $ m_\alpha(t) $, by Cayley-Hamilton,
\begin{align*}
	(t-\lambda)\mid m_\alpha(t)\mid \chi_\alpha(t).
\end{align*}
Conversely if $ \chi_\alpha(\lambda)=0 $ then $ \lambda $ is an eigenvalue of $ \alpha $, so let $ \mathbf 0 \ne v\in V_\lambda $. Then $ m_\alpha(\alpha)(v)=m_\alpha(\lambda)\cdot v $. But $ m_\alpha(\alpha)=0 $, so since $ v\ne \mathbf 0 $, we must have that $ m_\alpha(\lambda)=0 $.\qed
\par
We will introduce the notation that $ C_\lambda $ is the multiplicity of $ \lambda $ as a root of $ m_\alpha(t) $.
\begin{remark}
  By Cayley-Hamilton, if $ \lambda $ is an eigenvalue, then $ 1\le C_\lambda \le a_\lambda $.
\end{remark}
However there is no useful relationship between $ C_\lambda $ and $ g_\lambda $. Let's see this in two examples.
\begin{enumerate}
	\item Let $ A=\lambda I_n $. Then $ \chi_A(t)=(t-\lambda)^n $, $ V_k=\F^n $, so $ m_\alpha(t)=t-\lambda $. Hence $ C_\lambda=1 $, $ g_\lambda=n $, $ a_\lambda =n$.
	\item Let
		\[
		  A=\begin{pmatrix}
			  \lambda & 1 & 0 & \cdots & 0\\
			  0 & \lambda & 1 & \cdots & 0\\
			  \vdots & \vdots & \ddots & \ddots & \vdots\\
			  \vdots &\vdots & \vdots & \ddots & 1 \\
			  0 & \cdots & \cdots & 0 & \lambda
		  \end{pmatrix}.
		\]
		Then $ \chi_A(t)=(t-\lambda)^n $, so
		\[
		  V_\lambda=\langle \mathbf e_1\rangle,
		\]
		so 
		\[
			(A-\lambda I_n)^{n-1}\ne 0
		\]
		hence $ a_\lambda=n $, $ g_\lambda=1 $, $ C_\lambda =n $, as $ m_\alpha=\chi_\alpha $.
\end{enumerate}























\end{document}


