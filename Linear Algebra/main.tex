\documentclass{article}
\usepackage{../header}
\title{Linear Algebra}
\author{Notes by Finley Cooper}
\newcommand{\F}{\mathbb{F}}
\newcommand{\n}{\mathrm{n}}
\newcommand{\rk}{\mathrm{rk}}
\newcommand{\Col}{\mathrm{Col}}
\newcommand{\Row}{\mathrm{Row}}
\begin{document}
  \maketitle
  \newpage
  \tableofcontents
  \newpage
  \section{Vector Spaces}
  \subsection{Definitions}
  For this lecture course, $ \F $ will always be field.
  \begin{definition}
	  (Vector Space) A $ \F $\textit{-vector space} (or a vector space over $ \F $) is an abelian group $ (V,+,\boldsymbol 0) $ equipped with a function
	  \begin{align*}
	    \F\times V\to V \\
	    (\lambda, v)\to \lamda v
	  \end{align*}
	  which we call scalar multiplication such that $ \forall v,w\in V, \forall \lambda,\mu\in\F $
	  \begin{enumerate} 
		  \item $ (\lambda + \mu)v=\lambda v + \mu v $
		  \item $ \lambda(v + w)=\lambda v + \lambda w $
		  \item $ \lambda(\mu v)=(\lambda \mu)v $
		  \item $ 1\cdot v = v\cdot 1 = v $
	  \end{enumerate}
  \end{definition}
  Remember that $ \boldsymbol 0 $ and $ 0 $ are not the same thing. $ 0 $ is an element in the field $ \F $ and $ \boldsymbol 0 $ is the additive identity in $ V $.\par
  For an example consider $ \F^n $ n-dimensional column vectors with entries in $ \F $. We also have the example of a vector space $ \C^n $ which is a complex vector space, but also a real vector space (taking either $ \C $ or $ \R $ as the underlying scalar field).\par
  We also can see that $ M_{m\times n}(\F) $ form a vector space with $ m $ rows and $ n $ columns.\\
  For any non-empty set $ X $, we denote $ \F^X $ as the space of functions from $ X $ to $ \F $ equipped with operations such that:
\begin{align*}
	f+g \text{ is given by } (f+g)(x)=f(x)+g(x)\\
	\lambda f \text{ is given by } (\lambda f)(x)=\lambda f(x)
\end{align*}
\begin{proposition}
  For all $ v\in V $ we have that $ 0\cdot v = \boldsymbol 0 $ and $ (-1)\cdot v=-v $ where $ -v $ denotes the additive inverse of $ v $.
\end{proposition}
\pf Trivial.
\begin{definition}
	(Subspace) A \textit{subspace} of a $ \F $-vector space $ V $ is a subset $ U\subseteq V $ which is a $ \F $-vector space itself under the same operations as $ V $. Equivalently, $ (U,+) $ is a subgroup of $ (V,+) $ and $ \forall \lambda\in \F $, $\forall u\in U $ we have that $ \lambda u \in U $.
\end{definition}
\begin{remark}
  Axioms (i)-(iv) are always automatically inherited into all subspaces.
\end{remark}
\begin{proposition}
	(Subspace test) Let $ V $ be a $ \F $-vector space and $ U\subseteq V $ then $ U $ is a subspace of $ V $ if and only if,
	\begin{enumerate}
		\item $ U $ is nonempty.
		\item $ \forall \lambda\in\F $ and $ \forall u,w\in U $ we have that $ u+\lambda w \in U $.
	\end{enumerate}
\end{proposition}
\pf If $ U $ is a subspace then $ U $ satisfies (i) and (ii) since it contains $ 0 $ and is closed. Conversely suppose that $ U\subseteq V $ satisfies (i) and (ii). Taking $ \lambda = -1 $ so $ \forall u,w\in V $, $ u-w\in U $ hence $ (U,+) $ is a subgroup of $ (V,+) $ by the subgroup test. Finally taking $ u=\boldsymbol 0 $ so we have that $ \forall w\in U,\forall\lambda\in \F $ we have that $ \lambda w\in U $. So $ U $ is a subspace of $ V $.\qed\par
We notate $ U $ by $ U\le V $.\par
For some examples
\begin{enumerate}
	\item \[
	\left\{\begin{pmatrix}
			x \\
			y \\
			z \\
	\end{pmatrix}\in \R^3:x+y+z=t\right\}\subseteq \R^3, 
\]
for fixed $ t\in \R $ is a subspace of $ \R^3 $ iff $ t = 0 $.\par
\item Take $ \R^\R $ as all the functions from $ \R $ to $ \R $ then the set of continuous functions is a subspace.
\item Also we have that $ C^\infty(\R) $, the set of infintely differentiable functions from $ \R $ to $ \R $ is a subspace of $ \R^\R $ and the subspace of continuous functions.
\item A further subspace of all of those subspaces is the set of polynomial functions.
\end{enumerate}
\begin{lemma}
  For $ U,W\le V $ we have that $ U\cap W\le V $.
\end{lemma}
\pf We'll use the subspace test. Both $ U,W $ are subspaces so they contain $ \boldsymbol 0 $ hence $ \boldsymbol 0\in U\cap W $ so $ U\cap W $ is nonempty. Secondly take $ x,y\in U\cap W $ with $ \lambda \in \F $. Then $ U\le V $ and $ x,y\in U $ so $ x+\lambda y\in U $. Similarly with $ W $ so $ x+\lambda y \in W $ hence we have that $ x+\lambda y \in U \cap W $ hence $ U\cap W\le V $\qed
\begin{remark}
  This does not apply for subspaces, in fact from IA Groups, we know it doesn't even hold for the underlying abelian group.
\end{remark}
\begin{definition}
	(Subspace sum) For $ U,W\le V $, the \textit{subspace sum} of $ U, W $ is
	\[
		U+W=\{u+w:u\in U, w\in W\}.
	\]
\end{definition}
\begin{lemma}
  If $ U, W\le V $ then $ U+W\le V $.
\end{lemma}
\pf Simple application of the subspace test.
\begin{remark}
  $ U+W $ is the smallest subgroup of $ U, W $ in terms of inclusion, i.e. if $ K $ is such that $ U\subseteq K $ and $ W\subseteq K $ then $ U+W\subseteq K $.
\end{remark}
\subsection{Linear maps, isomorphisms, and quotients}
\begin{definition}
	(Linear map) For $ V $, $ W $ $ \F $-vector spaces. A \textit{linear map} from $ V $ to $ W $ is a group homomorphism, $ \varphi $, from $ (V,+) $ to $ (W,+) $ such that $ \forall v\in V $
	\[
	  \varphi(\lambda v) = \lambda\varphi(v)
	\]
\end{definition}
Equivalently to show any function $\alpha: V\to W $ is a linear map we just need to show that $ \forall u,w\in V $, $ \forall \lambda \in\F $ we have
\[
  \alpha(u+\lambda w)=\alpha(u)+\lambda\alpha(w).
\]
For some examples of linear maps
\begin{enumerate}
	\item $ V=\F^n, W=\F^m $ $ A\in M_{m\times n}(\F) $. Then let $ \alpha:V\to W $ be given by $ \alpha(v)=Av $. Then $ \alpha $ is linear.
	\item $ \alpha:C^\infty(\R)\to C^\infty(\R) $ defined by taking the derivative.
	\item $ \alpha: C(\R)\to \R $ defined by taking the integral from $ 0 $ to $ 1 $.
	\item $ X $ any nonempty  set, $ x_0\in X $,
		\begin{align*}
		  \alpha:\F^X\to \F \\
		  f\to f(x_0)
		\end{align*}
	\item For any $ V,W $ the identity mapping from $ V $ to $ V $ is linear and so is the zero map from $ V $ to $ W $.
	\item The composition of two linear maps is linear.
	\item For a non-example squaring in $ \R $ is not linear. Similiarly adding constants is not linear, since linear maps preserve the zero vector.
\end{enumerate}
\begin{definition}
	(Isomorphism) A linear map $ \alpha:V\to W $ is an \textit{isomorphism} if it is bijective.\par
	We say that $ V $ and $ W $ are isomorphic, if there exists an isomorphism from $ V\to W $ and denote this by $ V\cong W $.
\end{definition}
An example is the vector space $ V=\F^4 $ and $ W=M_{2\times 2}(\F) $ we can define the map
\begin{align*}
	\alpha:V &\to W\\
	        \begin{pmatrix}
	         a\\b\\c\\d
	       \end{pmatrix}
		 &\to \begin{pmatrix}
		       a & b \\
		       c & d 
	       \end{pmatrix}
\end{align*}
Then $ \alpha $ is an isomorphism.
\begin{proposition}
  If $ \alpha: V\to W $ is an isomorphism then $ \inv\alpha:W\to V $ is also an isomorphism.
\end{proposition}
\pf Clearly $ \inv\alpha $ is a bijection. We need to prove that $ \inv\alpha $ is linear. Take $ w_1,w_2\in W $ and $ \lambda\in \F $. So we can write $ w_i=\alpha(v_i) $ for $ i=1,2 $. Then \[ \inv\alpha(w_1+\lambda w_2)=\inv \alpha(\alpha(v_1)+\lambda\alpha(v_2))=\inv\alpha(\alpha(v_1+\lambda v_2))=v_1+\lambda v_2=\inv \alpha(w_1)+\lambda\inv \alpha(w_2) \]. Hence $ \inv\alpha $ is linear, so $ \inv\alpha $ is an isomorphism.\qed 
\begin{definition}
	(Kernal) Let $ V,W $ be $ \F $-vector spaces. Then the \textit{kernel} of the linear map $ \alpha:V\to W $ is
	\[
		\ker(\alpha)=\{v\in V:\alpha(v)=\mathbf 0_W\}\subseteq V
	\]
\end{definition}
\begin{definition}
	(Image) Let $ V,W $ be $ \F $-vector spaces. Then the \textit{image} of a linear map $ \alpha:V\to W $ is
	\[
		\ima(\alpha)=\{\alpha(v):v\in V\}\subseteq W
	\]
\end{definition}
\begin{lemma}
  For a linear map $ \alpha:V\to W $ the following hold.
  \begin{enumerate}
	  \item $ \ker\alpha\le V $ and $ \ima \alpha \le W $
	  \item $ \alpha $ is surjective if and only if $ \ima \alpha =W $
	  \item $ \alpha $ is injective if and only if $ \ker\alpha=\{\mathbf 0_V\} $
  \end{enumerate}
\end{lemma}
\pf $ \mathbf 0_V+\mathbf 0_V=\mathbf 0_V $, so applying $ \alpha $ to both sides any using the fact that $ \alpha $ is linear gives that $ \alpha(\mathbf 0_V)=\mathbf 0_W $. So $ \ker\alpha $ is nonempty. The rest of the proof is a simple application of the subspace test.\\
The second statement is immediate from the definition.\\
For the final statement suppose $ \alpha $ injective. Suppose $ v\in \ker \alpha $. Then $ \alpha(v)=\mathbf 0_W=\alpha(\mathbf 0_w) $ so $ v=\mathbf 0_V $ by injectivity. Hence $ \ker\alpha $ is trivial.
Conversely suppose that $ \ker \alpha=\{0_V\} $ Let $ u,v\in V $ and suppose that $ \alpha(u)=\alpha(v) $. The $ \alpha(u-v)=\mathbf 0_W $, so $ u-v\in\ker\alpha $, so $ u=v $.\qed\par

For $ V $ a $ \F $-vector space, $ W\le V $ write
	\[
		\frac VW=\{v+W:v\in V\}
	\]
	as the left cosets of $ W $ in $ V $. Recall that two cosets $ v + V $ and $ u+W $ are the same coset if and only if $ v-u\in W $.
\begin{proposition}
  $ V/W $ is an $ \F $-vector space under operations
  \begin{align*}
   (u+W)+(v+W)&=(u+v)+W \\
   \lambda (v+W)&=(\lambda v)+W
\end{align*}
We call $ V/W $ the quotient space of $ V $ by $ W $.
\end{proposition}
\pf The proof is long and requires a lot of vector space axioms so we'll just sketch out the proof.\\
We check that operations are well-defined, so for $ u,\overline u, v,\overline v \in V $ and $ \lambda\in \F $ if
\[
  u+W=\overline u + W,\quad v+W=\overline v + W
\]
then
\[
	(u+v)+W=(\overline u+\overline w)+W
\]
and
\[
	(\lambda u) + W = (\lambda\overline u)+W
\]
The vector space axioms are inherited from $ V $.\qed
\begin{proposition}
	(Quotient map) The function $ \pi_W: V\to \frac VW $ called a \textit{quotient map} is given by
  \[
    \pi_W(v)=v+W
  \]
  is a well-defined, surjective, linear map with $ \ker\pi_W=W $.
\end{proposition}
\pf Surjectivity is clear. For linearity let $ u,v \in V $ and $ \lambda\in \F $. Then 
\begin{align*}
	\pi_W(u+\lambda v)&=(u+\lambda v) + W \\
			  &= (u+W)+(\lambda v+W)\\
			  &= (u+W)+\lambda(v+W) \\
			  &= \pi_W(u)+\lambda\pi_W(v)
\end{align*}
For $ v\in V $, we have that  $ v\in\ker \pi_W \iff \pi_W(v) = \mathbf 0_{V/W} $. So $ v+W=\mathbf 0_V+W $ so finally $ v=v-\mathbf 0_V\in W $.\qed
\begin{theorem}
	(First isomorphism theorem) Let $ V,W $ be $ \F $-vector spaces and $ \alpha:V\to W  $ linear. Then there is an isomorphism
	\[
		\overline\alpha:\frac{V}{\ker \alpha}\to\ima\alpha
	\]
	given by $ \overline\alpha(v+\ker\alpha)=\alpha(v) $
\end{theorem}
\pf For $ u,v\in V $,
\begin{align*}
	u+K=v=K &\iff u-v\in K
		&\iff \alpha(u-v)=\mathbf 0_W
		&\iff \alpha(u)=\alpha(v)
		&\iff \overline \alpha(u+\ker\alpha)=\overline\alpha(v+\ker\alpha)
\end{align*}
The forward direction shows that $ \overline\alpha $ is well-defined, and the converse shows that $ \overline\alpha $ is injective. For surjectivity given $ w\in\ima\alpha $, there exists some $ v\in V \st w=\alpha(v)$. Then $ w=\overline\alpha(v+\ker\alpha) $.\\
Finally for linearity given $ u,v\in V $, $ \lambda\in\F $,
\begin{align*}
	\overline\alpha((u+\ker\alpha)+\lambda(v+\ker\alpha))&=\overline\alpha((u+\lambda v)+\ker\alpha)\\
  &= \alpha(u+\lambda v)\\
  &= \alpha(u)+\lambda\alpha(v) \\
  &= \overline\alpha(u+\ker\alpha)+\lambda\overline\alpha(v+\ker\alpha)
\end{align*}
So $ \overline \alpha$ is linear hence is an isomorphism\qed
\subsection{Basis}
\begin{definition}
	(Span) Let $ V $ be a $ \F $-vector space. Then the \textit{span} of some subset $ S\subseteq V $ is
	\[
		\langle S\rangle = \left\{\sum_{s\in S}\lambda_s\cdot s: \lambda_s\in \F\right\}
	\]
	where $ \sum $ denotes finite sums. An expression the form above is called a \textit{linear combination} of $ S $.\\
	We say that $ S $ \textit{spans} $ V $ if $ \langle S\rangle =V $
\end{definition}
\begin{definition}
	(Finite-dimensional) For a vector space $ V $ we say that it is \textit{finite-dimensional} if there exists a finite spanning set.
\end{definition}
We'll give some simple remarks without proof.
  \begin{enumerate}
	  \item $ \langle S \rangle\le V $ and conversely if $ W\le V $ and $ S\subseteq W $ then $ \langle S\rangle \le W $.
	  \item If $ S,T\subseteq W $ and $ S $ spans $ V $ and $ S\subseteq \langle V\rangle $ then $ T $ spans $ V $.
	  \item By convention $ \langle\emptyset\rangle =\{\mathbf 0_V\} $.
	  \item $ \langle S\cup T\rangle = \langle S\rangle + \langle T\rangle $
  \end{enumerate}
For an example consider $ V=\R^3 $ and consider the sets
\begin{align*}
  S=\left\{\begin{pmatrix}
    1\\0\\0
  \end{pmatrix},\begin{pmatrix}
    1\\1\\2
\end{pmatrix}\right\}\\
T=\left\{\begin{pmatrix}
  2\\1\\2
\end{pmatrix},\begin{pmatrix}
  0\\1\\2
\end{pmatrix},\begin{pmatrix}
  -1\\2\\4
\end{pmatrix}\right\}
\end{align*}
Then $ \langle S\rangle = \langle T\rangle =\left\{\begin{pmatrix}
  x\\y\\2y
\end{pmatrix}:x,y\in \R\right\} \le \R^3.$\par
For a second example consider $ V=\R^\N $ and set $ T=\{\delta_n:n\in \N\} $. This is not a spanning set, since we require infinitely many elements from $ T $ to make an element in $ V $. In fact we can write that
\[
	\langle T\rangle =\{f\in \R^\N:f(n)=0\text{ for all but finitely many terms}\}.
\]
\begin{definition}
	(Linear Independence) A subset $ S\subseteq V $ is called \textit{linearly independent} if, for all finite linear combinations
\[
	\sum_{s\in S}\lambda_ss\quad \text{of S}
\]
if the sum is the zero vector in $ V $ the $ \lambda_s=0 $ for all $ s\in S $.
\end{definition}
If $ S $ is not linearly indepedent we say that $ S $ is linearly dependent.\par
We'll make some more remarks
\begin{enumerate}
	\item If $ \mathbf 0 \in S $ then $ S $ is not linearly independent.
	\item If we have a finite set, then to show linearly independent, we only need to consider the linear combination of all elements, not all finite lienar combinations.
	\item However is $ S $ is infinite, then we have to consider every possible finite subset of $ S $ and show it's linearly independent.
	\item Every subset of a linearly independent set is itself linearly indepedent.	
\end{enumerate}
\begin{definition}
	(Basis) A subset $ S\subseteq V $ is a \textit{basis} for $ V $ if $ S $ is linearly independent and a spanning set.
\end{definition}
For an example consider $ e_i\in \F^n $ be given by
\[
  e_i=\begin{pmatrix}
    0\\ \vdots \\ 0 \\ 1 \\ 0 \\\vdots \\ 0
\end{pmatrix}\quad \text{with the 1 in the } i\text{th entry}
\]
then the set $ \{e_i:1\le i\le n\} $ is the standard basis for $ \F^n $.\par
For $ P(\R) $ the set of real polynomial functions and let $ p_n\in P(\R) $ be given by $ p_n(x)=x^n $, then $ \{p_n:n\in\Z_{\ge 0}\} $ is a basis for $ P(\R) $.
\begin{proposition}
  If $ S\subseteq V $ is a finite spanning set, then there exists a subset $ S'\subseteq S $ such that $ S' $ is a basis.
\end{proposition}
\pf If $ S $ is linearly independent then we're done. Otherwise write $ S=\{v_1,\dots, v_n\} $. Then there exists $ \lambda_1,\dots, \lambda_n $ such that $ \lambda_1v_1+\cdots\lambda_nv_n=\mathbf 0 $ wlog suppose that $ \lambda_n $ is nonzero. Then
\begin{align*}
	v_n=-\frac 1{\lambda_n}\sum_{i=1}^{n-1}\lambda_iv_i
\end{align*}
so $ v_n $ is in the span of the other vectors. Hence $ S\setminus \{v_n\} $ is still a spanning set. Repeat which the set is linearly independent, must terminate since the set is finite and the empty set is not a spanning set.\qed
\begin{corollary}
  Every finite-dimensional vector space has a finite basis.
\end{corollary}
\pf Trivial application of the proposition\qed
\begin{theorem}
	(Steinitz Exchange Lemma) Let $ S,T\subseteq V $ finite with $ S $ linearly independent and $ T $ a spanning set of $ V $. Then
	\begin{enumerate}
		\item $ |S|\le |T| $,
		\item and there exists $ T'\subseteq T $ which has size $ |T'|=|T|-|S| $ and $ S\cup T' $ spans $ V $.
	\end{enumerate}
\end{theorem}
\pf To come later...\par
Let's look at some consequences of the lemma first.
\begin{corollary}
	For a finite-dimensional vector space $ V $,
  \begin{enumerate}
	  \item Every basis for $ V $ is finite.
	  \item All finite basis have the same size.
  \end{enumerate}
\end{corollary}
\pf $ V $ has a finite basis $ B $, suppose we have some other basis $ B' $ infinite. Let $ B''\subseteq B' $ with $ |B''|=|B|+1 $ then $ |B''| $ is linearly independent, so applying (i) of the Steinitz exchange lemma with $ S=B'' $ and $ T=B $ we get a contradiction.\par
For the second part, let $ B_1,B_2 $ be finite basis for $ V $ then apply Steinitz symmetrically since both are spanning set and linearly independent, so we get that $ |B_1|\ge |B_2| $ and $ |B_1|\ge |B_2| $ so $ |B_1|=|B_2| $.\qed
\begin{definition}
	(Dimension) For a vector space $ V $ the \textit{dimension} of $ V $ is the size of any basis. We write this as \dim V.
\end{definition}
This definition is well-defined by the previous corollary.\par
For an example $ \dim \F^n=n $ since we've shown the standard basis has size $ n $. As a complex vector space $ \C $ is one-dimensional as a complex vector space and two-dimension as a real vector space, with basis $ \{1\} $ and $ \{1,i\} $ repectively.
\begin{corollary}
  For a vector space $ V $ let $ S,T\subseteq V $ finite, with $ S $ linearly independent and $ T $ a spanning set, then
  \begin{align*}
	  |S|\le \dim V\le |T|
	  \end{align*}
	  with equality if and only if $ S $ spans or $ V $ is linearly independent respectively.
\end{corollary}
\pf The inequalities are immediate from Steinitz. If $ S $ is a basis then $ |S|=\dim V $ from the previous corollary. Conversely if $ |S|=\dim V $ and let $ B $ be a basis for $ V $ so we have that $ |B|=|S| $ so $ B $ is a spanning set. So we can apply Steinitz (ii) to $ B $ so there exists $ B'\subseteq B $ with $ |B'|=|B|-|S|=0 $ and $ S\cup B'=S\cup \emptyset $ spans $ V $. So $ S $ is a basis. Similiar we have a very similar proof for equality in $ V $.\qed\par We will not prove that every vector space has a basis, however some non-finitely dimensional vector spaces have an infinite basis, for example $ P(\R) $.
\begin{proposition}
  If $ V $ is a finite-dimensional vector space, then if $ U\le V $ then $ U $ is finite-dimensional, namely, $ \dim U \le \dim V $ with equality if and only if $ U=V $.
\end{proposition}
\pf If $ U=\{\mathbf 0\} $, we're done. Otherwise let $ \mathbf 0\ne u_1\in U $. Then $ \{u_1\}\subseteq U $ is linearly indepedent. Repeating, after repeating $ k $ times suppose we have $ \{u_1,\dots, u_k\} $ linearly indepedent with $ k\le \dim(V) $ by the previously corollary. If the set spans $ U $ we're done, if not we'll add another vector, $ u_{k+1} $ outside of the span of our space. If $ \{u_1,\dots, u_{k+1}\} $ is not linearly indepedent, we can write $ \mathbf 0 $ non-trivially, so
\[
	\sum_{i=1}^{k+1}\lambda_i u_i=\mathbf 0
\]
with $ \lambda_{k+1}\ne 0 $ since $ \{u_1,\dots, u_k\} $ linearly indepedent. Thus we have that
\[
	u_{k+1}=-\frac 1{\lambda_{k+1}}\left(\sum_{i=1}^k\lambda_iu_i\right)
\]
this process must terminate after at most $ \dim V $ many steps, by the previous corollary. If $ \dim U=\dim V $ apply the previous corollary with $ S $ being any basis for $ U $.\qed
\begin{proposition}
	(Extending a basis) Let $ U\le V $. For any basis $ B_U $ of $ U $ there exists a basis $ B_V $ of $ V $ such that $ B_U\subseteq B_V $.
\end{proposition}
\pf Apply the second result from Steinitz with $ S=B_U $ and $ T $ is any basis for $ V $. We obtain that $ T'\subseteq T\st $ 
\[
  |T'|=|T|-|S|=\dim V-\dim U
\]
and $ B_V=B_U\cup T' $ spans $ V $. But we have that
\[
  |B_V|\le |B_U|+|T'|=\dim V
\]
so by the previous corollary, $ B_V $ is a basis for $ V $.\qed\par
Now we'll finally prove the Steinitz exchange lemma.\par
\pf Let $ S=\{u_1,\dots, u_m\} $, $ T=\{v_1,\dots, v_n\} $ with $ |T|=m $ and $ |T|=n $. If $ S $ is empty then we're done. Otherwise there exists $ \lambda_i\in \F $ such that 
\[
	u_1=\sum_{i=1}^n\lambda_iv_i
\]
so by renumbering we can say that $ \lambda_1\ne 0 $. Then
\[
	v_1=\frac 1{\lambda_1}\left(u_1-\sum_{i=2}^n\lambda_iv_i\right)
\]
So $ \{u_1,v_2,\dots, v_n\} $ spans $ V $. After repeating $ k $ times with $ k<m $ suppose $ \{u_1,\dots, u_k,v_{k+1},\dots, v_n\} $ spans V, then there exists $ \lambda_i,\mu_j\in\F $ such that
\begin{align*}
	u_{k+1}=\sum_{j=1}^k\mu_ju_j+\sum_{i=k+1}^n\lambda_iv_i
\end{align*}
If for all $ \lambda_i= 0 $ then
\[
	\left(\sum_{j=1}^k\mu_ju_j\right) -u_{k+1}=\mathbf 0
\]
which is a contradiction since $ S $ is linearly independent. So by relabeling we have that $ \lambda_{k+1}\ne 0 $ such that 
\[
	v_{k+1}=\frac 1{\lambda_{k+1}}\left(u_{k+1}-\sum_{j=1}^k\mu_ju_j-\sum_{i=k+1}^n\lambda_iv_i\right)
\]
so $ (u_1,\dots, u_{k+1},v_{k+2},\dots, v_n\}$ spans $ V $. So we can conclude that $ m\ne n $ and $ \{u_1,\dots, u_m,v_{m+1},\dots, v_n\} $ spans $ V $ hence the set $ T'=\{v_{m+1},\dots, v_n\} $ exists as claimed. \qed
\begin{definition}
	(Nullity) For a linear map $ \alpha:V\to W $ we define the \textit{nullity} of $ \alpha $ as
	\[
	  \n(\alpha)=\dim\ker\alpha.
	\]
\end{definition}
\begin{definition}
	(Rank) For a linear map $ \alpha:V\to W $ we define the \textit{rank} of $ \alpha $ as
	\[
	  \rk(\alpha)=\dim\ima\alpha.
	\]
\end{definition}
\begin{theorem}
	(Rank-nullity theorem) If $ V $ is a finite dimensional $ \F $-vector space and $ W  $ is a $ \F $-vector space. Then if $ \alpha:V\to W $ is linear then $ \ima \alpha $ is finite dimensional and
	\[
	  \dim V=\n(\alpha)+\rk(\alpha).
	\]
\end{theorem}
\pf Recall the first isomorphism theorem so
\[
	\frac V{\ker \alpha}\cong \ima\alpha
\]
It is sufficient to prove the lemma
\begin{lemma}
  For $ U\le V $,
  \[
    \dim(V/U)=\dim V-\dim U
  \]
\end{lemma}
\pf Let $ B_U=\{u_1,\dots, u_m\} $ be a basis of $ U $. Extend to a basis $ B_V=\{u_1,\dots, u_m,v_{m+1},\dots, v_n\} $ of $ V $ where $ m=\dim U $ and $ n=\dim V $.\\
Set $ B_{V/U}=\{v_i+U:m+1\le i\le n $. The we claim that $ B_{V/U} $ is a basis for $ V/U $ of size $ n-m $. To show spanning, for $ v\in V $ write
	\[
	  v=\sum_i\lambda_iv_i+\sum_j\mu_jv_j
	\]
	Then $ v+U=\sum_i\lambda_i(v_i+U)\in \langle B_{V/U}\rangle $. For linear independence, suppose
	\[
	  \sum_i\lambda_i(v_i+U)=\mathbf 0+U
  \]
	hence 
	\begin{align*}
	  =\left(\sum_i\lambda_iv_i\right)+U \\
	  \sum_i\lambda_iv_i\in U\\
	  \sum_i\lambda_iv_i=\sum_j\mu_ju_j
	\end{align*}
	since $ B_V $ is linearly independent, we have that all $ \lambda_i $ and $ \mu_j $ are zero. Similiarly if $ v_i+U=v_j+U $ with $ i\ne j $ then we can write $ v_i-v_j=\sum_j\mu_ju_j $ which is a contradiction.\qed
	\begin{remark}
	   We can maek a direct proof without quotient spaces by rearranging some of the arguments of the proof.
	\end{remark}
\begin{corollary}
	(Linear Pigeonhole principle) If $ \dim V=\dim W=n $ and $ \alpha: V\to W $ then the following conditions are equivalent.
	\begin{enumerate}
		\item $ \alpha $ is injective,
		\item $ \alpha $ is surjective,
		\item $ \alpha $ is an isomorphism.
	\end{enumerate}
\end{corollary}
\pf If $ \alpha $ injective then $ \n(\alpha)=0 $ so by rank nullity we have that $ \rk(\alpha)=n $ so $ \alpha $ is surjective. If $ \alpha $ is surjective then $ \rk(\alpha)=n $ so by rank nullity, the dimension of the kernel is $ 0 $ hence the kernel is trivial, so $ \alpha $ injective, hence $ \alpha $ is an isomorphism. If $ \alpha $ is an isomorphism, clearly it's injective, so all equivalent.\qed
\begin{proposition}
  Suppose $ V $ is a vector space with a basis $ B $. For any vector space $ W	 $ and any function $ f:B\to W $ there is a unique linear map $ F:V\to W $ such that $ F(B)=W $.
\end{proposition}
\pf First we'll show existance. For $ v\in V $ write $ v=\sum_b\lambda_bb $ for a finite sum. Then define
\[
  F(v)=\sum_b \lambda_bf(b).
\]
This is well-defined, since $ B $ is a basis the $ \lambda_b $ are uniquely determined by $ v $. For $ u,v\in V $ and $ \lambda\in \F $ we write
\[
  u=\sum_b\mu_bb,\quad \sum_b\lambda_bb.
\]
Then \begin{align*}
	F(u+\lambda v)&=F(\sum_b(\mu_b+\lambda\lambda_b)f(b)\\
		     &= \sum_b\mu_bf(b)+\lambda\sum_b\lambda_bf(b)\\
		     &= F(u)+\lambda F(v).
\end{align*}
So $ F $ is linear. To show uniqueness $ \overline F:V\to W $ is another linear map extending $ f $ then,
\[
  \overline F\left(\sum_b\lambda bb\right)=\sum_b\lambda_b\overline F(b)
\]
which is the same as our definition for $ F $ hence they are the same function.
\begin{corollary}
	For a vector space, $ V $, with $ \dim V=n $ with a basis $ B=\{v_1,\dots, v_n\} $ for $ V $ then there is a unique isomorphism
  \begin{align*}
	  F_B:V&\to \F^n\\
	       \sum_{i=1}^n\lambda_iv_i &\to \begin{pmatrix}
	        \lambda_1\\
		\vdots \\
		\lambda_n
	      \end{pmatrix}
  \end{align*}
  \end{corollary}
  \pf Let $ E=\{e_1,\dots, e_n\} $ be the standard basis for $ \F^n $. Define
  \begin{align*}
	  f:B&\to W\\
	  v_i&\to e_i
  \end{align*}
  and let $ F_B $ be the unique linear extension of $ f $ to $ V $. We see that $ f $ defines a bijection from $ B\to E $. Let $ \bar F_B $ be the unique linear extension of $ \inv f:E\to B $. Then $ \bar F_B\cdot F_B $ is the composition of two linear maps, hence it's linear, moreover it is $ \mathrm{id}_B $. But also $ \mathrm{id}_V $ is also a linear extension of $ \mathrm{id}_B $, by the proposition, they are the same map so $ \bar F_B\cdot F_B=F_B\cdot \bar F_B=\mathrm{id}_B $. Hence $ F_B $ is bijective, so it is an isomorphism.\qed
  \begin{corollary}
    If $ V,W $ are finite dimensional $ \F $-vector spaces. Then
    \[
      V\cong W\iff \dim V=\dim W
    \]
  \end{corollary}
  \pf Trivial from the corollary using the transitivity of the isomorphism relation.\qed
\begin{definition}
	(Coordinate vector) $ F_B(v)=[v]_B $ is the \textit{coordinate vector} of $ v $ with respect to the basis $ B $
\end{definition}
For an example if $ V\cong \F^n $ and $ U\le V $ with $ U\cong \F^m $ then $ \dim(V/U)=n-m $, so $ \frac VU\cong \F^{n-m} $.
\subsection{Direct sums}
\begin{definition}
	(External direct sum) For $ \F $-vector spaces, $ V $ and $ W $, we dnote the \textit{external direct sum} of $ V $ and $ W $ as $ V\oplus W $ with underlying set $ V\times W $ with addition and scalar multiplication given in the obvious sense.
\end{definition}
We can similarly define
\[
	V_1\oplus \cdots \oplus V_n=\bigoplus_{i=1}^nV_i.
\]
\begin{lemma}
  For $ V,W $ finite dimensional vector spaces,
  \[
    \dim(V\oplus W)=\dim V+\dim W
  \]
\end{lemma}
\pf\\(First Proof) Let $ B,C $ be basis for $ V,W $ respectively. Set
\[
	D=(B\times\{\mathbf 0_W\})\cup (\{\mathbf 0_V\}\times C)
\]
it is straightfoward to check that $ D $ is basis of $ V\oplus W $ of the size $ \dim V + \dim W $.\qed\par
(Second Proof) Suppose $ V\cong \F^n $ and $ W\cong \F^m $ construct an isomorphism $ V\oplus W\cong\F^{n+m} $.\qed

\begin{proposition}
  Let $ V $ be a vector space with $ U,W\le V $. There is a surjective linear map
  \begin{align*}
	  \varphi: U\oplus W &\to U+W\\
	  (u,w) &\to u+w
  \end{align*}
  with $ \ker\varphi\cong U\cap W $.
\end{proposition}
\pf Surjectively and linearity are clear. Note for $ (u,w)\in U\oplus W $ then $ (u,w)\in\ker\varphi $ if and only if $ w=-u $. Hence
\[
	\ker\varphi=\{(x,-x):x\in U\cap W\}
\]
the map $ \psi:U\cap W\to \ker\varphi $ sending $ x\to(x,-x) $ is an isomorphism.
\begin{corollary}
	(Sum-Intersection Formula) If $ V $ is finite dimensional and $ U,W\le V $ then
	\[
	  \dim (U+W)=\dim U + \dim V-\dim(U\cup V)
	\]
\end{corollary}
Applying the rank-nullity theorem to the linear map $ \varphi $ in the proposition we get that
\begin{align*}
	\dim U + \dim W&=\dim (U\oplus V)\\
		       &=\dim(\ker\varphi)+\dim(\ima\varphi)\\
		       &=\dim(U+W)+\dim(U\cap W)\qed
\end{align*}
We can also give an explicit basis. Given a basis $ B $ for $ U\cap W $, extend $ B $ to a basis $ B_U $ for $ U $, and a basis $ B_W $ for $ W $. Then $ B_U\cap B_W $ spans $ U+W $ and
\[
  |B_U\cup B_W|\le |B_U|+|B_W|-|B|=\dim(U+V)
\]
hence $ B_U\cup B_W $ is linearly independent so it's a basis for $ U+W $.
\begin{remark}
We could also check directly that $ B_U\cup B_W $ is linearly independent of the size $ \dim(U+V) $ without assuming the sum-intersection formula, so this alos servers as an alternative proof of the sum-intersection formula.
\end{remark}
\begin{definition}
	(Internal direct sum) Suppose $ U,W\le V $ satisify 
	\begin{enumerate}
		\item $ U+W=V $,
		\item $ U\cap W=\{\mathbf 0_V\} $.
	\end{enumerate}
	Then 
	\begin{align*}
	  \varphi: U\oplus W\to V
	\end{align*}
	is an isomorphism, and we say that $ V $ is the \textit{internal direct sum} of $ U $ and $ W $, and we write that $ V=U\oplus W $.
\end{definition}
Alternatively, every element $ v\in V $ can be written \textit{uniquely} as $ v=u+w $ for $ u\in U, w\in W $.
\begin{definition}
	(Direct complement) For $ U\le V $ a \textit{direct complement} to $ U $ in $ V $ is a subspace $ W\le V $ satisfying $ V=U\oplus W $.
\end{definition}
\begin{proposition}
  If $ V $ is finite dimensional then every subspace has a direct complement.
\end{proposition}
\pf Let $ U\le V $ and let $ B_U $ be a basis for $ U $. Extend to a basis $ B_V $ for $ V $. Set $ W=\langleB_V\setminus B_U\rangle $. Then
\begin{align*}
	V=\langle B_V\rangle &= \langle B_U\cup (B_V\setminus B_U)\rangle\\
			     &= \langle B_U\rangle + \langle B_V\setminus B_U\rangle \\
			     &=U+W.
\end{align*}
Moreover using the sum-intersection formula
\begin{align*}
	\dim (U\cap W)=|B_V|+|B_U|-|B_V\setminus B_U|=0.
\end{align*}
Hence $ U\oplus W=V $.\qed\par
More generally for $ U_1,\dots, U_n\le V $ we say that $ V $ is the direct sum of the $ U_i $ and write that
\[
	V=U_1\oplus+\cdots +\oplus V_n=\bigoplus_{i=1}^n V_i
\]
if the map
\begin{align*}
	\varphi: U_1\oplus\cdots\oplus U_n &\to V\\
	(u_1,\dots, u_n) &\to u_1,\dots, u_n
\end{align*}
is an isomorphism. Equivalently every $ v\in V $ can be uniquely written as $ v=u_1+\dots +u_n $ for $ u_i\in U_i $.
\section{Matrices and Linear Maps}
\subsection{Vector spaces of linear maps}
\begin{definition}
  For $ V,W $ $ \F $-vector spaces we define
  \[
	  \mathcal L(V,W)=\{\alpha:V\to W:\alpha\text{ is linear} \}
  \]
  which forms a $ \F $-vector space under pointwise addition and obvious scalar multiplication.
\end{definition}
Recall that $ M_{m\times n} $ is the space of matrices over $ \F $ with $ m $ rows and $ n $ columns. For $ A\in M_{m\times n}(\F) $ we write $ A=(a_{ij}) $ where $ a_{ij}\in \F $ is the entry in the $ i $th row and the $ j $th column.\par
Let $ B=\{v_1,\dots, v_n\}, C=\{w_1,\dots, w_m\} $ are \textit{ordered} basis for $ V,W $.\par
Let $ \alpha\in\mathcal L(V,W) $. We can write
\begin{align*}
	\alpha(v_1)&=a_{11}w_1+a_{21}w_2+\cdots + a_{m1}w_m\\
	\alpha(v_2)&=a_{12}w_1+a_{22}w_2+\cdots + a_{m1}w_m\\
	^\vdots\\
	\alpha(v_n)&=a_{1n}w_1+a_{2n}w_2+\cdots +a_{mn}w_m
\end{align*}
\begin{definition}
	(Matrix) The \textit{matrix} of $ \alpha $ with respect to the ordered basis $ B,C $ is
	\[
		[\alpha]^B_C=(a_{ij})\in M_{m\times n}(\F)
	\]
\end{definition}
Recall we have a linear isomorphism
\begin{align*}
	\varepsilon_B:V &\to \F^n\\
	v=\sum_{i=1}^n\lambda_iv_i &\to (\lambda_i)_i=[v]_B
\end{align*}
where $ [v]_B $ is the coordinate vector of $ v $ with respect to $ B $.
\begin{theorem}
  For finite-dimensional vector spaces $ V,W $ with basis $ B,C $ respectively and $ \alpha:V\to W $ linear  then
  \begin{enumerate}
	  \item For all $ v\in V $
		  \[
			  [\alpha]^B_C[v]_B=[\alpha(v)]_C
		  \]
	  \item $ [\alpha]_C^B $ is the only matrix $ A\in M_{m\times n}(\F) $ satisfying $ A[v]_B =[\alpha(v)]_C $ for all $ v\in V $.
	  \item There is an isomorphism of $ \F $-vector spaces
		  \begin{align*}
			  \varepsilon_C^B:\mathcal L(V,W)&\to M_{m\times n}(\F)\\
			  \alpha &\to [\alpha]^B_C
		  \end{align*}
\end{theorem}
\pf Let $ v\in V $ write $ v=\sum_{j=1}^n\lambda_jv_j $. Then
\begin{align*}
	\alpha(v)&=\sum_{j=1}^n\lambda_j\alpha(v_j)\\
		 &=\sum_{j=1}^n\lambda_j\sum_{i=1}^ma_{ij}w_i\\
		 &=\sum_{i=1}^m\left(\sum_{j=1}^n\lambda_ja_{ij}\right)w_i.
\end{align*}
So
\begin{align*}
	[\alpha(v)]_C&=\left(\sum_{j=1}^n a_{ij}\lambda_j\right)_i\\
		     &=(a_{ij})\cdot(\lambda_j)\\
		     &=[\alpha]^B_C[v]_B.
\end{align*}
Hence (i) is proved. For (ii), take $ 1\le j\le n $, so $ [v_j]_B=e_j $. Hence for $ A\in M_{m\times n}(\F) $, $ A[v_j]_B $ is the $ j $th column of $ A $. But if $ A[v_j]_B=[\alpha(v_j)]_C=[\alpha]^B_C[v_j]_B=[\alpha]^B_Ce_j $, then $ A[v_j]_B $ is also the $ j $th column of $ [\alpha]^B_C $. Since this holds for all $ j $ in our range, they are the same matrix.\par
Now for part (iii), let $ \alpha,\beta\in\mathcal L(V,W) $ and $ \lambda\in \F $. Then
\begin{align*}
	[\alpha+\lambda\beta]^B_C[v]_B&=[(\alpha+\lambda\beta)(v)]_C\\
				      &=[\alpha(v)+\lambda\beta(v)]_C\\
				      &= [\alpha(v)]_C+\lambda[\beta(v)]_C\\
				      &= ([\alpha]^B_C+\lambda[\beta]^B_C)[v]_B
\end{align*}
for all $ v\in V $. Hence by (ii) we get that $ [\alpha+\lambda\beta]^b_c=[\alpha]_C^B+\lambda[\beta]^B_C $ so the map is linear. Let $ \alpha\in\ker(\varepsilon_C^B) $ so that $ [\alpha]^B_C=0\in M_{m\times n} (\F)$. Then by (i) we have that $ [\alpha(v)]_C=0 $ for all $ v\in V $. But $ \varepsilon:w\to [w]_C $ is an isomorphism so $ \alpha(v)=0 $ for all $ v\in V $ hence $ \alpha=0 $ and $ \alpha $ is injective. For surjectivity let $ A\in M_{m\times n}(\F) $ and define $ f:B\to W $ by $ f(v_j)=\sum_{i=1}^na_{ij}w_I $ and extend $ f $ to a linear map $ F:V\to W $. Then $ [F]_C^B=A $. So $ \varepsilon_C^B $ is an isomorphism.\qed
\begin{proposition}
  Let $ V,W,X $ be finite-dimensional $ \F $-vector spaces with basis $ B,C,D $ and $ \alpha\in\mathcal L(V,W) $ and $ \beta\in\mathcal L(W,X) $. Then
  \[
	  [\beta\circ \alpha]^B_D=[\beta]^C_D[\alpha]^B_C.
  \]
\end{proposition}
\pf By the theorem $ [\beta\circ\alpha]^B_D $ is the unique matrix $ A $ satisfying
\[
	A[v]_B=[\beta(\alpha(v))]_D,\quad \forall v\in V.
\]
But $ [\beta]^C_D[\alpha]^B_C[v]_B=[\beta]^C_D[\alpha(v)]_C=[\beta(\alpha(v))]_D $. So by (ii) of theorem they are equal.\qed
\begin{remark}
  For any basis $ B $ of $ V $,
  \[
	  [\mathrm{id}_V]^B_B=I_{\dim V}.
  \]
\end{remark}
\begin{definition}
	(Change of basis matrix) Let $ B,B' $ be basis for $ V $ and $ \dim V=n $. The \textit{change of basis matrix} from $ B $ to $ B' $ is given by
	\[
		P=[\mathrm{id}_V]^B_{B'}\in M_{m\times n}(\F)
	\]
\end{definition}
Equivalently letting $ B=\{v_i\}_{i=1}^n $ and $ B'=\{v_i'\}_{i=1}^n $, then
\[
	P=(p_{ij})\quad\text{where}\quad v_j=\sum_{i=1}^n p_{ij}v_i'
\]
so the $ j $th column of $ P $ is $ [v_j]_{B'} $.
\begin{proposition}
	For $ V,W $ finite-dimensional vector spaces,\smallskip
  \begin{enumerate}
	  \item $ [\mathrm{id}_V]^B_{B'}\in GL_n(\F) $ with inverse $ [\mathrm{id}_V]^{B'}_B $.
	  \item If $ \alpha\in\mathcal L(V,W) $ and $ B,B' $ basis for $ V $ and $ C,C' $ basis for $ W $, then
		  \[
			  [\alpha]^{B'}_{C'}=[\mathrm{id}_W]^C_{C'}[\alpha]^B_C[\mathrm{id}_V]^{B'}_B.
		  \]
  \end{enumerate}
\end{proposition}
\pf By the remark,
\begin{align*}
	I_n=[\mathrm{id}_V]^B_B=[\mathrm{id}_V]^{B'}_B[\mathrm{id}_V]^B_{B'}
\end{align*}
and symmetrically swapping $ B $ and $ B' $. For the second part the result is immediate from the proposition.
\begin{definition}
	(Equivalent matrices) Let $ A,A'\in M_{m\times n}(\F) $. We say that $ A $ and $ A' $ are \textit{equivalent} if $ \exists P\in GL_m(\F) $, $ Q\in GL_n(\F) $ such that $ A'=PAQ $. 
\end{definition}
\begin{remark}
  Certianly $ A $ is equivalent to itself by $ P=I_m $ and $ Q=I_n $.\\
  If $ A'=PAQ $ then $ A=\inv PA'\inv Q $.\\
  If $ A''=RA'S $ too, then $ A''=(RP)A(QS) $, so the equivalence of matrices is an equivance relation on $ M_{m\times n}(\F) $.
\end{remark}
\begin{theorem}
  Let $ V,W $ be finite-dimensional $ \F $-vector spaces. Let $ \dim V=n $, $ \dim W=m $ and let $ \alpha\in\mathcal L(V,W) $. Let $ r=\rk(\alpha) $. Then,
  \begin{enumerate}
	  \item There exists basis $ B,C $ for $ V,W $ respectively such that
		  \[
			  [\alpha]^B_C=\begin{pmatrix}
				  I_r & 0 \\
				  0 & 0
			  \end{pmatrix}\in M_{m\times n}(\F)
		  \]
		  where $ I_r $ is the identity matrix of size $ r $, and the zeros are block zero matrices.
	  \item If \[
			  [\alpha]^{B'}_{C'}=\begin{pmatrix}
				  I_{r'} & 0 \\
				  0 & 0
			  \end{pmatrix}\in M_{m\times n}(\F)
		  \]
		  for some basis $ B',C'$ of $ V,W $ respectively, then $ r'=r $
  \end{enumerate}
\end{theorem}
\pf By rank-nullity $ \n(\alpha)=n-r $. Let $ \{v_{r+1},\dots, v_n\} $ be a basis for $ \ker\alpha $. Extend to a basis $ B=\{v_1,\dots, v_r,v_{r+1},\dots, v_n\} $. Then $ \{\alpha(v_1),\dots, \alpha(v_r)\} $ spans the image, and has size at most $ \dim(\ima(\alpha)) $, so it's linearly independent, hence we can extend it to form a basis of $ W $.
\[
	C=\{w_1=\alpha(v_1),\dots, w_r=\alpha(v_r),w_{r+1},\cdots, w_m\}
\]
Then
\[
  \alpha(v_j)=\begin{cases}
	  w_j & 1\le j\le r \\
	  \mathbf 0 & \text{otherwise}
  \end{cases}
\]
hence we have that $ [\alpha]^B_C= \begin{pmatrix}
				  I_{r} & 0 \\
				  0 & 0
			  \end{pmatrix} $.\par
			  For the second part, if $ [\alpha]^{B'}_{C'}= \begin{pmatrix}
				  I_{r'} & 0 \\
				  0 & 0
			  \end{pmatrix} $ then
			  \[
			    \alpha(v_j')=\begin{cases}
	  w_j' & 1\le j\le r' \\
	  \mathbf 0 & \text{otherwise}
  \end{cases}.


			  \]
			  Hence $ w_1',\dots, w_{r'}' $ span $ \ima(\alpha) $ and are linearly independent. Hence $ \rk(\alpha)=r' $.\qed
\begin{definition}
	(Column-space) For $ A\in M_{m\times n}(\F) $ the \textit{column-space} $ \Col(A) $ is the subspace of $ \F^m $ spanned by the columns of $ A $. The dimension of the column-space is called the \textit{column-rank} of $ A $.
\end{definition}

\begin{definition}
	(Row-space) For $ A\in M_{m\times n}(\F) $ the \textit{row-space} $ \Row(A) $ is the subspace of $ \F^m $ spanned by the rows of $ A $ (when transposed as column vectors). The dimension of the row-space is called the \textit{row-rank} of $ A $.
\end{definition}
\begin{remark}
  \[
    \Row(A)=\Col(A^T)
  \]
  hence the row-rank of $ A $ is the same as the column-rank of $ A^T $.
\end{remark}
\begin{remark}
	Given a matrix $ A\in M_{m\times n}(F) $ we can define a linear map $ \alpha:\F^n\to \F^m $ by $ \alpha(v)=Av $. Then $ \ima(\alpha)=\Col(A) $, so the rank of $ \alpha $ is the same as the column-rank of $ A $. Moreover, $ A=[\alpha]^{E_n}_{E_m} $ where $ E_k $ are the standard basis for $ \F^k $.
\end{remark}
We may write $ \ima A,\ker A, \rk(A),\n(A) $ to refer to the corresponding concepts for $ \alpha $.
\begin{theorem}
	Let $ A, A' \in M_{m\times n}(\F) $, then
	\begin{enumerate}
		\item $ A $ is equivalent to
\[
  \begin{pmatrix}
	  I_r & 0 \\
	  0 & 0
  \end{pmatrix}
  \text{ where } r \text{ is the column-rank of } A
\]
\item $ A $ and $ A' $ are equivalent if and only if the have the same column-rank.

	\end{enumerate}
\end{theorem}
\pf We'll first prove a lemma.
\begin{lemma}
	For $ A\in M_{m\times n}(\F) $ and $ B\in M_{n\times p}(\F) $ then $ \rk(A\cdot B)\le \min(\rk(A),\rk(B)) $.
\end{lemma}
\pf We have that $ \ima(AB)\le \ima(A) $ so $ \rk(AB)\le \rk(A) $. If $ Bv=\mathbf 0 $ for $ v\in \F^p $, then $ ABv=\mathbf 0 $, so $ \n(B)\ge \n(AB) $, so applying rank-nullity, we get that
\[
  p-\rk(B)\le p-\rk(AB)\implies \rk(AB)\le \rk(B)\qed
\]
\par
Now we'll prove the first part of the theorem. Let $ \alpha $ the natural linear map corresponding to $ A $, so $ A=[\alpha]^{E_n}_{E_m} $. By the previous theorem, there exists matrices $ B,C $ of $ \F^n,\F^m $ such that
\[
  \begin{pmatrix}
	  I_r & 0\\ 0 & 0
  \end{pmatrix} = [\alpha]^B_C=[\mathrm{id}_{\F^m}]^{E_m}_C[\alpha]^{E_n}_{E_m}[\mathrm{id}_{\F^n}]^B_{E_n}=PAQ
\]
where $ r=\rk(\alpha) $ which we know is equal to the column-rank of $ A $.\par
If $ A' $ has column-rank $ r $ then both matrices are equivalent to $ \begin{pmatrix}
	I_r&0\\0 &0
\end{pmatrix} $, so by transitivity, $ A $ and $ A' $ are equivalent. Conversely suppose that $ A $ and $ A' $ are equivalent, so $ A'=PAQ $. By the lemma $ \rk(A')\ge \rk(AQ)\ge\rk(A) $ and symmetrically we get that $ \rk(A)\ge \rk(A') $, hence $ \rk(A')=\rk(A) $.\qed
\begin{theorem}
	For any $ A\in M_{m\times n}(\F) $, the row-rank of $ A $ is equal to the column-rank of $ A $.
\end{theorem}
\pf Note that if $ P $ is invertiable, then so it the tranpose with inverse $ (\inv P)^T $. Let $ r $ be the column-rank of $ A $. So there exists matrices $ P\in GL_m(\F) $ and $ Q\in GL_n(\F) $ such that $ PAQ=\begin{pmatrix}
	I_r&0\\ 0 & 0
\end{pmatrix}\in M_{m\times n}(\F) $. Then $ A^T $ is equivalent to $ Q^TA^TP^T=(PAQ)^T=\begin{pmatrix}
	I_r & 0 \\ 0 & 0
\end{pmatrix} \in M_{n\times m}(\F)$. By the previous theorem, the column-rank of $ A^T $ is $ r $ which also the row-rank of $ A $.\qed
\par
Let $ V $ be a finite-dimensional vector space and $ B,B' $ be basis for $ V $. Now let $ \alpha\in\mathrm{End}(V)=\mathcal L(V,V) $. Then
\[
	[\alpha]^{B'}_{B'}=[\mathrm{id}_V]^B_{B'}[\alpha]^B_B[\mathrm{id}_V]^{B'}_B
\]
\begin{definition}
	(Similarity) For matrices $ A,A'\in M_{n\times m}(\F) $ are \textit{similar} if there exists $ P\in GL_n(\F) $ such that $ A'=\inv P AP $.
\end{definition}
\begin{remark}
We have some remarks showing the similarity and equivalence are not the same thing.
\begin{enumerate}
	\item Similarity is an equivalence relation on $ M_{n\times n}(\F) $.
	\item Similar matrices are equivalent but equivalent matrices need not be similar.
\end{enumerate}
For example every matrix in $ GL_n(\F) $ is equivalent to $ I_n $ but $ I_n $ forms its only single element equivalence class, when we think about similarity.
\end{remark}
















\end{document}
