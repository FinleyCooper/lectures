\documentclass{article}
\usepackage{../header}
\title{Statistics}
\author{Notes by Finley Cooper}
\newcommand{\iid}{\overset{\text{idd}}{\sim}}
\begin{document}
  \maketitle
  \newpage
  \tableofcontents
  \newpage
  \section{Parametric Estimation}
  \subsection{Review of IA Probability}
  \subsubsection{Starting axioms}
  We observe some data $ X_1,\dots, X_n $ iid random variables taking values in a sample space $ \mathcal X $. Let $ X=(X_1,\dots, X_n) $. We assume that $ X_1 $ belongs to a \textit{statistical model} $ \{p(x; \theta):\theta \in \Theta\} $ with $ \theta $ unknown. For example $ p(x;\theta) $ could be a pdf.\par
  Let's see some examples
  \begin{enumerate}
    
	  \item Suppose that $ X_1\sim \text{Poisson}(\lambda) $ where $ \theta=\lambda\in\Theta=(0,\infty) $.

  \item Suppose that $ X_1\sim\mathcal N(\mu,\sigma^2) $, where $ \theta=(\mu,\sigma^2)\in\Theta=\R\times(0,\infty) $.
\end{enumerate}
We have some common questions about these statistical models.
\begin{enumerate}
	\item We want to give an estimate $ \hat{\theta}:\mathcal X^n\to \Theta $ of the true value of $ \theta $.
	\item We also want to give an interval estimator $ (\hat\theta_1(X),\hat\theta_2(X)) $ of $ \theta $.
	\item Further we want to test of hypothesis about $ \theta $. For example we might make the hypothesis that $ H_0:\theta=0 $.
\end{enumerate}
Let's do a quick reivew of IA Probability. Let $ X:\Omega\to \R $ be a random variable defined on the probability space $ (\Omega,\mathcal F, \mathbb P) $. So $ \Omega $ is the sample space, $ \mathcal F $ is the set of events, and $ \mathbb P: \mathcal F\to [0,1] $ is the probability measure.\par
The cumulative distribution function (cdf) of $ X $ is $ F_X(s)=\prob{X\le x} $. A discrete random variable takes values in a countable set $ \mathcal X $ and has probability mass function (pmf) given by $ p_X(x)=\prob{X=x} $. A continuous random variable has probability density function (pdf) $ f_X $ satisfying $ P(X\in A)=\int_A f_X(x)\mathrm dx $ (for measurable sets $ A $). We say that $ X_1,\dots, X_n $ are independent if $ \prob{X_1\le x_1,\dots, X_n\le x_n}=\prod_{i=1}^n \prob{X_i\le x_i} $ for all choices $ x_1,\dots, x_n $. If $ X_1,\dots, X_n $ have pdfs (or pmfs) $ f_{X_1},\dots, f_{X_n} $, then this is equivalent to $ f_X(x)=\prod_{i=1}^nf_{X_i}(x_i) $ for all $ x_i $. The expectation of $ X $ is,
\[
  \mathbb E(x)=\begin{cases}
	  \sum_{x\in \mathcal X}xp_X(x) & \text{if } X\text{ is discrete}\\
	  \int_{-\infty}^\infty xf_X(x) & \text{if } X\text{ is continuous}
  \end{cases}.
\]
The variance of $ X $ is $ \Var{X}=\mathbb E[(X-\mathbb E(X))^2]$. The moment generating function of $ X $ is $ M(t)=\mathbb E[e^{tX}] $ and can be used to generate the momentum of a random varaible by taking derivatives. If two random variables have the same moment generating functions, then they have the same distribution.\par
The expectation operator is linear and
\[
	\Var{a_1X_1+\dots+a_nX_n}=\sum_{i,j=1}^n a_ia_j\Cov{X_i,X_j},
\]
where $ \Cov{X_i,X_j}=\mathbb E[(X_i-\mathbb E(X_i)(X_j-\mathbb E(X_j))] $. In vector notation writing $ X $ as the column vector of $ X_i $ and $ a $ as the column vector for $ a_i $ we get that 
\[
	\mathbb E[a^T X]=a^T E[X].
\]
Similar for the variance we get that
\[
	\Var{a^TX}=a^T\Var X a
\]
where $ \Var X $ is the covariance matrix for $ X $ with entries $ \Cov{X_i,X_j} $.
\subsubsection{Joint random variables}
If $ X $ is a discrete random variable with pmf $ P_{X,Y}(x,y)=\prob{X=x, Y=y} $ and marginal pmf $ P_Y(y)=\sum_{x\in X}P_{X,Y}(x,y) $, then the conditional pmf is
\[
	P_{X\mid Y}(x\mid y)=\prob{X=x\mid Y=y}=\frac{P_{X,Y}(x,y)}{P_Y(y)}.
\]
If $ X,Y $ are continuous then the join pdf $ f_{X,Y} $ satifies
\[
	\prob{X=x,Y=y} = \int_{-\infty}^x\int_{-\infty}^y f_{X,Y}\mathrm dx\mathrm dy
\]
and the marginal pdf of $ Y $ is
\[
	f_Y(y)=\int_{-\infty}^\infty f_{X,Y}(x,y)\mathrm dx.
\]
The \textit{conditional pdf} of $ X $ given $ Y $ is $ f_{X\mid Y}(x\mid y)=\frac{f_{X,Y}(x,y)}{f_Y(y)} $.\par
The conditional expectation of $ X $ given $ Y $ is
\[
E(X\mid Y) = \begin{cases}
	\sum_{x\in X} x\mathbb P_{X|Y}(x\mid Y) & \text{if } X \text{ is discrete} \\
	\int_{-\infty}^\infty xf_{X\mid Y}(x\mid Y)\mathrm dy & \text{if } Y \text{ is continuous}
\end{cases}.
\]
\begin{remark}
$ \mathbb E(X\mid Y) $ is a function of $ Y $ so $ \mathbb E(X\mid Y) $ is a random variable.
\end{remark}
We also have the law of total expectation,
\[
	\mathbb E[X]=\mathbb E[\mathbb E[X\mid Y]].
\]
This is a consequence of the law of total probability which is
\[
	p_X(x)=\sum_y p_{X\mid Y}(x\mid y)p_Y(y).
\]
Now we have a new (but less useful) theorem similar to the tower property of expectation.
\begin{theorem}
	(Law of total variance)
	\[
		\Var{X}=\mathbb E[\Var{X\mid Y}] +\Var{\mathbb E[X\mid Y]}.
	\]
\end{theorem}
\pf Write $ \Var{X} = \mathbb E[X^2]-(\mathbb E[X])^2 $, so
\begin{align*}
	\Var{X} &= \mathbb E(\mathbb E(X^2\mid Y) - (\mathbb E(\mathbb E(X\mid Y)))^2 \\
		&= \mathbb E[\mathbb E(X^2\mid Y)-(\mathbb E(X\mid Y))^2] + \mathbb E((\mathbb E(X\mid Y))^2) - (\mathbb E(\mathbb E(X\mid Y)))^2\\
		&= \mathbb E[\Var{X\mid Y}] +\Var{\mathbb E[X\mid Y]}.\qed
\end{align*}
We also have the change of variables formula. If we have a mapping $ (x,y)\to (u,v) $, a bijection from $ \R^2\to \R^2 $, then
\[
	f_{U,V}(u,v) = f_{X,Y}(x(u,v),y(u,v))|\det J|,
\]
where $ J $ is the Jacobian matrix.
\subsubsection{Limit theorems}
Suppose $ X_1,\dots, X_n $ are iid random variables with mean $ \mu $ and variance $ \sigma^2 $. Define the sum $ S=\sum_{i=1}^n X_i $ and the sample mean $ \bar X_n = \frac{S_n}n $. We have the following theorems.
\begin{theorem}
	(Weak Law of Large Numbers)
	\[
	  \bar X_n\to \mu
	\]
	where $ \to $ means that $ \prob{|\bar X_n-\mu|>\varepsilon}\to 0 $ as $ n\to \infty $ for all $ \varepsilon>0 $.
\end{theorem}
\begin{theorem}
	(Strong Law of Large Numbers)
	\[ \bar X_n\to \mu \]
	almost surely. So $ \prob{\lim_{n\to\infty}\bar X_n=\mu}=1 $.
\end{theorem}
\begin{theorem}
	(Central Limit Theorem)
	The random variables
	\[
		Z_n = \frac{S_n-n\mu}{\sigma\sqrt n}
	\]
	is approximately $ \mathcal N(0,1) $ for large $ n $. Or we can write this as
	\[
	  S_n\approx \mathcal N(n\mu, n\sigma^2).
	\]
	Formally this means that $ \prob{Z_n\le z}\to \Phi(z) $ for all $ z\in \R $ where $ \Phi(z) $ is the cdf of $ \mathcal N(0,1) $.
\end{theorem}
\subsection{Estimators}
Suppose that $ X_1,\dots, X_n $ are iid with pdf $ f_X(x\mid \theta) $ and parameter $ \theta $ unknown.
\begin{definition}
	(Estimator) A function of the data $ T(X)\to\hat\theta $ which is used to approximate the true parameter $ \theta $ is called an \textit{estimator} (or sometimes a \textit{statistic}). The distribution of $ T(X) $ is the \textit{sampling distribution}
\end{definition}
For an example suppose that $ X_1,\dots, X_n\sim\mathcal N(\mu,1) $ and let $ \hat \mu = T(x) = \frac 1n \sum_{i=1}^n X_i $. The sampling distribution of $ \hat \mu $ is $ T(X)\sim \mathcal N(\mu, \frac 1{n}) $.
\begin{definition}
	(Bias) The \textit{bias} of a random variable $ \hat\theta=T(X) $ is 
	\[
		\mathrm{bias}(\hat\theta) = \mathbb E_\theta(\hat\theta)-\theta,
	\]
	where the expectation is taken over the model $ X_1\sim f_X(\cdot \mid \theta) $.
\end{definition}
\begin{remark}
 In general the bias might be a function of $ \theta $ which is not explicit in the notation.
\end{remark}
\begin{definition}
	(Unbiased estimator) We say that an estimator is \textit{unbiased} if $ \mathrm{bias}(\hat\theta)=0 $ for all $ \theta\in\Theta $.
\end{definition}
So for our estimator from before, $ \hat\mu $, is unbiased since
\[
	\mathbb E_\mu(\hat \mu) = \frac 1n\sum_{i=1}^n \mathbb E_\mu(X_i) = \mu.
\]
\subsubsection{Bias-variance decomposition}
\begin{definition}
	(Mean squared error) The \textit{mean squared error} of an estimator $ \hat\theta $ is
	\[
		\mathrm{mse}(\hat\theta) = \mathbb E_\theta[(\hat\theta-\theta)^2].
	\]
\end{definition}
\begin{remark}
  Note that the MSE is generally a function of $ \theta $ like the bias. Again this is not clear from the notation.
\end{remark}
\begin{proposition}
	(Bias-variance decomposition) For an estimator $ \hat\theta $ of a parameter $ \theta $, we have that
	\[
		\mathrm{mse}(\hat\theta)=\left(\mathrm{bias}(\hat\theta)\right)^2 + \mathrm{Var}_\theta(\hat\theta).
	\]
\end{proposition}
\pf 
\begin{align*}
	\mathrm{mse}(\hat\theta) &= \mathbb E_\theta[(\hat\theta-\theta)^2] \\
				 &= \mathbb E_\theta\left[\left(\hat\theta-\mathbb E_\theta(\hat\theta)+\mathbb E_\theta(\hat\theta)-\theta\right)^2\right]\\
				 &= \mathbb E_\theta[(\hat\theta -\mathbb E_\theta(\hat\theta))^2]+(\mathbb E_\theta(\hat\theta)-\theta)^2 + 2(\mathbb E_\theta(\hat\theta) - \theta) \cdot \mathbb E_\theta[\hat\theta - \mathbb E_\theta(\hat\theta)]\\
				 &= \left(\mathrm{bias}(\hat\theta)\right)^2 + \mathrm{Var}_\theta(\hat\theta).\qed
\end{align*}
Let's see an example. Suppose that $ X\sim \text{Binomial}(n,\theta) $ where is $ n $ is known and we want to estimate $ \theta\in[0,1] $. Let $ T_u=\frac{X}n $ be an estimator, so $ \mathbb E_\theta(T_u)=\frac{\mathbb E(X)}n = \frac{n\theta}n = \theta $, hence this estimator is unbiased. And $ \mathrm{mse}(T_u) = \Var{T_u} +\mathrm{bias}(T_u) = \frac{\theta(1-\theta)}n $.\par
Instead if we used the estimator $ T_b = \frac{X+1}{n+2}  = \omega\frac Xn + (1-\omega)\frac 12 $ where $ \omega = \frac n{n+2} $. We get that
\begin{align*}
	\mathrm{bias}(T_b) &= (1-\omega)(\frac 12 - \theta)\\
	\Var{T_b} &= \omega^2\frac{\theta(1-\theta)}n.
\end{align*}
Giving that
\[
	\mathrm {mse}(T_b) = \omega^2 \frac{\theta(1-\theta)}n + (1-\omega)^2(\frac 12 - \theta)^2
\]
\subsection{Sufficient statistics}
Suppose $ X_1,\dots, X_n $ are iid random variables taking values in $ \chi $ with pdf $ f_{X_1}(\cdot\mid \theta) $. Consider $ \theta $ as fixed. Denote $ X=(X_1,\dots, X_n) $.
\begin{definition}
(Sufficient statistics) A statistics $ T $ is \textit{sufficient} for $ \theta $ if the conditional distribution of $ X$ given $ T(X) $ does not depend on $ \theta $.
\end{definition}
\begin{remark}
  The parameter $ \theta $ may be a vector, and $ T(X) $ may be a vector.
\end{remark}
Suppose $ X_1,\dots, X_n \sim \mathrm{Binomial}(1,\theta) $ iid for some $ \theta\in [0,1] $. Then 
\begin{align*}
	f_X(x\mid \theta)&=\prod_{i=1}^n \theta^{x_i}(1-\theta)^{1-x_i}\\
			 &= \theta^{\sum x_i}(1-\theta)^{n-\sum x_i}
\end{align*}
Define $ T(X)=\sum_{i=1}^n x_i $. Now 
\begin{align*}
	f_{X\mid T =t}(x\mid T(x)=t)&=\frac{\mathbb P_\theta(X=x,T(X)=t)}{\mathbb P_\theta(T(X) =t)}\\
				    &= \frac{\mathbb P_\theta(X=x)}{\mathbb P_\theta(T(X)=t)} = \frac{\theta^{\sum x_i}(1-\theta)^{n-\sum x_i}}{\binom nt \theta^t (1-\theta)^{n-t}} = \frac 1{\binom nt}.
\end{align*}
\begin{theorem}
	(Factorisation criterion) The statistics $ T $ is sufficient for $ \theta $ if and only if $ f_X(x\mid \theta)= g(T(x),\theta)h(x) $ for some suitable $ g $ and $ h $.
\end{theorem}
\pf Suppose that $ f_X(x\mid \theta) = g(T(x),\theta)h(x) $. We can compute
\begin{align*}
	f_{X\mid T=t}(x\mid T=t) &= \frac{\mathbb P_\theta(X=x,T(x) =t)}{\mathbb P_\theta(T(x)=t)}\\
				 &= \frac{g(T(x),\theta)h(x)}{\sum_{x';T(x')=t}g(t,\theta)h(x')}\\
				 &= \frac{h(x)}{\sum_{x';T(x')=t}h(x')}
\end{align*}
which doesn't depend on $ \theta $, so $ T(X) $ is sufficient.\par
Conversely, suppose $ T(X) $ is sufficient. We can write
\begin{align*}
\mathbb P_\theta(X=x) &= \mathbb P_\theta(X=x,T(X)=T(x))\\
		      &= \mathbb P_\theta(X=x\mid T(X)=T(x))\mathbb P_\theta(T(X)=T(x))\\
		      &= h(x)g(T(X),\theta).
\end{align*}
So we're done.\qed
\begin{remark}
	For our example before we can define $ T(x) = \sum x_i $ and $ g(t,\theta)  = \theta^t (1-\theta)^{n-t} $ and $ h(x) = 1 $.
\end{remark}
Let's see another example. Let $ X_1,\dots,X_n $ be iid uniform on $ [0,\theta] $ for some $ \theta\in (0,\infty) $. So
\begin{align*}
	f_X(x=\theta)&=\prod_{i=1}^n \frac 1\theta \mathbf 1\{x_i\in [0,\infty]\}\\
		     &=\frac 1{\theta^n} \mathbf 1\{\max x_i\le \theta\}\mathbf 1\{\min x_i\ge 0 \}\\
		     &= g(T(x),\theta)h(x).
\end{align*}
\subsection{Minimal sufficiency} 
\begin{definition}
	(Minimal sufficient) A sufficient statistics $ T(X) $ is \textit{minimal sufficient} if it is a function of every other sufficient statistic. So if $ T'(X) $ is also sufficient, then $ T'(x)=T'(y)\implies T(x)=T(y) $ for all $ x,y\in \chi $.
\end{definition}
\begin{remark}
  Minimal sufficient statistics are unique up to bijection. 
\end{remark}
\begin{theorem}
	Suppose $ T(X) $ is a statistics such that $ \frac{f_X(x\mid \theta)}{f_X(y\mid \theta)} $ is constant a function of $ \theta $ if and only if $ T(x)=T(y) $. Then $ T $ is minimal sufficient.
\end{theorem}
Let's see an example before we prove this. Suppose that $ X_1,\dots, X_n\sim\mathcal N(\mu,\sigma^2) $. Then
\begin{align*}
	\frac{f_X(x\mid \mu,\sigma^2)}{f_X(y\mid \mu,\sigma^2)} &=\frac{(2\pi \sigma^2)^{-n/2}\exp\left(-\frac 1{2\sigma^2}\sum(x_i-\mu)^2\right)}{(2\pi \sigma^2)^{-n/2}\exp\left(-\frac 1{2\sigma^2}\sum(y_i-\mu)^2\right)}\\
								&=\exp\left(-\frac{1}{2\sigma^2}\left(\sum_i x_i^2 - \sum_i y_i^2\right)+\frac \mu{\sigma^2}\left(\sum_i x_i-\sum_i y_i\right)\right)
\end{align*}
This is constant in $ (\mu,\sigma^2) $ if and only if $ \sum_i x_i=\sum_i y_i $ and $ \sum_i x_i^2 = \sum_i y_i^2 $ therefore $ T(X)=\left(\sum_{i=1}^n X_i,\sum_{i=1}^n X_i^2\right) $ is minimal sufficient.
\par
\pf Need to show that such a statistics is sufficient and minimal. First we'll show sufficiency. For each $ t $ pick a $ x_t $ such that $ T(x_t) = t $. Now let $ x\in \chi_N^2 $ and let $ T(x)=t $. So $ T(x) = T(x_t) $, so by the hypothesis $ \frac{f_X(x,\theta)}{f_X(x_t,\theta)} $ does not depend on $ \theta $. Let this be $ h(x) $ and let $ g(t,\theta) = f_X(x,\theta) $ then we have that $ f_X(x,\theta)=g(t,\theta)h(x) $ so sufficient.\par
Now let $ S $ be any other sufficient statistic. By the factorisation criterion, there exists $ g_S, h_S $ such that $ f_X(x\mid\theta)=G_S(S(x),\theta)h_S(x) $. Suppose $ S(x)=S(y) $. Then
\begin{align*}
	\frac{f_X(x\mid \theta)}{f_X(y\mid\theta)} = \frac{g_S(S(x),\theta)h_S(x)}{g_S(S(y),\theta)h_S(y)} = \frac{h_S(x)}{h_S(y)}
\end{align*}
which does not depend on $ \theta $ so $ T(x)=T(y) $ so $ T $ is minimal sufficient.\qed
\par
We know that bijections of minimal sufficient statistics are still minial sufficient statistics, so we can write our minimal sufficient statistic for $ X_1,\dots, X_n\sim\mathcal N(\mu,\sigma^2) $ as
\[
	S(X)=(\overline X,S_{XX})
\]
where $ \overline X = \frac 1n \sum_i X_i $ and $ S_{XX} = \sum_i(X_i-\overline X)^2 $, since there is a bijection between them.\par
Until now we used $ \mathbb E_\theta $ and $ \mathbb P_\theta $ to denote expectation and probability when $ X_1,\dots, X_n $ are iid from a distribution with pdf $ f_X(x\mid \theta) $. From now on we drop the subscript $ \theta $ to simplify notation.
\begin{theorem}
	(Rao-Blackwell Theorem) Let $ T $ be a sufficient statistic for $ \theta $ and let $ \tilde\theta $ be an estimator for $ \theta $ with $ \mathbb E(\tilde\theta^2)<\infty,\ \forall\theta $. Define a new estimator $ \hat\theta = \mathbb E[\tilde\theta\mid T(X)] $. Then for all $\theta $,
	\[
		\mathbb E[(\hat\theta-\theta)^2] \le \mathbb E[(\tilde\theta-\theta)^2].
	\]
	This inequality is strict unless $ \tilde\theta $ is a function of $ T $.
\end{theorem}
\begin{remark}
	We have that $ \hat\theta(T)=\int \tilde\theta(x)f_{X\mid T}(x\mid T)\mathrm dx $. By sufficiency of $ T $, the conditional pdf does \textit{not} depend on $ \theta $ so $ \hat \theta $ does not depend on $ \theta $, and is valid estimator.
\end{remark}
\pf By the tower property of expectation,
\[
	\mathbb E[\hat\theta] = \mathbb E[ \mathbb E(\tilde\theta\mid T)] = \mathbb E[\tilde \theta].
\]
So $ \mathrm{bias}(\hat\theta) = \mathrm{bias}(\tilde\theta) $ for all $ \theta $. By the conditional variance formula,
\begin{align*}
	\Var{\tilde \theta} &= \mathbb E\left[\Var{\tilde\theta\mid T}\right] + \Var{\mathbb E\left(\tilde \theta\mid T\right)}\\
			    &= \mathbb E\left[\Var{\tilde\theta\mid T}\right] + \Var{\hat\theta}\\
			    &\ge \Var{\hat\theta}.
\end{align*}
So
\[
	\mathrm{mse}(\tilde\theta)\ge \mathrm{mse}(\hat\theta).
\]
Equality is achieved only when $ \Var{\tilde\theta\mid T} = 0 $ with probability $ 1 $ which requiers $ \tilde \theta $ to be a function of $ T $.\qed
\par Let's see an example of this. Suppose that $ X_1,\dots, X_n\sim \text{Poisson}(\lambda) $ iid. Let $ \theta = \prob{X_1=0}=e^{-\lambda} $. Then
\[
	f_X(x\mid\theta) = \frac{e^{-n\lambda}\lambda^{\sum x_i}}{\prod_i x_i!} = \frac{\theta^n (-\log \theta)^{\sum x_i}}{\prod_i x_i!}.
\]
By the factorisation criterion, $ T(X) = \sum_i x_i $ is sufficient. Recall that $ \sum x_i \sim \text{Poisson}(n\lambda) $. Let $ \tilde\theta = \mathbf 1\{X_1 = 0\} $. Then
\begin{align*}
	\hat \theta = \mathbb E[\tilde\theta\mid T=t] &= \prob{X_1 = 0\mid \sum_{i=1}^n X_i = t}\\
						      &= \frac{\prob{X_1  =0, \sum_{i=2}^n X_i = t}}{\prob{\sum_{i=1}^n X_i = t}}\\
						      &= \frac{\prob{X_1 = 0}\prob{\sum_{i=2}^n X_i = t}}{\prob{\sum_{i=1}^n X_i =t}}\\
						      &= \frac{e^{-\lambda }e^{-(n-1)\lambda}\frac{((n-1)\lambda)^t}{t!}}{e^{-n\lambda} \frac{(n\lambda)^t}{t!}} = \left(\frac{n-1}n\right)^t
\end{align*}
Hence $ \hat\theta= \left(1-\frac1n\right)^{\sum x_i} $ has $ \mathrm{mse}(\hat\theta)< \mathrm{mse}(\tilde\theta) $ for all $ \theta $.  We can see that as $ n\to \infty $, $ \hat\theta\to e^{-\overline X} = e^{-\lambda} =\theta$.
\par
Let $ X_1,\dots, X_n\sim \mathrm{Uniform}([0,\theta]) $ and suppose we want to estimate $ \theta\ge 0 $. Last time we saw that $ T = \max{X_i} $ is sufficient for $ \theta $. Let $ \tilde\theta = 2X_1 $ be an estimator (unbias). Then
\begin{align*}
	\hat \theta = \mathbb E[\tilde\theta\mid T=t] &= 2\mathbb E[X_1\mid \max X_i =t]\\
						      &= 2\mathbb E[X_1\mid \max X_i = t, X_1 = \max X_i]\prob{X_1 = \max X_i\mid \max X_i = t}\\
						      &\qquad+ 2\mathbb E[X_1\mid \max X_i =t, X_1\ne \max X_i] \prob{X_1\ne \max X_i \mid \max X_i t}\\
						      &= 2t\frac 1n + 2\mathbb E\left[X_1\mid X_1<t, \max_{i>1} X_i =t\right]\left(\frac{n-1}n\right)\\
						      &=\left(\frac{n+1}n\right)t.
\end{align*}
Hence $ \hat\theta = \frac{n+1}n \max_i X_i $ is an estimator with $\text{mse}(\hat\theta)<\text{mse}(\tilde\theta) $.
\subsection{Likelihood}
\begin{definition}
	(Likelihood) Let $ X=(X_1,\dots X_n) $ have a joint pdf $ f_X(x\mid \theta) $. The \textit{likelihood} of $ \theta $ is the function
	\[
	  L:\theta \to f_X(x\mid\theta).
	\]
	The max likelihood estimator (MLE) is the value of $ \theta $ maximizing $ L $.
\end{definition}
	If $ X_1,\dots, X_n\sim f_X(\cdot\mid\theta) $ iid, then $ L(\theta) = \prod_{i=1}^n f_X(x_i\mid \theta) $. \par
It's usually easier to work with the log-likelihood, since this reduces to a sum. So in the iid case,
\begin{align*}
	\ell(\theta)= \log(L(\theta)) = \sum_{i=1}^n \log f_X(x_i\mid \theta).
\end{align*}
For example let $ X_1,\dots, X_n \sim \text{Bernoulli}(p)$ iid. Then we get that
\[
	\ell(p) = \left(\sum_{i=1}^n X_i\right)\log p + \left(n-\sum_{i=1}^n X_i\right) \log(1-p).
\]
Taking the derivative with respect to $ p $,
\begin{align*}
	\frac{\partial \ell}{\partial p} = \frac{\sum_i X_i} p - \frac{n-\sum_i X_i}{1-p}.
\end{align*}
So setting the derivative to zero we get that
\[
	p = \frac{\sum X_i} n.
\]
Hence the MLE is
\[
	\hat p = \frac{\sum_i X_i}n,
\]
and since $ \mathbb E[\hat p] = p $, this is unbiased.\par
Now suppose $ X_1,\dots, X_n\sim\mathcal N(\mu,\sigma^2) $.
\begin{align*}
	\ell(\mu,\sigma^2) &= -\frac n2 \log(2\pi) -\frac n2 \log(\sigma^2) -\frac 1{2\sigma^2} \sum_{i=1}^n (X_i-\mu)^2.
\end{align*}
So
\begin{align*}
	\frac{\partial \ell}{\partial \mu} = -\frac 1{\sigma^2} \sum_{i=1}^n (X_i - \mu) 
\end{align*}
which is zero when $ \mu = \frac{\sum_i X_i}n $ regardless of $ \sigma $. Also
\begin{align*}
	\frac{\partial \ell}{\partial \sigma^2} = -\frac{n}{2\sigma^2} + \frac 1{2\sigma^4} \sum_{i=1}^n (X_i-\mu)^2.
\end{align*}
If we set $ \mu = \frac{\sum_i X_i}n $ then we get $ \frac{\partial\ell}{\partial\sigma^2}=0 $ if $ \sigma^2 = \frac 1n \sum(X_i-\bar X)^2 = \frac {S_{xx}}n $. Hence the MLE is
\[
	(\hat \mu , \hat{\sigma}^2 = (\bar X, \frac{S_{xx}}n).
\]
Notte that $ \mu $ is unbiased, but we will see later that
\[
	\frac{S_{xx}}{\sigma^2} = \frac{n\hat\sigma^2}{\sigma^2} \sim \chi_{n-1}^2.
\]
So $ \mathbb E[\hat\sigma^2] = \mathbb E\left(\frac{S_{xx}}n\right) = \frac {n-1}n \sigma^2 $. So $ \hat\sigma^2 $ is not unbiased, but is asymptotically unbiased as $ n\to\infty $.\par
Suppose now that $ X_1,\dots, X_n\sim \text{Uniform}([0,\theta]) $ iid. Then
\[
	\ell(\theta) = \frac 1{\theta^n} \mathbf 1\{\max_i X_i\le \theta\}.
\]
Hence the MLE is $ \hat\theta_{\text{MLE}} = \max_i X_i $. Recall that last time, we had an aunbiased estimator $ \tilde\theta $ and by the Rao-Blackwell Theorem we found the estimator $ \hat\theta = \mathbb E[ \tilde\theta\mid T] = \frac{n+1}n \max_i X_i $. Note that $ \hat\theta_{\text{MLE}} =\frac{n}{n+1} \hat \theta	$, so $ \mathbb E[\hat \theta_{\text{MLE}}] = \frac{n}{n+1} \mathbb E[\hat\theta] = \frac{n}{n+1} \theta $. Again this not unbiased, but is asymptotically unbiased.
\par 
Let's see some properties of the MLE.
\begin{enumerate}
	\item If $ T $ is a sufficient statistic, the MLE is a function of $ T $. We can factorise $ L(\theta) = g(T(x),\theta)h(x) $. 
	\item If $ \phi = h(\theta) $ where $ h $ is a bijection, the MLE of $ \phi $ is $ \hat\phi = h(\hat\theta) $ where $ \hat\theta $ is the MLE of $ \theta $.
	\item Asymptotic normality: $ \sqrt n (\hat\theta_{\text{MLE}} -\theta) $ is approximately normal with mean $ 0 $ for large $ n $. The covariance matrix is the "smallest attainable" (see II Principles of Statistics).
\end{enumerate}
\subsection{Confidence intervals}
\begin{definition}
	(Confidence intervals) A $ (100\gamma)\%$ confidience interval for a paramter $ \theta $ is a random interval $ (A(X),B(X)) $ such that $ \prob{A(X)\le \theta\le B(X)}= \gamma $ for some $ \gamma\in (0,1)  $ and all values of the true parameter $ \theta $. 
\end{definition}
\begin{remark}
  The incorrect interperatation: Having observed $ X=x $, there is a $ 1-\gamma $ probability that $ \theta $ is in $ (A(X),B(X)) $. This is wrong.
\end{remark}
Suppose that $ X_1,\dots, X_n\sim \mathcal N(\theta,1) $ iid. We want to find a $ 95\% $ confidence interval for $ \theta $. We know that $ \overline X = \frac 1n \sum_{i=1}^n X_i\sim \mathcal N(\theta,\frac 1n) $. If we define $ Z = \sqrt n (\overline X - \theta) $ Then $ Z\sim \mathcal N(0,1) $ no matter the value of $ \theta $. Let $ z_1,z_2 $ be numbers with $ \Phi(z_1)-\Phi(z_2) = 0.95 $ where $ \Phi $ is the cdf of the standard normal. $ \prob{z_1\le \sqrt n(\overline X-\theta)\le z_2} = 0.95 $ rearranging we get that
\begin{align*}
	\prob{\overline X - \frac{z_2}{\sqrt n} \le \theta\le \overline X - \frac{z_1}{\sqrt n}} = 0.95
\end{align*}
hence 
\[
	\left(\overline X - \frac{z_2}{\sqrt n}, \overline X - \frac{z_1}{\sqrt n}\right)
\]
is a $ 95\% $ confidence interval.\par
This is the recipe for confidence intervals.
\begin{enumerate}
	\item Find a quantity $ R(X,\theta) $ such that this $ \mathbb P_\theta $ distribution of $ R(X,\theta) $ does not depend on $ \theta $. This is called a \textit{pivot} for example $ R(X,\theta) = \sqrt n(\overline X-\theta) $.
	\item Write down the statement
		\[
			\prob{c_1\le R(X,\theta)\le c_2}=\gamma
		\]
		where $ (c_1,c_2) $ are quantiles of the distribution of $ R(X,\theta) $. 
	\item Rearranging the above to leave $ \theta $ in the middle of the inequality, so we get something in the form
		\[
			\prob{A(X)\le \theta \le B(X)}.
		\]
\end{enumerate}
\begin{remark}
	When $ \theta $ is a vector, we talk about \textit{confidence sets} rather than intervals.
\end{remark}
Suppose that $ X_1,\dots, X_n\sim \mathcal N(0,\sigma^2) $ iid. We want a $ 95\% $ confidence interval for $ \sigma^2 $. Note that
\[
	\frac{X_i} \sigma \sim \mathcal N(0,1),
\]
so $ \sum_{i=1}^n \frac{X_i^2}{\sigma^2}\sim \chi^2_n $. Hence \[
	R(X,\sigma^2) = \sum_{i=1}^n \frac{X_i^2}{\sigma^2}
\]
is a \textit{pivot}. Let $ \inv F_{\chi^2_n}(0.025) $ and $ \inv F_{\chi^2_n}(0.975) $. Then
\begin{align*}
	\prob{c_1\le \sum_{i=1}^n \frac{X_i^2}{\sigma^2} \le c_2} = 0.95,
\end{align*}
so rearranging we get that
\[
	\prob{\frac{\sum X_i^2}{c_2} \le \sigma^2 \le \frac{\sum X_i^2}{c_1}}=0.95
\]
gives our confidence interval.
\par
Now suppose that $ X_1,\dots, X_n\sim \mathrm{Bernoulli}(p) $ for large $ n $. We will find an approximate 95\% confidence interval for $ p $. The maximium likelihood estimator of $ p $ is $ \hat p = \frac 1n \sum_{i=1}^n X_i $. By the central limit theorem $ \hat p \sim \mathcal N\left(p,\frac{p(1-p)}n\right) $ approximately. Thus
\[
	\frac{\sqrt n (\hat p - p)}{\sqrt{p(1-p)}}\sim \mathcal N(0,1)
\]
for large $ n $. So we have our pivot, which gives
\[
	\prob{-z_{0.025} \le \frac{\sqrt n(\hat p -p)}{\sqrt{p(1-p)}} \le z_{0.025}} \approx 0.95
\]
Instead of inverting directly, if $ n $ is large $ \hat p \approx p $, so switching $ p $ with $ \hat p $ on the denominator we get that
\[
	\prob{\hat p - z_{0.025}\sqrt{\frac{\hat p(1-\hat p)}n}\le p\le \hat p + z_{0.025}\sqrt{\frac{\hat p(1-\hat p)}n}}\approx 0.95
\]
which gives our confidence interval. Since for all $ \hat p \in [0,1] $ we have $ \hat p(1-\hat p)\le \frac 14 $ we would also report a conservative confidence interval of $ \hat p \pm z_{0.025}\sqrt{\frac 1{4n}} $.
\subsection{Bayesian estimation}
So far we've been using frequentist methods treating $ \theta\in\Theta $ as fixeed. For Bayesian methods, we treat $ \theta $ as random with a prior distribution $ \pi(\theta) $. Conditional on $ \theta $ the data $ X $ has pdf $ f_X(\cdot\mid \theta) $. Having observed that $ X=x $, we combine with the prior to form a posterior distribution $ \pi(\theta\mid X) $. By Bayes' rule,
\begin{align*}
	\pi(\theta\mid x) = \frac{\pi(\theta)f_X(x\mid \theta)}{f_X(x)}
\end{align*}
where $ f_X(x) $ is the marginal distribution of $ X $, so
\[
  f_X(x) = \begin{cases}
	  \int_\Theta f_X(x\mid \theta)\pi(\theta) \mathrm d\theta & \text{if }\theta \ \text{is continuous}\\
	  \sum_{\theta\in \Theta} f_X(x\mid \theta)\pi(\theta) & \text{if}\ \theta\ \text{is discrete}
  \end{cases}.
\]
More simply,
\[
  \pi(\theta \mid x)\propto \pi(\theta)f_X(x\mid \theta).
\]
Often is it easier to reconginise the RHS as propotional to a known distribution. 
\begin{remark}
  By the factorisation criterion, the posterior only depends on $ X $ through a sufficient statistic.
  \begin{align*}
	  \pi(\theta\mid x)&\propto \pi(\theta) \cdot f_X(x\mid \theta) = \pi(\theta)\cdot g(T(x),\theta)h(x)\\
			   &\propto \pi(\theta)g(T(x),\theta).
  \end{align*}
\end{remark}
Suppose $ \theta\in [0,1] $ is the mortality rate for some procedure at Addenbrooks. In the first 10 operations there are no deaths. In other hospitals across the country the mortality rate is between $ 3-20\% $, with average of $ 10\% $. Consider the prior distribution $ \pi(\theta)\sim \text{Beta}(a,b) $ we can choose $ (a,b) = (3,27) $ so $ \pi(\theta) $ as mean $ 0.1 $ and $ \pi(0.03 <\theta<0.2) = 0.9 $.
\par
Let $ X_i\sim \text{Bernoulli}(\theta) $ be indicator for whether $ i $th patient at Addenbrookes dies.
\[
	f_X(x\mid \theta) = \theta^{\sim x_i}(1-\theta)^{n-\sum x_i}.
\]
The posterior is
\begin{align*}
	\pi(\theta\mid X) &\propto \pi(\theta)f_X(x\mid \theta) \\
			  &\propto \theta^{a-1}(1-\theta)^{b-1} \theta^{\sum x_i}(1-\theta)^{n-\sum x_i}\\
			  &= \theta^{\sum x_i+a-1}(1-\theta)^{b+n-\sum x_i-1}.
\end{align*}
Hence
\[
	\pi(\theta\mid X) \sim \text{Beta}(a+\sum x_i, b+n-\sum x_i)
\]
so pluggin in $ a=3,b=27,n=10,\sum X_i = 0 $, so the posterior is $ \text{Beta}(3,37) $.
\begin{remark}
	In the example the prior and posterior were from the same family of distributions known as \text{conjugacy}.
\end{remark}
Supposewe put a $ \text{Beta}(a,b) $ prior on the parameter $ \theta $ of kidney cancer death rates in each county. We can estimate $ (a,b)=(27, 58000) $ with $ \frac a{a+b}\approx 4.65\times 10^{-9} $ being the kidney cancer death rate in the United States. The previous example shows that if we observe $ \sum_{i=1}^n X_i$ deaths in a county, the posterior mean estimate is $ \frac{a+\sum X_i}{a+b-n} $. This is equal to
\begin{align*}
	\frac n{a+b+n}\cdot\frac{\sum X_i}n + \frac{a+b}{a+b+n}\cdot \frac a{a+b}.
\end{align*}
For large $ n $, we use $ \approx \frac{\sum X_i}n $ as our estimate, for small $ n $ we use $ \frac a{a+b} $ and in between we shrink our estimate between them.\par
What is the use of the posterior distribution? This opens us to decision theory.
\begin{enumerate}
	\item We must pick a decision $ \delta \in D $;
	\item We have a loss function $ L(\theta,\delta) $ which gives loss incurred in making decision $ \delta $ when the true paramter value is $ \theta $.
	\item Von-Neumann-Morgenstern Theorem: Under axioms of rational behaviour, pick $ \delta $ that minimises expected loss under posterior.
\end{enumerate}
\begin{definition}
	(Bayes estimator) The \textit{Bayes estimator} $ \hat\theta^{(b)} $ is defined by
	\[
	  h(\delta) = \int_\Theta L(\theta,\delta)\pi(\theta\mid X)\mathrm d\theta
	\]
and
\[
	\hat \theta^{(b)}= \mathrm{arg}\ \mathrm{min} h(\delta)
\]
\end{definition}
Consider the case where we have quadartic loss, so $ L(\theta,\delta) = (\theta-\delta)^2 $. Then we have that
\[
  h(\delta) =\int_\Theta (\theta-\delta)^2\pi(\theta\mid X)\mathrm d\theta.
\]
Differentiating with resepct to $ \delta $ we get that $ h'(\delta)=0 $ if
\[
  \int_\Theta (\theta-\delta)\pi(\theta\mid X)\mathrm d\theta = 0
\]
so
\[
  \delta = \int_\Theta \theta \pi(\theta\mid x)\mathrm d\theta
\]
is the posterior mean. Now suppose we have absolute loss, so $ L(\theta,\delta) = |\theta- \delta| $. So
\begin{align*}
	h(\delta) &= \int_\Theta |\theta-\delta|\pi(\theta\mid X)\mathrm d\theta\\
		  &= \int_{-\infty}^\delta -(\theta-\delta)\pi(\theta\mid X\mathrm d\theta + \int_\delta^\infty (\theta-\delta)\pi(\theta\mid X)\mathrm d\theta\\
		  &= -\int_\infty^\delta \theta\pi(\theta\mid X)\mathrm d\theta + \int_\delta^\infty \theta\pi(\theta \mid X)\mathrm d\theta + \delta\int_{-\infty}^\delta \pi(\theta\mid X)\mathrm d\theta - \delta\int_\delta^\infty \pi(\theta\mid X)\mathrm d\theta
\end{align*}
Taking derivatives and applying FTC we get that
\[
	h'(\delta) = \int_{-\infty}^\delta \pi(\theta\mid X)\mathrm d\theta - \int_{\delta}^\infty \pi(\theta\mid X)\mathrm d\theta.
\]
Hence $ h'(\delta) = 0 $ if and only if 
\[
	\int_{-\infty}^\delta \pi(\theta\mid X)\mathrm d\theta = \int_{\delta}^\infty \pi(\theta\mid X)\mathrm d\theta
\]
so $ \hat \theta^{(b)} $ is the posterior median.
\par
Supose we have $ X_1,\dots , X_n\iid \mathcal N(\mu, 1) $ and prior $ \pi(\mu) $ that is $ \mathcal N(0,\frac 1{\tau^2}) $ for some known $ \tau>0 $. Then
\begin{align*}
	\pi(\mu \mid X) &\propto f_X(x\mid \mu)\pi(\mu)\\
			&\propto \exp\left(-\frac 12 \sum_{i=1}^n (X_i-\mu)^2\right)\exp\left(-\frac{-\mu^2 \tau^2}{2}\right)\\
			&\propto \exp\left(-\frac 12 (n+\tau^2)\left(\mu - \frac{\sum X_i}{n+\tau^2}\right)^2\right)
\end{align*}
This is the pdf of a $\mathcal N\left(\frac{\sum X_I}{n+\tau^2}, \frac 1{n+\tau^2}\right) $ distribution. The posterior mean and median are both $ \frac{\sum X_i}{n+\tau^2} $.
\begin{definition}
	(Credible interval) A $ 100\gamma\% $ \textit{credible interval} satifies that
	\[
	  \pi(A(X)\le \theta\le B(X)\mid X=x) = \gamma.
	\]
\end{definition}
\section{Hypothesis Testing}
\begin{definition}
	(Hypothesis) A \textit{hypothesis} is an assumption about a distribution of data $ X $ taking values in $ \chi $.
\end{definition}
\begin{definition}
	(Null/Alternative hypothesis) The \textit{null hypothesis} $ H_0 $ is the base case. The \textit{alternative hypothesis} is the positive or negative effect the interesting case, denoted by $ H_1 $.
\end{definition}
For example let $ X_1,\dots, X_n\iid \mathrm{Bernoulli}(\theta) $. We may have the null hypothesis $ H_0:\theta=\frac 12 $ and then make alternative hypothesis $ H_1: \theta = \frac 34 $ or $ H_1:\theta \ne \frac 12 $ for example.\par
Suppose that $ X_1,\dots, X_n $ are iid. Then we have the hypotheses:
\begin{align*}
	H_0 &: X_i \ \text{has pdf}\ f_0 \\
	H_1 &: X_i \ \text{has pdf}\ f_1
\end{align*}
This is called a goodness of fit test.\par
Now suppose that $ X $ has pdf $ f(\cdot\mid \theta) $ for some $ \theta\in \Theta $. 
\begin{align*}
	H_0 &: \theta\in\Theta_0\nsubseteq \Theta\\
	H_1 &: \theta\notin \Theta_0
\end{align*}
\subsection{Simple hypotheses}
\begin{definition}
	(Simple/composite hypothesis) A \textit{simple hypothesis} fully specifies the distribution of $ X $. Otherwise we say the hypothesis is \textit{composite}.
\end{definition}
\begin{definition}
	(Test and critical regions) A \textit{test} of $ H_0 $ is defined by a \textit{critical region}, C. When $ X\in C $, we reject $ H_0 $, otherwise we do not reject $ H_1 $.
\end{definition}
\begin{definition}
	(Type I Error) A \textit{Type I Error} occurs when we reject $ H_0 $ when $ H_0 $ is true.
\end{definition}
\begin{definition}
	(Type II Error) A \textit{Type II Error} occurs when we fail to reject $ H_0 $ when $ H_1 $ is true.
\end{definition}
When $ H_0 $ and $ H_1 $ are simple hypotheses we have the following.
\begin{definition}
	(Size) We define $ \alpha $ as the \textit{size} of the test, defined as
	\[
		\alpha = \mathbb P_{H_0}(H_0\ \text{rejected}) = \mathbb P_{H_0}(X\in C).
	\]
\end{definition}
\begin{definition}
	(Power) We define the \textit{power} of the test as $ 1-\beta $ where
	\[
		\beta = \mathbb P_{H_1}(H_0\text{ not rejected}) = \mathbb P_{H_1}(X\notin C).
	\]
\end{definition}
\begin{remark}
  Note that $ \alpha $ is the probability of a Type I error and $ \beta $ is the probability of a Type II error.
\end{remark}
\begin{remark}
  Type I and Type II errors correspond to a false positive and a false negative respectively.
\end{remark}
Usually we set $ \alpha $ at an acceptable level for example $ 1\% $, and choose a test that minimises $ \beta $ subject to $ \alpha\le 1\% $.
\begin{definition}
	(Likelihood ratio statistic) Let $ H_0 $ and $ H_1 $ be simple hypotheses with $ X $ having pdf $ f_i $ under $ H_i $. The \textit{likelihood ratio statistic} is
	\[
		\Lambda_X(H_0,H_1) = \frac{f_1(X)}{f_0(X)}.
	\]
\end{definition}
\begin{definition}
	(Likelihood ratio test) A \textit{Likelihood ratio test} (LRT) rejects $ H_0 $ when $ X\in C=\{x\in \Lambda_X(H_0,H_1)>k\} $ for some $ k>0$ .
\end{definition}
\begin{theorem}
	(Neyman-Pearson Lemma) Suppose that $ f_0 $ and $ f_1 $ are nonzero on the same sets and $ \exists k$ such that the LRT with critical region $ C=\{x: \frac{f_1(x)}{f_0(x)}>k\} $ has size $ \alpha $. Out of all tests with size $ \le\alpha $ the LRT is the test with smallest $ \beta $.
\end{theorem}
\pf Let $ \overline C $ be the complement of $ C $. Then
\begin{align*}
	\alpha &= \mathbb P_{H_0}(X\in C) = \int_C f_0(x)\mathrm dx\\
	\beta &= \mathbb P_{H_1}(X\in \overline C) = \int_{\overline C}f_1(x)\mathrm dx
\end{align*}
Let $ C^* $ be the critical region of another test of size $ \alpha^*\le \alpha $. We want to show that $ \beta\le\beta^* $.
\begin{align*}
	\beta-\beta^*&=\int_{\overline C} f_1(x)\mathrm dx-\int_{\overline C^*}f_1(x)\mathrm dx\\
		     &= \int_{\overline C\cap \overline C^*} f_1(x)\mathrm dx-\int_{\overline C^*\cap C} f_1(x)\mathrm dx\\
		     &= \int_{\overline C \cap C^*}\frac{f_1(x)}{f_0(x)}f_0(x)\mathrm dx-\int_{\overline C^*\cap C}\frac{f_1(x)}{f_0(x)}f_0(x)\mathrm dx
\end{align*}
and the result follows from algebra manipulation.\qed
\begin{remark}
  A LRT with size $ \alpha $ for any given $ \alpha $ doesn't always exist. However we can always define a "randomised" test with exact level $ \alpha $.
\end{remark}
Suppose $ X_1,\dots, X_n \iid \mathcal N(\mu , \sigma^2_0$ where $ \sigma^2_0 $ is known. We want to find the best size $ \alpha $ test for
\begin{align*}
	H_0 &: \mu = \mu_0 \\
	H_1 &: \mu = \mu_1
\end{align*}
for some fixed $ \mu_1 > \mu_0 $. We have that
\begin{align*}
	\Lambda_X(H_0;H_1) &= \frac{(2\pi \sigma_0)^{-n/2} \exp\left(-\frac1{2\pi \sigma^2_0} \sum_{i=1}^n (X_i - \mu_1)^2\right)}{(2\pi \sigma_0)^{-n/2} \exp\left(-\frac1{2\pi \sigma^2_0} \sum_{i=1}^n (X_i - \mu_0)^2\right)}\\
			   &= \exp\left(\frac{\mu_1 - \mu_0}{\sigma^2_0} n\overline X + \frac{n(\mu_0^2 - \mu_1^2)}{2\sigma_0^2}.
\end{align*}
Since $ \Lambda_X(H_0;H_1) $ is monotone in $ \overline X $. We can depend our critical region for the LRT on $ \overline X $ equivalently. If we define
\[
	z = \frac{\sqrt n (\overline X - \mu_0)}{\sigma_0}
\]
the rejection region is now $ \{ z>c'\} $ for some $ c' $. Under $ H_0 $ we have that $ Z\sim \mathcal N(0,1) $ so the test that rejects $ H_0 $ when $ \{x:z>\inv \Phi(1-\alpha)\} $ has size $ \alpha $. This is called a $ z $-test.
\par
\begin{definition}
	($ p $-value) For any test with critical region of the form $ \{x: T(x) > k\} $ where $ T $ is some statistic, we usually report the $ p $-\textit{value} 
	\[
	p = \mathbb P_{H_0}(T(X)>T(X^*))
	\]
	where $ x^* $ is the observed data.
\end{definition}
This is the probability  of observing "more extreme" data under $ H_0 $.
\begin{proposition}
	Uner $ H_0 $ the $ p $-value is $ \mathrm{Uniform}[0,1]$.
\end{proposition}
\pf Let $ F $ be the distribution funciton of $ T $. Then
\begin{align*}
	\mathbb P_{H_0}(p<u) &= \mathbb P_{H_0}(1-F(T)<u)\\
			     &= \mathbb P_{H_0}(F(T) > 1 - u)\\
			     &= \mathbb P_{H_0}(T> F(1-t))\\
			     &=1 - F(\inv F(1 - u)) = u\qed
\end{align*}
\begin{definition}
	(Acceptance region) The \textit{acceptance region} of a test is the complement of the critical region.
\end{definition}
Let $ X\sim f_X(\cdot \mid \theta) $ for some $ \theta\in \Theta $.
\begin{theorem}
	\begin{enumerate}
		\item Suppose that for each $ \theta_0\in \Theta $ there exists a test of $ H_0:\theta = \theta_0 $ of size $ \alpha $ with acceptance region $ A(\theta_0) $. Then the set $ I(X) = \{ \theta: X\in A(\theta)\} $ is a $ 100(1-\alpha)\% $ confidence set.
		\item Suppose that $ I(X) $ is a $ 100(1-\alpha)\% $ confidence set for $ \theta $. Then 
			\[
				A(\theta_0) = \{x:\theta_0\in I(X)\}
			\]
			is the acceptance region of a size $ \alpha $ test for
			$ H_0:\theta = \theta_0 $ for each $ \theta \in \Theta $.
	\end{enumerate}
\end{theorem}
\pf In both cases $ \theta_0\in I(X)\iff X\in A(\theta_0) $.
\begin{enumerate}
\item We want to show that $ \mathbb P_{\theta_0}(\theta_0 \in I(X)) = 1-\alpha $.
	\begin{align*}
		\mathbb P_{\theta_0}(X\in A(\theta_0)) &= \mathbb P_{\theta_0}(\text{do not reject } H_0) = 1-\alpha
	\end{align*}
\item We want to show that $ \mathbb P_{\theta_0}(X\notin A(\theta_0)) = \alpha $.
	\begin{align*}
		\mathbb P_{\theta_0} (X\notin A(\theta_0)) &= \mathbb P_{\theta_0}(\theta_0 \notin I(X)) =\alpha.
	\end{align*}
\end{enumerate}
Hence we're done. \qed
\par
Suppose that $ X=(X_1,\dots, X_n)\iid \mathcal N(\mu ,\sigma_0^2) $ where $ \sigma_0 $ is known. We foudn a $ 100(1-\alpha)\% $ confidence interval for $ \mu $, namely, $ I(X)=\overline X \pm \frac{\sigma_0}{\sqrt n} $. Using the second part of the theorem, we can find a size $ \alpha $ test for $ H_0: \mu = \mu_0 $, by defining the acceptance region as
\begin{align*}
	A(\mu_0 \in I(x)_0) = \{x: \mu_0\in \left[\overline X - \frac{z_{\alpha/2} \sigma_0}{\sqrt n}, \overline X + \frac{z_{\alpha/2} \sigma_0}{\sqrt n}\right]\}.
\end{align*}
Equivalently we reject $ H_0 $ when
\[
	z_{\alpha/2} < \frac{\sqrt n |\mu_0 - \overline X|}{\sigma_0}.
\]
This is a two sided LRT. We could also equivalently go in the opposite direction.\par
\subsection{Composite hypotheses}
Previously we considered $ H_0 $ and $ H_1 $ as \textit{simple} hypotheses with error porbabilities
\[
\alpha = \mathbb P_{H_0}(X\in C), \qquad \beta = \mathbb P_{H_1}(X\notin C).
\]
Now we consider $ X\sim f_X(\cdot \mid \theta) $, with $ \theta\in\Theta $ and
\begin{align*}
	H_0 &: \theta\in\Theta_0 \subseteq \Theta\\
	H_1 &: \theta\in\Theta_1 \subseteq \Theta
\end{align*}
\begin{definition}
	(Power function) The \textit{power function} is $ W(\theta)= \mathbb P_\theta(X\in C) $.
\end{definition}
\begin{definition}
	(Size) The \textit{size} of a test with composite null $ H_0 $ is the worst-case Type I error probability, so
	\[
		\alpha = \sup_{\theta\in \Theta_0} W(\theta).
	\]
\end{definition}
\begin{definition}
	(Uniformly most powerful) We say that a test of $ H_0 $ against $ H_1 $ is \textit{uniformly most powerful} (UMP) of size $ \alpha $ if
	\begin{enumerate}
		\item $ \sup_{\theta\in\Theta_0} W(\theta)\le \alpha $;
		\item For any other test of size $ \alpha $, with power function $ W^* $ we have that
			\[
			  W(\theta) \ge W^*(\theta)\quad \forall\theta\in \Theta_1.
			\]
	\end{enumerate}
\end{definition}
\begin{remark}
  A UMP test might not exist. However many LRTs are UMP.
\end{remark}
Let's see an example for a one-sided test for a normal location. Suppose that $ X_1,\dots,X_n\iid \mathcal N(\mu,\sigma_0^2) $ with $ \sigma_0^2 $ known. We wish to test that
\begin{align*}
	H_0&: \mu \le \mu_0 \\
	H_1&: \mu > \mu_0.
\end{align*}
Recall that for the simple hypotheses,
\begin{align*}
	H_0'&: \mu = \mu_0 \\
	H_1'&: \mu = \mu_1,
\end{align*}
the LRT had critical region $ C : \left\{x: z = \frac{\sqrt n(\overline X - \mu_0)}{\sigma_0} > z_\alpha\right\} $. We will show that the same test is UMP for $ H_0 $ against $ H_1 $. The power function is
\begin{align*}
	W(\mu) &= \mathbb P_\mu(\text{reject} \ H_0)\\
	       &= \mathbb P_\mu\left(\frac{\sqrt n (\overline X - \mu_0)}{\sigma_0} < z_\alpha\right)\\
	       &= \mathbb P_\mu\left(\frac{\sqrt n(\overline X - \mu)}{\sigma_0} > z_\alpha + \frac{\sqrt n(\mu_0-\mu)}{\sigma_0}\right) \\
	       &= 1- \Phi\left(z_\alpha + \frac{\sqrt n (\mu_0-\mu)}{\sigma_0}\right)
\end{align*}
Thus $ \sup_{\mu\in \Theta_0}W(\mu) =\alpha $, so the test has size $ \alpha $. Now consider any other size $ \le \alpha $ with power function $ W^* $. We want to show that $ W(\mu_1) \ge W^*(\mu_1) $ for all $ \mu_1\in \Theta_1 $. Any other test of size $ \le \alpha $ also has size $ \le \alpha $ for $ H_0' $ against $ H_1 $ since 
\[
	W^*(\mu_0)\le \sup_{\mu \in\Theta_0} W^*(\mu) \le \alpha.
\]
Thus by Neyman-Pearson, $ W(\mu_1) \ge W^*(\mu_1) $. Since this argument works for any $ \mu_1>\mu_0 $ we are done.
\subsubsection{Generalised likelihood ratio tests}
The generalised likelihood ratio (GLR) statistic for
\begin{align*}
	H_0 &: \theta \in \Theta_0 \subseteq \Theta\\
	H_1 &: \theta \in \Theta_1 \subseteq \Theta
\end{align*}
as
\[
	\Lambda_X(H_0;H_1) = \frac{\sup_{\theta\in\Theta_1} f_X(\cdot \mid \theta)}{\sup_{\theta\in \Theta_0}f_X(\cdot\mid \theta)}.
\]
We reject $ H_0 $ when $ \Lambda_X $ is large.\par
Suppose that $ X_1,\dots, X_n\idd \mathcal N(\mu,\sigma^2_0) $ with $ \sigma_0^2 $ known. We wish to test
\begin{align*}
	H_0&: \mu = \mu_0\\
	H_1&: \mu \ne \mu_0.
\end{align*}
Thus, $ \Theta_0 = \{\mu_0\} $ and $ \Theta_1 = \mathbb R \setminus \{\mu_0\} $, and the GLR statistic is
\begin{align*}
	\Lambda_X(H_0;H_1) = \frac{(2\pi \sigma_0^2)^{-n/2}\exp\left(-\frac 1{2\sigma_0^2} \sum(X_i - \overline X)^2\right)}{(2\pi \sigma_0^2)^{-n/2} \exp\left(-\frac 1{2\sigma_0^2}\sum(X_i - \mu_0)^2\right)}
\end{align*}
and after simplication we get that
\[
	2\log \Lambda_X(H_0;H_1) = \frac 1{\sigma_0^2} \left(\sum(X_i-\mu_0)^2 - \sum(X_i-\overline X)^2\right) = \frac n {\sigma^2_0} (\overline X - \mu_0)^2.
\]
Thus GLRT rejects $ H_0 $ if $ \frac{\sqrt n |\overline X - \mu_0|}{\sigma_0} $ is large. Under $ H_0 $, $ \frac{\sqrt n(\overline X - \mu_0)}{\sigma_0} \sim\mathcal N(0,1) $ so a test of size $ \alpha $ rejects $ H_0 $ if $ \frac{\sqrt n|\overline X - \mu|}{\sigma_0}>z_{\alpha/2} $.
\par
Note that in this example $ 2\log \Lambda_X(H_0;H_1) = \frac{n(\overline X-\mu_0)^2}{\sigma^2_0}\sim \chi^2_1 $. Thus the critical region of t he GLRT can also be written as $ \left\{x:\frac{\sqrt n(\overline X - \mu_0)^2}{\sigma_0^2} > \chi_1^2(\alpha)\right\} $. In a fact a more general result says that $ 2\log\Lambda_X(H_0;H_1) \approx \chi^2 $ when $ n $ is large.
\begin{theorem}
	(Wilks' theorem) Suppose the parameter $ \theta $ is $ k $-dimensional so $ \theta = (\theta_1,\dots, \theta_k) $. The \textit{dimension} of a hypothesis $ H_0:\theta\in\Theta_0 $ is the number of "free parameters in $ \Theta_0 $ for example
	\begin{enumerate}
		\item If $ \Theta_0= \{\theta\in \R^k:\theta_1=\theta_2=\dots = \theta_p = 0\} $ then $ \dim(\Theta_0) = k-p $.
		\item Suppose $ \Theta_0 = \{\theta\in\R^k: \theta_i = f_i(\phi),\ \forall 1\le i \le k, $ for some $ \phi\in\R^{k-p}\} $. Then $ \dim(\Theta_0) = k-p $.
	\end{enumerate}
\end{theorem}
\pf Not included.
\begin{theorem}
  Suppose that $ \Theta_0\subseteq \Theta_1 $ and $ \dim(\Theta_1)-\dim(\Theta_0) = p $. If $ (X_1,\dots, X_n) $ are idd then under regularity conditions, as $ n\to\infty $ the limiting distribution of $ 2\log\Lambda_X(H_0;H_1) $ is $ \chi_p^2 $. Thus if we reject $ H_0 $ when $ 2\log\Lambda_X(H_0;H_1)\ge \chi_p^2(\alpha) $ and we have a test of size $ \approx \alpha $.
\end{theorem}
\begin{example}
  In the two-sided normal mean test, we had
  \begin{align*}
	  \Theta_0 &= \{\mu_0\}\\
	  \Theta_1 &= \R\setminus \{\mu_0\}
  \end{align*}
  and we saw that
  \[
    2\log\Lambda_X(H_0;H_1) \sim \chi_1^2
  \]
  exactly. In a different parametric family, with large $ n $ this would hold aproximately.
\end{example}
\subsection{Goodness-of-fit tests}
\begin{example}
  In one of this experiments, Mendel crossed 556 smooth, yellow, male peas with wrinkled green female peas. He obtained a table of data of the phenotypes of the produced peas. Is there evidence in his data to reject the hypothesis that the genetic theory is correct?\par
  Suppose $ X_1,\dots, X_n $ are iid samples from a distribution on $ \{ 1,\dots, k\ $. Let $ p_i = \mathbb P(X_1 = i) $ and let $ N_i $ be the number of observations in $ \{X_1,\dots, X_n\} $ of type $ i $. Hence $ \sum p_i = 1 $ and $ \sum N_i = n $. Then we have that
	  \[
		  (N_1, \dots, N_k) \sim \mathrm{Mult}(n; p_1,p_2,\dots, p_n).
	  \]
	  The likelihood is $ L(p)\propto p_1^{N_1}p_2^{N_2}\dots p_k^{N_k} $, so
	  \[
		  \ell(p) = \log(L(p)) = \text{const.} + \sum_{i=1}^k N_i\log p_i.
	  \]
	  We can test $ H_0 $ against $ H_1 $ using a GLRT
	  \[
		  2\log\Lambda = 2\left(\sup_{p\in\Theta_1} \ell(p) - \sup_{p\in \Theta_0} \ell(p)\right).
	  \]
	  Notice  that
	  \[
		  \sup_{p\in \Theta_1} \ell(p) = \sup_{p:\sum p_i = 1} \sum_i N_i \log p_i.
	  \]
	  Using the Lagrangian and calculating we get that the MLE is giving by
	  \[
		  \hat p = \hat p_i = \frac{N_i}n.
	  \]
	  So 
	  \[
		  2\log\Lambda = 2\sum_{i=1}^k N_i \log\left(\frac {N_i}{n\tilde p_i}\right).
	  \]
	  Wilks' theorem says that 
	  $ 2\log \Lambda\approx \chi_p^2 $ with $ p=\dim(\Theta_1)-\dim(\Theta_0) = k-1 - 0 = k-1 $. Thus reject $ H_0 $ if $ 2\log \Lambda\ge \chi){k-1}^2(\alpha) $. It is common to write
	  \[
		  2\log(\Lambda) = 2\sum_i o_i \log\left(\frac{o_i}{e_i}\right)
	  \]
	  where $ o_i $ is $ N_i $ and $ e_i $ is the expected number of type $ i $ equal to $ i = n\tilde p_i $.\par
	  Let $ \delta_i = o_i - e_i $. Then 
	  \[
		  2\log\Lambda \approx \sum_i \frac{\delta_i^2}{e_i} = \sum_i \frac{(o_i-e_i)^2}{e_i}
	  \]
	  which is Pearson's chi-square statistic.
	\end{example}
	Suppose we wish to test
	\begin{align*}
		H_0 &: p_1 =\theta^2,\  p_2=2\theta(1-\theta),\ p_3= (1-\theta)^2,\ \theta\in[0,1].
	\end{align*}
	Then the GLRT is
	\begin{align*}
		2\log \Lambda&= 2\left(\sup_{p:\sum p_i = 1} \ell(p) - \sup_\theta \ell(p(\theta))\right) \\
			     &= 2(\ell(\hat p) - \ell(p(\hat \theta))) = 2\sum N_i \log\left(\frac {N_i}{n p_i(\hat\theta)}\right)\\
			     &= 2\sum_i o_i\log\left(\frac{o_i}{e_i}\right).
	\end{align*}
\subsection{Contingency tables}
\subsubsection{Testing independence}
Suppose we have $ (X_1,Y_1),\dots, (X_n,Y_n) $ are iid with $ X_i $ taking values in $ \{1,\dots, r\} $ and $ Y_i $ taking values on $ \{1,\dots, c\} $. We wish to test the null hypothesis that $ X_i $ and $ Y_i $ are independent. The entries in a contingency table are 
\[
	N_{ij} = # \{ \ell : 1\le \ell \le n; (X_\ell,Y_\ell) = (i,j)\}.
\]
\begin{example}
  Part III admissions statistics from a recent year, by gender/stream
  \begin{center}
	  \begin{tabular}{c |c c c}
      \hline
      Stream & F & M & Other \\
      \hline
      MASA & 10 & 9 & 1 \\
      MASTH & 11 & 59 & 0 \\
      MASP & 10 & 56 & 2 \\
      MASS & 6 &23 & 0 \\
      MMATH & 13 & 87 & 1 \\
      \hline 
    \end{tabular}
  \end{center}
  We wish to test the null hypothesiss that gender and stream are independent. Observe $ n $ examples (fixed, given). A sample is of type $ (i,j) $ with probability $ p_{ij} $. Then $ (N_{11},N_{12},\dots, N_{21}, \dots, N_{rc})\sim \mathrm{Mult}(n,p_{11},p_{12},\dots, p_{rc}) $. Then let $ p_{i+} = \sum p_{ij} $ and $ p_{+j} = \sum_i p_{ij} $ so
  \begin{align*}
	  H_0 &: p_{ij} = p_{i+}p_{+j}\\
	  H_1 &: \{p_{ij}\} \ \text{unconstrained}.
  \end{align*}
  Using Lagrangian methods, the MLEs are $ \hat p_{ij} = \frac{N_{ij}}n $ under $ H_1 $ and $ (\hat p_{i+}, \hat p_{+j} ) = \left(\frac{N_{i+}}n, \frac{N_{+j}}n\right)$ under $ H_0 $. Then the GLR statistic is
  \[
	  2\log \Lambda = 2\sum_{i=1}^r\sum_{j=1}^c N_{ij}\log\left(\frac{\hat p_{ij}}{\hat p_{i+}\hat p_{+j}}\right).
  \]
  Writing $ o_{ij} =N_{ij} $ and $ e_{ij} = h\hat p_{i+}\hat p_{+j} $ we have that
  \[
	  2\log\Lambda = 2\sum_i \sum_j o_{ij} \log\left(\frac{o_{ij}}{e_{ij}}\right)\approx \sum_i \sum_j \frac{(o_{ij}-e_{ij})^2}{e_{ij}}
  \]
  which converges to $ \chi^2 $ by Wilks' theorem. The number of degrees of freedom is $ p = \dim(\Theta_1) - \dim(\Theta_0) = (rc -1 ) - ((r-1)+(c-1)) = (r-1)(c-1) $.
  \par
  Applying this to our sample we get that
  \[
	  \sum_i \sum_j \frac{(o_{ij} - e_{ij})^2}{e_{ij}} \approx 22.2.
  \]
  Hence our $ p $-value is $ \mathbb P(\chi_8^2 \ge 22.2) \approx 0.005 $, so reject the null hypothesis of indepedence.
\end{example}
\begin{example}
  MMATH admissions statistic by gender/year
  \begin{center}
	  \begin{tabular}{c | c c c}
		  \hline
		   & F & M  & Other \\
		  \hline
		  Year 1 & 13 & 87 & 1 \\
		  Year 2 & 10 & 81 & 0 \\
		  Year 3 & 15 & 87 & 4 \\
		  Year 4 & 9 & 103 & 0 \\
		  Year 5 & 16 & 126 & 0 \\
\hline
	  \end{tabular}
  \end{center}
  If we have hypotheses
  \begin{align*}
	  H_0&: \text{gender and year and independent} \\
	  H_1&: \text{gender and year are not independent}
  \end{align*}
  then our test statistic is
  \[
	  \sum_i \sum_j \frac{(o_{ij} - e_{ij})^2}{e_{ij}} \approx 15.5.
  \]
  And our $ p $-value is $ \mathbb P(\chi^2_8\ge 15.5) = 0.051 $ so we do not reject $ H_0 $ at the $ 5\% $ significance level.
\end{example}
\subsubsection{Tests of homogeneity}
\begin{example}
  Suppose that $ 150 $ patients are randomly allocated to three groups of equal size. Two sets of patients were given a drug of two different doses. The third group were given a placebo. This is different from the last setting since the row totals are fixed. We want to test the null hypothesis of "homogeneity" which corresponds to the probability of response being unchanged between groups.\par
  Our probability model is $ N_{i1},\dots, N_{ic}\sim \text{Mult}(n_{i+}; p_{i1},\dots, p_{ic}) $ independently for each $ 1\le i \le r $ where $ n_{i+} $'s are fixed row totals.
  \begin{align*}
	  H_0 &: p_{ij} = p_{2j} = \dots = p_{rj} \\
	  H_1 &: (p_{i1},\dots, p_{ic}) \text{ is an arbitrary probability distribution}
  \end{align*}
  Under $ H_1 $,
  \begin{align*}
  L(p) = \prod_{i=1}^r \frac{n_{i+}!}{N_{i1}!\dots N_{ic}!}p_{i1}^{N_{i1}}\dots p_{ic}^{N_{ic}}
  \end{align*}
  and
  \[
	  \ell(p) = \log L(p) = \text{const.} + \sum_{i,j} N_{ij}\log p_{ij}.
  \]
  Ynder Lagrangian methods with constraints $ \sum_j p_{ij} = 1 $ we find the MLE
  \[
	  \hat p_{ij} = \frac{N_{ij}}{n_{i+}}.
  \]
  Under $ H_0 $ let $ p_{ij} = p_j $ for all $ i $. Then
  \[
	  \ell(p) = \text{const.} + \sum_{i,j} N_{ij}\log p_j= \text{const.} + \sum_{j=1}^c N_{+j} \log p_j.
  \]
  Using the Lagrangian method with constraint $ \sum_j p_j = 1 $ we have the MLE $ \hat p_j = \frac{N_{+j}}{n_{++}} $. Hence
  \[
	  2\log\Lambda = 2\sum_{i=1}^r \sum_{j=1}^c N_{ij} \log\left(\frac{\hat p_{ij}}{\hat p_j}\right) = 2\sum_{i,j} N_{ij} \log\left(\frac{N_{ij}}{n_{i+}n_{+j}/n_{++}}\right)
  \]
If we define $ o_{ij} = N_{ij} $ and $ e_{ij} = \frac{n_{i+}N_{+j}}{n_{++}} $, then
\[
	2\log\Lambda \approx \sum_{i,j}\frac{(o_{ij}-e_{ij})^2}{e_{ij}}.
\]
And Wilks' theorem states that asympototically the distribution of $ 2\log\Lambda $ is $ \chi^2_p $ with 
\[
  p = \dim(\Theta_1) - \dim(\Theta_0) = r(c-1) - (c-1) = (r-1)(c-1).
\]
\begin{remark}
  The number of degrees of freedom is the same for both tests of homogeneity and independence.
\end{remark}
\end{example}
\subsection{Multivariate normal theory}
Let $ X = \begin{pmatrix}
   X_1 \\
   \vdots \\
   X_n 
\end{pmatrix} $ be a vector of random variables. Recall that
\[
  \mathbb E(X) = \begin{pmatrix}
    \mathbb E(X_1) \\
    \vdots \\
    \mathbb E(X_n)
\end{pmatrix}\quad \text{and}\quad \Cov{X} = \mathbb E[(X-\mathbb E(X))(X-E(X))^T]=(\Cov{X_i,X_j})_{ij}.
\]
Furthermore if $ A\in \R^{m\times n} $ and $ b\in \R^m $,
\[
	\mathbb E(AX+b) = A\mathbb E(X) + b\quad \text{and} \quad \Cov{AX+b} = A\Cov{X}A^T.
\]
\begin{definition}
	(Multivariate normal) We say that $ X $ has a multivariate normal distribution if for any $ t\in \R^n $ the random variable $ t^TX $ has a normal distribution.
\end{definition}
\begin{proposition}
  If $ X $ is multivariate normal then $ AX+b $ is multivariate normal.
\end{proposition}
\pf Take any $ t\in \R^m $. Them $ t^T(AX+b) = (A^T t)^TX + t^Tb $. Since $ X $ is multivariate normal, $ (A^Tt)^TX \sim\mathcal N(\mu,\sigma^2) $ for some $ (\mu,\sigma^2) $ then $ t^T(AX+b) \sim \mathcal N(\mu+t^Tb,\sigma^2) $.\qed











\end{document}
