\documentclass{article}
\usepackage{../header}
\title{Statistics}
\author{Notes by Finley Cooper}
\begin{document}
  \maketitle
  \newpage
  \tableofcontents
  \newpage
  \section{Parametric Estimation}
  \subsection{Review of IA Probability}
  \subsubsection{Starting axioms}
  We observe some data $ X_1,\dots, X_n $ iid random variables taking values in a sample space $ \mathcal X $. Let $ X=(X_1,\dots, X_n) $. We assume that $ X_1 $ belongs to a \textit{statistical model} $ \{p(x; \theta):\theta \in \Theta\} $ with $ \theta $ unknown. For example $ p(x;\theta) $ could be a pdf.\par
  Let's see some examples
  \begin{enumerate}
    
	  \item Suppose that $ X_1\sim \text{Poisson}(\lambda) $ where $ \theta=\lambda\in\Theta=(0,\infty) $.

  \item Suppose that $ X_1\sim\mathcal N(\mu,\sigma^2) $, where $ \theta=(\mu,\sigma^2)\in\Theta=\R\times(0,\infty) $.
\end{enumerate}
We have some common questions about these statistical models.
\begin{enumerate}
	\item We want to give an estimate $ \hat{\theta}:\mathcal X^n\to \Theta $ of the true value of $ \theta $.
	\item We also want to give an interval estimator $ (\hat\theta_1(X),\hat\theta_2(X)) $ of $ \theta $.
	\item Further we want to test of hypothesis about $ \theta $. For example we might make the hypothesis that $ H_0:\theta=0 $.
\end{enumerate}
Let's do a quick reivew of IA Probability. Let $ X:\Omega\to \R $ be a random variable defined on the probability space $ (\Omega,\mathcal F, \mathbb P) $. So $ \Omega $ is the sample space, $ \mathcal F $ is the set of events, and $ \mathbb P: \mathcal F\to [0,1] $ is the probability measure.\par
The cumulative distribution function (cdf) of $ X $ is $ F_X(s)=\prob{X\le x} $. A discrete random variable takes values in a countable set $ \mathcal X $ and has probability mass function (pmf) given by $ p_X(x)=\prob{X=x} $. A continuous random variable has probability density function (pdf) $ f_X $ satisfying $ P(X\in A)=\int_A f_X(x)\mathrm dx $ (for measurable sets $ A $). We say that $ X_1,\dots, X_n $ are independent if $ \prob{X_1\le x_1,\dots, X_n\le x_n}=\prod_{i=1}^n \prob{X_i\le x_i} $ for all choices $ x_1,\dots, x_n $. If $ X_1,\dots, X_n $ have pdfs (or pmfs) $ f_{X_1},\dots, f_{X_n} $, then this is equivalent to $ f_X(x)=\prod_{i=1}^nf_{X_i}(x_i) $ for all $ x_i $. The expectation of $ X $ is,
\[
  \mathbb E(x)=\begin{cases}
	  \sum_{x\in \mathcal X}xp_X(x) & \text{if } X\text{ is discrete}\\
	  \int_{-\infty}^\infty xf_X(x) & \text{if } X\text{ is continuous}
  \end{cases}.
\]
The variance of $ X $ is $ \Var{X}=\mathbb E[(X-\mathbb E(X))^2]$. The moment generating function of $ X $ is $ M(t)=\mathbb E[e^{tX}] $ and can be used to generate the momentum of a random varaible by taking derivatives. If two random variables have the same moment generating functions, then they have the same distribution.\par
The expectation operator is linear and
\[
	\Var{a_1X_1+\dots+a_nX_n}=\sum_{i,j=1}^n a_ia_j\Cov{X_i,X_j},
\]
where $ \Cov{X_i,X_j}=\mathbb E[(X_i-\mathbb E(X_i)(X_j-\mathbb E(X_j))] $. In vector notation writing $ X $ as the column vector of $ X_i $ and $ a $ as the column vector for $ a_i $ we get that 
\[
	\mathbb E[a^T X]=a^T E[X].
\]
Similar for the variance we get that
\[
	\Var{a^TX}=a^T\Var X a
\]
where $ \Var X $ is the covariance matrix for $ X $ with entries $ \Cov{X_i,X_j} $.
\subsubsection{Joint random variables}
If $ X $ is a discrete random variable with pmf $ P_{X,Y}(x,y)=\prob{X=x, Y=y} $ and marginal pmf $ P_Y(y)=\sum_{x\in X}P_{X,Y}(x,y) $, then the conditional pmf is
\[
	P_{X\mid Y}(x\mid y)=\prob{X=x\mid Y=y}=\frac{P_{X,Y}(x,y)}{P_Y(y)}.
\]
If $ X,Y $ are continuous then the join pdf $ f_{X,Y} $ satifies
\[
	\prob{X=x,Y=y} = \int_{-\infty}^x\int_{-\infty}^y f_{X,Y}\mathrm dx\mathrm dy
\]
and the marginal pdf of $ Y $ is
\[
	f_Y(y)=\int_{-\infty}^\infty f_{X,Y}(x,y)\mathrm dx.
\]
The \textit{conditional pdf} of $ X $ given $ Y $ is $ f_{X\mid Y}(x\mid y)=\frac{f_{X,Y}(x,y)}{f_Y(y)} $.\par
The conditional expectation of $ X $ given $ Y $ is
\[
E(X\mid Y) = \begin{cases}
	\sum_{x\in X} x\mathbb P_{X|Y}(x\mid Y) & \text{if } X \text{ is discrete} \\
	\int_{-\infty}^\infty xf_{X\mid Y}(x\mid Y)\mathrm dy & \text{if } Y \text{ is continuous}
\end{cases}.
\]
\begin{remark}
$ \mathbb E(X\mid Y) $ is a function of $ Y $ so $ \mathbb E(X\mid Y) $ is a random variable.
\end{remark}
We also have the law of total expectation,
\[
	\mathbb E[X]=\mathbb E[\mathbb E[X\mid Y]].
\]
This is a consequence of the law of total probability which is
\[
	p_X(x)=\sum_y p_{X\mid Y}(x\mid y)p_Y(y).
\]
Now we have a new (but less useful) theorem similar to the tower property of expectation.
\begin{theorem}
	(Law of total variance)
	\[
		\Var{X}=\mathbb E[\Var{X\mid Y}] +\Var{\mathbb E[X\mid Y]}.
	\]
\end{theorem}
\pf Write $ \Var{X} = \mathbb E[X^2]-(\mathbb E[X])^2 $, so
\begin{align*}
	\Var{X} &= \mathbb E(\mathbb E(X^2\mid Y) - (\mathbb E(\mathbb E(X\mid Y)))^2 \\
		&= \mathbb E[\mathbb E(X^2\mid Y)-(\mathbb E(X\mid Y))^2] + \mathbb E((\mathbb E(X\mid Y))^2) - (\mathbb E(\mathbb E(X\mid Y)))^2\\
		&= \mathbb E[\Var{X\mid Y}] +\Var{\mathbb E[X\mid Y]}.\qed
\end{align*}
We also have the change of variables formula. If we have a mapping $ (x,y)\to (u,v) $, a bijection from $ \R^2\to \R^2 $, then
\[
	f_{U,V}(u,v) = f_{X,Y}(x(u,v),y(u,v))|\det J|,
\]
where $ J $ is the Jacobian matrix.
\subsubsection{Limit theorems}
Suppose $ X_1,\dots, X_n $ are iid random variables with mean $ \mu $ and variance $ \sigma^2 $. Define the sum $ S=\sum_{i=1}^n X_i $ and the sample mean $ \bar X_n = \frac{S_n}n $. We have the following theorems.
\begin{theorem}
	(Weak Law of Large Numbers)
	\[
	  \bar X_n\to \mu
	\]
	where $ \to $ means that $ \prob{|\bar X_n-\mu|>\varepsilon}\to 0 $ as $ n\to \infty $ for all $ \varepsilon>0 $.
\end{theorem}
\begin{theorem}
	(Strong Law of Large Numbers)
	\[ \bar X_n\to \mu \]
	almost surely. So $ \prob{\lim_{n\to\infty}\bar X_n=\mu}=1 $.
\end{theorem}
\begin{theorem}
	(Central Limit Theorem)
	The random variables
	\[
		Z_n = \frac{S_n-n\mu}{\sigma\sqrt n}
	\]
	is approximately $ \mathcal N(0,1) $ for large $ n $. Or we can write this as
	\[
	  S_n\approx \mathcal N(n\mu, n\sigma^2).
	\]
	Formally this means that $ \prob{Z_n\le z}\to \Phi(z) $ for all $ z\in \R $ where $ \Phi(z) $ is the cdf of $ \mathcal N(0,1) $.
\end{theorem}
\subsection{Estimators}
Suppose that $ X_1,\dots, X_n $ are iid with pdf $ f_X(x\mid \theta) $ and parameter $ \theta $ unknown.
\begin{definition}
	(Estimator) A function of the data $ T(X)\to\hat\theta $ which is used to approximate the true parameter $ \theta $ is called an \textit{estimator} (or sometimes a \textit{statistic}). The distribution of $ T(X) $ is the \textit{sampling distribution}
\end{definition}
For an example suppose that $ X_1,\dots, X_n\sim\mathcal N(\mu,1) $ and let $ \hat \mu = T(x) = \frac 1n \sum_{i=1}^n X_i $. The sampling distribution of $ \hat \mu $ is $ T(X)\sim \mathcal N(\mu, \frac 1{n}) $.
\begin{definition}
	(Bias) The \textit{bias} of a random variable $ \hat\theta=T(X) $ is 
	\[
		\mathrm{bias}(\hat\theta) = \mathbb E_\theta(\hat\theta)-\theta,
	\]
	where the expectation is taken over the model $ X_1\sim f_X(\cdot \mid \theta) $.
\end{definition}
\begin{remark}
 In general the bias might be a function of $ \theta $ which is not explicit in the notation.
\end{remark}
\begin{definition}
	(Unbiased estimator) We say that an estimator is \textit{unbiased} if $ \mathrm{bias}(\hat\theta)=0 $ for all $ \theta\in\Theta $.
\end{definition}
So for our estimator from before, $ \hat\mu $, is unbiased since
\[
	\mathbb E_\mu(\hat \mu) = \frac 1n\sum_{i=1}^n \mathbb E_\mu(X_i) = \mu.
\]
\subsubsection{Bias-variance decomposition}
\begin{definition}
	(Mean squared error) The \textit{mean squared error} of an estimator $ \hat\theta $ is
	\[
		\mathrm{mse}(\hat\theta) = \mathbb E_\theta[(\hat\theta-\theta)^2].
	\]
\end{definition}
\begin{remark}
  Note that the MSE is generally a function of $ \theta $ like the bias. Again this is not clear from the notation.
\end{remark}
\begin{proposition}
	(Bias-variance decomposition) For an estimator $ \hat\theta $ of a parameter $ \theta $, we have that
	\[
		\mathrm{mse}(\hat\theta)=\left(\mathrm{bias}(\hat\theta)\right)^2 + \mathrm{Var}_\theta(\hat\theta).
	\]
\end{proposition}
\pf 
\begin{align*}
	\mathrm{mse}(\hat\theta) &= \mathbb E_\theta[(\hat\theta-\theta)^2] \\
				 &= \mathbb E_\theta\left[\left(\hat\theta-\mathbb E_\theta(\hat\theta)+\mathbb E_\theta(\hat\theta)-\theta\right)^2\right]\\
				 &= \mathbb E_\theta[(\hat\theta -\mathbb E_\theta(\hat\theta))^2]+(\mathbb E_\theta(\hat\theta)-\theta)^2 + 2(\mathbb E_\theta(\hat\theta) - \theta) \cdot \mathbb E_\theta[\hat\theta - \mathbb E_\theta(\hat\theta)]\\
				 &= \left(\mathrm{bias}(\hat\theta)\right)^2 + \mathrm{Var}_\theta(\hat\theta).\qed
\end{align*}
Let's see an example. Suppose that $ X\sim \text{Binomial}(n,\theta) $ where is $ n $ is known and we want to estimate $ \theta\in[0,1] $. Let $ T_u=\frac{X}n $ be an estimator, so $ \mathbb E_\theta(T_u)=\frac{\mathbb E(X)}n = \frac{n\theta}n = \theta $, hence this estimator is unbiased. And $ \mathrm{mse}(T_u) = \Var{T_u} +\mathrm{bias}(T_u) = \frac{\theta(1-\theta)}n $.\par
Instead if we used the estimator $ T_b = \frac{X+1}{n+2}  = \omega\frac Xn + (1-\omega)\frac 12 $ where $ \omega = \frac n{n+2} $. We get that
\begin{align*}
	\mathrm{bias}(T_b) &= (1-\omega)(\frac 12 - \theta)\\
	\Var{T_b} &= \omega^2\frac{\theta(1-\theta)}n.
\end{align*}
Giving that
\[
	\mathrm {mse}(T_b) &= \omega^2 \zfrac{\theta(1-\theta)n} + (1-\omega)^2(\frac 12 - \theta)^2
\]
\subsection{Sufficient statistics}
Suppose $ X_1,\dots, X_n $ are iid random variables taking values in $ \chi $ with pdf $ f_{X_1}(\cdot\mid \theta) $. Consider $ \theta $ as fixed. Denote $ X=(X_1,\dots, X_n) $.
\begin{definition}
(Sufficient statistics) A statistics $ T $ is \textit{sufficient} for $ \theta $ if the conditional distribution of $ X$ given $ T(X) $ does not depend on $ \theta $.
\end{definition}
\begin{remark}
  The parameter $ \theta $ may be a vector, and $ T(X) $ may be a vector.
\end{remark}
Suppose $ X_1,\dots, X_n \sim \mathrm{Binomial}(1,\theta) $ iid for some $ \theta\in [0,1] $. Then 
\begin{align*}
	f_X(x\mid \theta)&=\prod_{i=1}^n \theta^{x_i}(1-\theta)^{1-x_i}\\
			 &= \theta^{\sum x_i}(1-\theta)^{n-\sum x_i}
\end{align*}
Define $ T(X)=\sum_{i=1}^n x_i $. Now 
\begin{align*}
	f_{X\mid T =t}(x\mid T(x)=t)&=\frac{\mathbb P_\theta(X=x,T(X)=t)}{\mathbb P_\theta(T(X) =t)}\\
				    &= \frac{\mathbb P_\theta(X=x)}{\mathbb P_\theta(T(X)=t)} = \frac{\theta^{\sum x_i}(1-\theta)^{n-\sum x_i}}{\binom nt \theta^t (1-\theta)^{n-t}} = \frac 1{\binom nt}.
\end{align*}
\begin{theorem}
	(Factorisation criterion) The statistics $ T $ is sufficient for $ \theta $ if and only if $ f_X(x\mid \theta)= g(T(x),\theta)h(x) $ for some suitable $ g $ and $ h $.
\end{theorem}
\pf Suppose that $ f_X(x\mid \theta) = g(T(x),\theta)h(x) $. We can compute
\begin{align*}
	f_{X\mid T=t}(x\mid T=t) &= \frac{\mathbb P_\theta(X=x,T(x) =t)}{\mathbb P_\theta(T(x)=t)}\\
				 &= \frac{g(T(x),\theta)h(x)}{\sum_{x';T(x')=t}g(t,\theta)h(x')}\\
				 &= \frac{h(x)}{\sum_{x';T(x')=t}h(x')}
\end{align*}
which doesn't depend on $ \theta $, so $ T(X) $ is sufficient.\par
Conversely, suppose $ T(X) $ is sufficient. We can write
\begin{align*}
\mathbb P_\theta(X=x) &= \mathbb P_\theta(X=x,T(X)=T(x))\\
		      &= \mathbb P_\theta(X=x\mid T(X)=T(x))\prob_\theta(T(X)=T(x))\\
		      &= h(x)g(T(X),\theta).
\end{align*}
So we're done.\qed
\begin{remark}
	For our example before we can define $ T(x) = \sum x_i $ and $ g(t,\theta)  = \theta^t (1-\theta)^{n-t} $ and $ h(x) = 1 $.
\end{remark}
Let's see another example. Let $ X_1,\dots,X_n $ be iid uniform on $ [0,\theta] $ for some $ \theta\in (0,\infty) $. So
\begin{align*}
	f_X(x=\theta)&=\prod_{i=1}^n \frac 1\theta \mathbf 1\{x_i\in [0,\infty]\}\\
		     &=\frac 1{\theta^n} \mathbf 1\{\max x_i\le \theta\}\mathbf 1\{\min x_i\ge 0 \}\\
		     &= g(T(x),\theta)h(x).
\end{align*}
\subsection{Minimal sufficiency} 
\begin{definition}
	(Minimal sufficient) A sufficient statistics $ T(X) $ is \textit{minimal sufficient} if it is a function of every other sufficient statistic. So if $ T'(X) $ is also sufficient, then $ T'(x)=T'(y)\implies T(x)=T(y) $ for all $ x,y\in \chi $.
\end{definition}
\begin{remark}
  Minimal sufficient statistics are unique up to bijection. 
\end{remark}
\begin{theorem}
	Suppose $ T(X) $ is a statistics such that $ \frac{f_X(x\mid \theta)}{f_X(y\mid \theta)} $ is constant a function of $ \theta $ if and only if $ T(x)=T(y) $. Then $ T $ is minimal sufficient.
\end{theorem}
Let's see an example before we prove this. Suppose that $ X_1,\dots, X_n\sim\mathcal N(\mu,\sigma^2) $. Then
\begin{align*}
	\frac{f_X(x\mid \mu,\sigma^2)}{f_X(y\mid \mu,\sigma^2)} &=\frac{(2\pi \sigma^2)^{-n/2}\exp\left(-\frac 1{2\sigma^2}\sum(x_i-\mu)^2\right)}{(2\pi \sigma^2)^{-n/2}\exp\left(-\frac 1{2\sigma^2}\sum(y_i-\mu)^2\right)}\\
								&=\exp\left(-\frac{1}{2\sigma^2}\left(\sum_i x_i^2 - \sum_i y_i^2\right)+\frac \mu{\sigma^2}\left(\sum_i x_i-\sum_i y_i\right)\right)
\end{align*}
This is constant in $ (\mu,\sigma^2) $ if and only if $ \sum_i x_i=\sum_i y_i $ and $ \sum_i x_i^2 = \sum_i y_i^2 $ therefore $ T(X)=\left(\sum_{i=1}^n X_i,\sum_{i=1}^n X_i^2\right) $ is minimal sufficient.
\par
\pf Need to show that such a statistics is sufficient and minimal. First we'll show sufficiency. For each $ t $ pick a $ x_t $ such that $ T(x_t) = t $. Now let $ x\in \chi_^N $ and let $ T(x)=t $. So $ T(x) = T(x_t) $, so by the hypothesis $ \frac{f_X(x,\theta)}{f_X(x_t,\theta)} $ does not depend on $ \theta $. Let this be $ h(x) $ and let $ g(t,\theta) = f_X(x,\theta) $ then we have that $ f_X(x,\theta)=g(t,\theta)h(x) $ so sufficient.\par
Now let $ S $ be any other sufficient statistic. By the factorisation criterion, there exists $ g_S, h_S $ such that $ f_X(x\mid\theta)=G_S(S(x),\theta)h_S(x) $. Suppose $ S(x)=S(y) $. Then
\begin{align*}
	\frac{f_X(x\mid \theta)}{f_X(y\mid\theta)} = \frac{g_S(S(x),\theta)h_S(x)}{g_S(S(y),\theta)h_S(y)} = \frac{h_S(x)}{h_S(y)}
\end{align*}
which does not depend on $ \theta $ so $ T(x)=T(y) $ so $ T $ is minimal sufficient.\qed
\par
We know that bijections of minimal sufficient statistics are still minial sufficient statistics, so we can write our minimal sufficient statistic for $ X_1,\dots, X_n\sim\mathcal N(\mu,\sigma^2) $ as
\[
	S(X)=(\overline X,S_{XX})
\]
where $ \overline X = \frac 1n \sum_i X_i $ and $ S_{XX} = \sum_i(X_i-\overline X)^2 $, since there is a bijection between them.\par
Until now we used $ \mathbb E_\theta $ and $ \mathbb P_\theta $ to denote expectation and probability when $ X_1,\dots, X_n $ are iid from a distribution with pdf $ f_X(x\mid \theta) $. From now on we drop the subscript $ \theta $ to simplify notation.
\begin{theorem}
	(Rao-Blackwell Theorem) Let $ T $ be a sufficient statistic for $ \theta $ and let $ \tilde\theta $ be an estimator for $ \theta $ with $ \mathbb E(\tilde\theta^2)<\infty,\ \forall\theta $. Define a new estimator $ \hat\theta = \mathbb E[\tilde\theta\mid T(X)] $. Then for all $\theta $,
	\[
		\mathbb E[(\hat\theta-\theta)^2] \le \mathbb E[(\tilde\theta-\theta)^2].
	\]
	This inequality is strict unless $ \tilde\theta $ is a function of $ T $.
\end{theorem}
\begin{remark}
	We have that $ \hat\theta(T)=\int \tilde\theta(x)f_{X\mid T}(x\mid T)\mathrm dx $. By sufficiency of $ T $, the conditional pdf does \textit{not} depend on $ \theta $ so $ \hat \theta $ does not depend on $ \theta $, and is valid estimator.
\end{remark}
\pf By the tower property of expectation,
\[
	\mathbb E[\hat\theta] = \mathbb E[ \mathbb E(\tilde\theta\mid T)] = \mathbb E[\tilde \theta].
\]
So $ \mathrm{bias}(\hat\theta) = \mathrm{bias}(\tilde\theta) $ for all $ \theta $. By the conditional variance formula,
\begin{align*}
	\Var{\tilde \theta} &= \mathbb E\left[\Var{\tilde\theta\mid T}\right] + \Var{\mathbb E\left(\tilde \theta\mid T\right)}\\
			    &= \mathbb E\left[\Var{\tilde\theta\mid T}\right] + \Var{\hat\theta}\\
			    &\ge \Var{\hat\theta}.
\end{align*}
So
\[
	\mathrm{mse}(\tilde\theta)\ge \mathrm{mse}(\hat\theta).
\]
Equality is achieved only when $ \Var{\tilde\theta\mid T} = 0 $ with probability $ 1 $ which requiers $ \tilde \theta $ to be a function of $ T $.\qed
\par Let's see an example of this. Suppose that $ X_1,\dots, X_n\sim \text{Poisson}(\lambda) $ iid. Let $ \theta = \prob{X_1=0}=e^{-\lambda} $. Then
\[
	f_X(x\mid\theta) = \frac{e^{-n\lambda}\lambda^{\sum x_i}}{\prod_i x_i!} = \frac{\theta^n (-\log \theta)^{\sum x_i}}{\prod_i x_i!}.
\]
By the factorisation criterion, $ T(X) = \sum_i x_i $ is sufficient. Recall that $ \sum x_i \sim \text{Poisson}(n\lambda) $. Let $ \tilde\theta = \mathbf 1\{X_1 = 0\} $. Then
\begin{align*}
	\hat \theta = \mathbb E[\tilde\theta\mid T=t] &= \prob{X_1 = 0\mid \sum_{i=1}^n X_i = t}\\
						      &= \frac{\prob{X_1  =0, \sum_{i=2}^n X_i = t}}{\prob{\sum_{i=1}^n X_i = t}}\\
						      &= \frac{\prob{X_1 = 0}\prob{\sum_{i=2}^n X_i = t}}{\prob{\sum_{i=1}^n X_i =t}}\\
						      &= \frac{e^{-\lambda }e^{-(n-1)\lambda}\frac{((n-1)\lambda)^t}{t!}}{e^{-n\lambda} \frac{(n\lambda)^t}{t!}} = \left(\frac{n-1}n\right)^t
\end{align*}
Hence $ \hat\theta= \left(1-\frac1n\right)^{\sum x_i} $ has $ \mathrm{mse}(\hat\theta)< \mathrm{mse}(\tilde\theta) $ for all $ \theta $.  We can see that as $ n\to \infty $, $ \hat\theta\to e^{-\overline X} = e^{-\lambda} =\theta$.
\par
Let $ X_1,\dots, X_n\sim \mathrm{Uniform}([0,\theta]) $ and suppose we want to estimate $ \theta\ge 0 $. Last time we saw that $ T = \max{X_i} $ is sufficient for $ \theta $. Let $ \tilde\theta = 2X_1 $ be an estimator (unbias). Then
\begin{align*}
	\hat \theta = \mathbb E[\tilde\theta\mid T=t] &= 2\mathbb E[X_1\mid \max X_i =t]\\
						      &= 2\mathbb E[X_1\mid \max X_i = t, X_1 = \max X_i]\prob{X_1 = \max X_i\mid \max X_i = t}\\
						      &\qquad+ 2\mathbb E[X_1\mid \max X_i =t, X_1\ne \max X_i] \prob{X_1\ne \max X_i \mid \max X_i t}\\
						      &= 2t\frac 1n + 2\mathbb E\left[X_1\mid X_1<t, \max_{i>1} X_i =t\right]\left(\frac{n-1}n\right)\\
						      &=\left(\frac{n+1}n\right)t.
\end{align*}
Hence $ \hat\theta = \frac{n+1}n \max_i X_i $ is an estimator with $\text{mse}(\hat\theta)<\text{mse}(\tilde\theta) $.
\subsection{Likelihood}
\begin{definition}
	(Likelihood) Let $ X=(X_1,\dots X_n) $ have a joint pdf $ f_X(x\mid \theta) $. The \textit{likelihood} of $ \theta $ is the function
	\[
	  L:\theta \to f_X(x\mid\theta).
	\]
	The max likelihood estimator (MLE) is the value of $ \theta $ maximizing $ L $.
\end{definition}
	If $ X_1,\dots, X_n\sim f_X(\cdot\mid\theta) $ iid, then $ L(\theta) = \prod_{i=1}^n f_X(x_i\mid \theta) $. \par
It's usually easier to work with the log-likelihood, since this reduces to a sum. So in the iid case,
\begin{align*}
	\ell(\theta)= \log(L(\theta)) = \sum_{i=1}^n \log f_X(x_i\mid \theta).
\end{align*}
For example let $ X_1,\dots, X_n \sim \text{Bernoulli}(p)$ iid. Then we get that
\[
	\ell(p) = \left(\sum_{i=1}^n X_i\right)\log p + \left(n-\sum_{i=1}^n X_i\right) \log(1-p).
\]
Taking the derivative with respect to $ p $,
\begin{align*}
	\frac{\partial \ell}{\partial p} = \frac{\sum_i X_i} p - \frac{n-\sum_i X_i}{1-p}.
\end{align*}
So setting the derivative to zero we get that
\[
	p = \frac{\sum X_i} n.
\]
Hence the MLE is
\[
	\hat p = \frac{\sum_i X_i}n,
\]
and since $ \mathbb E[\hat p] = p $, this is unbiased.\par
Now suppose $ X_1,\dots, X_n\sim\mathcal N(\mu,\sigma^2) $.
\begin{align*}
	\ell(\mu,\sigma^2) &= -\frac n2 \log(2\pi) -\frac n2 \log(\sigma^2) -\frac 1{2\sigma^2} \sum_{i=1}^n (X_i-\mu)^2.
\end{align*}
So
\begin{align*}
	\frac{\partial \ell}{\partial \mu} = -\frac 1{\sigma^2} \sum_{i=1}^n (X_i - \mu) 
\end{align*}
which is zero when $ \mu = \frac{\sum_i X_i}n $ regardless of $ \sigma $. Also
\begin{align*}
	\frac{\partial \ell}{\partial \sigma^2} = -\frac{n}{2\sigma^2} + \frac 1{2\sigma^4} \sum_{i=1}^n (X_i-\mu)^2.
\end{align*}
If we set $ \mu = \frac{\sum_i X_i}n $ then we get $ \frac{\partial\ell}{\partial\sigma^2}=0 $ if $ \sigma^2 = \frac 1n \sum(X_i-\bar X)^2 = \frac {S_{xx}}n $. Hence the MLE is
\[
	(\hat \mu , \hat{\sigma}^2 = (\bar X, \frac{S_{xx}}n).
\]
Notte that $ \mu $ is unbiased, but we will see later that
\[
	\frac{S_{xx}}{\sigma^2} = \frac{n\hat\sigma^2}{\sigma^2} \sim \chi_{n-1}^2.
\]
So $ \mathbb E[\hat\sigma^2] = \mathbb E\left(\frac{S_{xx}}n\right) = \frac {n-1}n \sigma^2 $. So $ \hat\sigma^2 $ is not unbiased, but is asymptotically unbiased as $ n\to\infty $.\par
Suppose now that $ X_1,\dots, X_n\sim \text{Uniform}([0,\theta]) $ iid. Then
\[
	\ell(\theta) = \frac 1{\theta^n} \mathbf 1\{\max_i X_i\le \theta\}.
\]
Hence the MLE is $ \hat\theta_{\text{MLE}} = \max_i X_i $. Recall that last time, we had an aunbiased estimator $ \tilde\theta $ and by the Rao-Blackwell Theorem we found the estimator $ \hat\theta = \mathbb E[ \tilde\theta\mid T] = \frac{n+1}n \max_i X_i $. Note that $ \hat\theta_{\text{MLE}} =\frac{n}{n+1} \hat \theta	$, so $ \mathbb E[\hat \theta_{\text{MLE}}] = \frac{n}{n+1} \mathbb E[\hat\theta] = \frac{n}{n+1} \theta $. Again this not unbiased, but is asymptotically unbiased.
\par 
Let's see some properties of the MLE.
\begin{enumerate}
	\item If $ T $ is a sufficient statistic, the MLE is a function of $ T $. We can factorise $ L(\theta) = g(T(x),\theta)h(x) $. 
	\item If $ \phi = h(\theta) $ where $ h $ is a bijection, the MLE of $ \phi $ is $ \hat\phi = h(\hat\theta) $ where $ \hat\theta $ is the MLE of $ \theta $.
	\item Asymptotic normality: $ \sqrt n (\hat\theta_{\text{MLE}} -\theta) $ is approximately normal with mean $ 0 $ for large $ n $. The covariance matrix is the "smallest attainable" (see II Principles of Statistics).
\end{enumerate}
\subsection{Confidence intervals}
\begin{definition}
	(Confidence intervals) A $ (100\gamma)\%$ confidience interval for a paramter $ \theta $ is a random interval $ (A(X),B(X)) $ such that $ \prob{A(X)\le \theta\le B(X)}= \gamma $ for some $ \gamma\in (0,1)  $ and all values of the true parameter $ \theta $. 
\end{definition}
\begin{remark}
  The incorrect interperatation: Having observed $ X=x $, there is a $ 1-\gamma $ probability that $ \theta $ is in $ (A(X),B(X)) $. This is wrong.
\end{remark}
Suppose that $ X_1,\dots, X_n\sim \mathcal N(\theta,1) $ iid. We want to find a $ 95\% $ confidence interval for $ \theta $. We know that $ \overline X = \frac 1n \sum_{i=1}^n X_i\sim \mathcal N(\theta,\frac 1n) $. If we define $ Z = \sqrt n (\overline X - \theta) $ Then $ Z\sim \mathcal N(0,1) $ no matter the value of $ \theta $. Let $ z_1,z_2 $ be numbers with $ \Phi(z_1)-\Phi(z_2) = 0.95 $ where $ \Phi $ is the cdf of the standard normal. $ \prob{z_1\le \sqrt n(\overline X-\theta)\le z_2} = 0.95 $ rearranging we get that
\begin{align*}
	\prob{\overline X - \frac{z_2}{\sqrt n} \le \theta\le \overline X - \frac{z_1}{\sqrt n}} = 0.95
\end{align*}
hence 
\[
	\left(\overline X - \frac{z_2}{\sqrt n}, \overline X - \frac{z_1}{\sqrt n}\right)
\]
is a $ 95\% $ confidence interval.\par
This is the recipe for confidence intervals.
\begin{enumerate}
	\item Find a quantity $ R(X,\theta) $ such that this $ \mathbb P_\theta $ distribution of $ R(X,\theta) $ does not depend on $ \theta $. This is called a \textit{pivot} for example $ R(X,\theta) = \sqrt n(\overline X-\theta) $.
	\item Write down the statement
		\[
			\prob{c_1\le R(X,\theta)\le c_2}=\gamma
		\]
		where $ (c_1,c_2) $ are quantiles of the distribution of $ R(X,\theta) $. 
	\item Rearranging the above to leave $ \theta $ in the middle of the inequality, so we get something in the form
		\[
			\prob{A(X)\le \theta \le B(X)}.
		\]
\end{enumerate}
\begin{remark}
	When $ \theta $ is a vector, we talk about \textit{confidence sets} rather than intervals.
\end{remark}
Suppose that $ X_1,\dots, X_n\sim \mathcal N(0,\sigma^2) $ iid. We want a $ 95\% $ confidence interval for $ \sigma^2 $. Note that
\[
	\frac{X_i} \sigma \sim \mathcal N(0,1),
\]
so $ \sum_{i=1}^n \frac{X_i^2}{\sigma^2}\sim \chi^2_n $. Hence \[
	R(X,\sigma^2) = \sum_{i=1}^n \frac{X_i^2}{\sigma^2}
\]
is a \textit{pivot}. Let $ \inv F_{\chi^2_n}(0.025) $ and $ \inv F_{\chi^2_n}(0.975) $. Then
\begin{align*}
	\prob{c_1\le \sum_{i=1}^n \frac{X_i^2}{\sigma^2} \le c_2} = 0.95,
\end{align*}
so rearranging we get that
\[
	\prob{\frac{\sum X_i^2}{c_2} \le \sigma^2 \le \frac{\sum X_i^2}{c_1}}=0.95
\]
gives our confidence interval.
\par
Now suppose that $ X_1,\dots, X_n\sim \mathrm{Bernoulli}(p) $ for large $ n $. We will find an approximate 95\% confidence interval for $ p $. The maximium likelihood estimator of $ p $ is $ \hat p = \frac 1n \sum_{i=1}^n X_i $. By the central limit theorem $ \hat p \sim \mathcal N\left(p,\frac{p(1-p)}n\right) $ approximately. Thus
\[
	\frac{\sqrt n (\hat p - p)}{\sqrt{p(1-p)}}\sim \mathcal N(0,1)
\]
for large $ n $. So we have our pivot, which gives
\[
	\prob{-z_{0.025} \le \frac{\sqrt n(\hat p -p)}{\sqrt{p(1-p)}} \le z_{0.025}} \approx 0.95
\]
Instead of inverting directly, if $ n $ is large $ \hat p \approx p $, so switching $ p $ with $ \hat p $ on the denominator we get that
\[
	\prob{\hat p - z_{0.025}\sqrt{\frac{\hat p(1-\hat p)}n}\le p\le \hat p + z_{0.025}\sqrt{\frac{\hat p(1-\hat p)}n}}\approx 0.95
\]
which gives our confidence interval. Since for all $ \hat p \in [0,1] $ we have $ \hat p(1-\hat p)\le \frac 14 $ we would also report a conservative confidence interval of $ \hat p \pm z_{0.025}\sqrt{\frac 1{4n}} $.
\subsection{Bayesian estimation}
So far we've been using frequentist methods treating $ \theta\in\Theta $ as fixeed. For Bayesian methods, we treat $ \theta $ as random with a prior distribution $ \pi(\theta) $. Conditional on $ \theta $ the data $ X $ has pdf $ f_X(\cdot\mid \theta) $. Having observed that $ X=x $, we combine with the prior to form a posterior distribution $ \pi(\theta\mid X) $. By Bayes' rule,
\begin{align*}
	\pi(\theta\mid x) = \frac{\pi(\theta)f_X(x\mid \theta)}{f_X(x)}
\end{align*}
where $ f_X(x) $ is the marginal distribution of $ X $, so
\[
  f_X(x) = \begin{cases}
	  \int_\Theta f_X(x\mid \theta)\pi(\theta) \mathrm d\theta & \text{if }\theta \ \text{is continuous}\\
	  \sum_{\theta\in \Theta} f_X(x\mid \theta)\pi(\theta) & \text{if}\ \theta\ \text{is discrete}
  \end{cases}.
\]
More simply,
\[
  \pi(\theta \mid x)\propto \pi(\theta)f_X(x\mid \theta).
\]
Often is it easier to reconginise the RHS as propotional to a known distribution. 
\begin{remark}
  By the factorisation criterion, the posterior only depends on $ X $ through a sufficient statistic.
  \begin{align*}
	  \pi(\theta\mid x)&\propto \pi(\theta) \cdot f_X(x\mid \theta) = \pi(\theta)\cdot g(T(x),\theta)h(x)\\
			   &\propto \pi(\theta)g(T(x),\theta).
  \end{align*}
\end{remark}
Suppose $ \theta\in [0,1] $ is the mortality rate for some procedure at Addenbrooks. In the first 10 operations there are no deaths. In other hospitals across the country the mortality rate is between $ 3-20\% $, with average of $ 10\% $. Consider the prior distribution $ \pi(\theta)\sim \text{Beta}(a,b) $ we can choose $ (a,b) = (3,27) $ so $ \pi(\theta) $ as mean $ 0.1 $ and $ \pi(0.03 <\theta<0.2) = 0.9 $.
\par
Let $ X_i\sim \text{Bernoulli}(\theta) $ be indicator for whether $ i $th patient at Addenbrookes dies.
\[
	f_X(x\mid \theta) = \theta^{\sim x_i}(1-\theta)^{n-\sum x_i}.
\]
The posterior is
\begin{align*}
	\pi(\theta\mid X) &\propto \pi(\theta)f_X(x\mid \theta) \\
			  &\propto \theta^{a-1}(1-\theta)^{b-1} \theta^{\sum x_i}(1-\theta)^{n-\sum x_i}\\
			  &= \theta^{\sum x_i+a-1}(1-\theta)^{b+n-\sum x_i-1}.
\end{align*}
Hence
\[
	\pi(\theta\mid X) \sim \text{Beta}(a+\sum x_i, b+n-\sum x_i)
\]
so pluggin in $ a=3,b=27,n=10,\sum X_i = 0 $, so the posterior is $ \text{Beta}(3,37) $.
\begin{remark}
	In the example the prior and posterior were from the same family of distributions known as \text{conjugacy}.
\end{remark}
Supposewe put a $ \text{Beta}(a,b) $ prior on the parameter $ \theta $ of kidney cancer death rates in each county. We can estimate $ (a,b)=(27, 58000) $ with $ \frac a{a+b}\approx 4.65\times 10^{-9} $ being the kidney cancer death rate in the United States. The previous example shows that if we observe $ \sum_{i=1}^n X_i$ deaths in a county, the posterior mean estimate is $ \frac{a+\sum X_i}{a+b-n} $. This is equal to
\begin{align*}
	\frac n{a+b+n}\cdot\frac{\sum X_i}n + \frac{a+b}{a+b+n}\cdot \frac a{a+b}.
\end{align*}
For large $ n $, we use $ \approx \frac{\sum X_i}n $ as our estimate, for small $ n $ we use $ \frac a{a+b} $ and in between we shrink our estimate between them.\par
What is the use of the posterior distribution? This opens us to decision theory.
\begin{enumerate}
	\item We must pick a decision $ \delta \in D $;
	\item We have a loss function $ L(\theta,\delta) $e which gives loss incurred in making decision $ \delta $ when the true paramter value is $ \theta $.
	\item Von-Neumann-Morgenstern Theorem. Under axioms of rational behaviour, pick $ \delta $ that minimises expected lsos under posterior.
\end{enumerate}






\end{document}
