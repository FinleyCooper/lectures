\documentclass{article}
\usepackage{../header}
\title{Methods}
\author{Notes made by Finley Cooper}
\begin{document}
\maketitle
\newpage
\tableofcontents
\newpage
\section{Fourier Series}
\subsection{Motivation}
In 1807 J. Fourier was studying head conduction along a metal rod. This lead him to study $ 2\pi $-periodic functions i.e. functions $ f:\R\to \R $ was such that $ f(\theta+2\pi)=f(\theta) $ for all $ \theta\in \R $ then he found that if
\[
	f(\theta)=\sum_{n\in\Z}\hat{f_n}e^{in\theta}
\]
\begin{align*}
	\frac{1}{2\pi}\int_{-n}^n e^{i\lambda x}\hat f(\lambda)\,\mathrm d\lambda - f(x)
	&= \allint \sin(ny)\frac{f(x+y)-f(x)}{\pi y}\,\mathrm dy\\
	&= \allint \sin(ny)F(y;x)\,\mathrm dy\\
	&= \frac{1}{n}\allint \cos(ny)F_y(y;x)\,\mathrm dy \to 0.
\end{align*}
\qed
\[
	V=\{f:\R\to\C:\text{ with } f \text{ a "nice" function},\text { } f(\theta+L)=f(\theta), \forall \theta\in\R\}.
\]
Note for $ f\in V $ need only to consider values of $ f $ taken in an interval of length $ L $, i.e. $ [0,L) $ or $ (-\frac L2,\frac L2] $ since periodicity covers elsewhere.\par
We can introduce an inner product on $ V $ with
\[
	\langle f,g\rangle=\int_0^1f(\theta)\overline{g(\theta)}\mathrm d\theta.
\]
This gives the associated norm,
\[
	||f||=\sqrt{\langle f,f\rangle}.
\]
For $ n\in \Z $ consider $ e_n\in V $ defined by $ e_n(\theta)=e^{2\pi i n\theta / L }$.
\[
\langle e_n,e_m\rangle = \int_0^L e^{2\pi i (n-m)\theta / L}\,\mathrm d\theta = L\,\delta_{nm}.
\]
So $ \{e_n\} $ are orthogonal and $ ||e_n||^2=L $ for each $ n\in \Z $. This looks like IA Vectors and Matrices.\par
Recall that if $ v_N $ is $ N $-dim vector space equipped with usual inner product and $ \{ e_n\}^N_{n=1} $ are orthogonal with $ | e_n|=L $, then for each $  x\in V $ we can write $  x = \sum_{n=1}^N \hat{x}_n{e_n} $ for some $ \{\hat {x}_n\} $. To find $ \{\hat{x}_n\} $ take the inner product of both sides with $  e_m $. So
\[
	( x,  e_m)=\sum_{n=1}^N \hat{x}_n( e_n\cdot  e_m)=L\hat{x}_m
\]
i.e
\[
\hat x_n = \frac 1L( x\cdot  e_n).
\]
Now could this work on $ V $? $ V $ is not finite dimensional so it's not obvious. Every subset of $ \{e_n\} $ is linearly indepedent. Ignoring this for now we assume that for all $ f\in V $ we can write $ f $ in our basis $ \{e_n\} $. Then
\[
  f(\theta)=\sum_n\hat f_ne_n(\theta),
\]
So taking the inner product as before
\[
  \langle f,e_m\rangle = \sum_n\hat f_n\langle e_n,e_m\rangle
\]
so using the delta as before
\[
  =L\hat f_m
\]
i.e.
\[
	\hat f_n=\frac{1}{L}\langle f, e_n\rangle = \frac{1}{L} \int_0^1 f(\theta)e^{-2\pi in\theta/L}\mathrm d\theta
\]
\begin{definition}
	(Complex Fourier series) For an $ L $-periodic $ f:\R\to \C $ define its \textit{complex Fourier series} by
	\[
		\sum_n\hat f_n e^{2\pi in \theta /L}
	\]
	where
	\[
		\hat f_n = \frac 1L \int_0^1 f(\theta) e^{-2\pi in\theta/L}\mathrm d \theta
	\]
	are called the complex Fourier coefficients. We will write for $ f\in V $
	\[
		f(\theta)\sim \sum_n\hat f_n e^{2\pi in\theta/L}
	\]
	to mean the series on the right corresponds to complex Fourier series for the function on the left.
\end{definition}
We'd like to replace the $ \sim $ symbol with equality, but we require a bit more than that.\par
If we split the complex Fourier series into the parts $\{n=0\}\cup\{n>0\}\cup\{n<0\}$ we get
\[
\sum_n\hat f_n e^{2\pi i n\theta / L} = \hat f_0 + \sum_{n=1}^\infty \hat f_n\left[\cos\left(\frac{2\pi n\theta}{L}\right)+i\sin\left(\frac{2\pi n\theta}{L}\right)\right]
+
\sum_{n=1}^\infty \hat f_{-n}\left[\cos\left(\frac{2\pi n\theta}{L}\right)-i\sin\left(\frac{2\pi n\theta}{L}\right)\right].
\]

\begin{definition}
	(Fourier series) For $ f: \R\to \C $ an $ L $-periodic function define its \textit{Fourier series} by
	\[
		\frac 1L a_0+\sum_{n=1}^\infty\left[a_n\cos\left(\frac{2\pi n\theta}L\right)+b_n\sin\left(\frac{2\pi n\theta}L\right)\right]
	\]
	where
	\[
		a_n=\frac 2L\int^L_0f(\theta)\cos\left(\frac{2\pi n\theta}L\right)\mathrm d\theta
	\]
	and
	\[
		b_n=\frac 2L\int^L_0f(\theta)\sin\left(\frac{2\pi n\theta}L\right)\mathrm d\theta
	\]
	are called the Fourier cofficients for $ f $.
\end{definition}
If we set
\begin{align*}
c_n(\theta) &= \cos\left(\frac{2\pi n\theta}{L}\right),\\
s_n(\theta) &= \sin\left(\frac{2\pi n\theta}{L}\right),
\end{align*}
then we can show, for $ m,n\ge 1 $ that $ \langle c_n,c_m\rangle=\langle s_n, s_m\rangle =\frac L2 \delta_{mn} $ and
\[
  \langle c_n,1\rangle = \langle s_m,1\rangle = \langle c_n, s_m \rangle = 0.
\]
So we have that $ \{1,c_n,c_n\} $ is orthogonal set in $ V $.\par
For an example take $ f:\R\to \R $, $1$-periodic, such that $ f(\theta)=\theta(1-\theta)$ on $[0,1)$. For $n\neq 0$ we have
\[
\hat f_n = \int_0^1 \theta(1-\theta)e^{-2\pi i n\theta}\,\mathrm d\theta.
\]
Integrating by parts (or using a standard Fourier integral computation) yields
\[
\hat f_n = -\frac{1}{2(\pi n)^2},\qquad n\neq 0,
\]
and
\[
\hat f_0 = \int_0^1 (\theta-\theta^2)\,\mathrm d\theta = \frac{1}{6}.
\]
Hence
\[
f(\theta) \sim \frac{1}{6} - \sum_{n\neq 0}\frac{e^{2\pi i n\theta}}{2(\pi n)^2}.
\]
so the sine terms cancel in the sum giving just cosine terms as we expect since our $ f $ function is even.
\subsection{Convergence of Fourier series}
This subject is extremely subtle.

\begin{definition}
	For $ f:\R\to \C $ an $ L $-periodic function we defined the \textit{partial Fourier series} as
	\begin{align*}
		(S_Nf)(\theta)&=\sum_{|n|<N}\hat f_ne^{2\pi in\theta /L} \\
			      &= \frac 12a_0+\sum_{n=1}^N\left[a_n\cos\left(\frac {2\pi n\theta}L\right)+b_n\sin\left(\frac{2\pi n\theta}L\right)\right]
	\end{align*}
\end{definition}
Natural to ask if $ (S_Nf)\to f $. For this we need to specify what type of functional convergence we're looking at. Pointwise? Uniform? Maybe they converge in the idea of our new norm?
\[
	||S_Nf-f||=\sqrt{\int_0^L|(S_Nf)(\theta)-f(\theta)|^2\mathrm d\theta}\to 0
\].
For simplicity, we will only consider pointwise convergence.
\begin{proposition}
  Let $ f:\R\to\C $ be an $ L $-periodic function for which on $ [0,L) $ we have the following,
  \begin{enumerate}
	  \item $ f $ has finitely many discontinuities.
	  \item $ f $ has finitely many local maxima and minima.
  \end{enumerate}
  Then for each $ \theta\in[0,1) $ we have
  \begin{align*}
	  \frac{\theta_++\theta_-}2 &= \lim_{n\to \infty}(S_Nf)(\theta)\\
				    &= \sum_n\hat f_n e^{2\pi in\theta/L}
  \end{align*}
where $ f(\theta_\pm) = \lim_{\varepsilon\to 0^+}f(\theta \pm \varepsilon) $. So at the points of continuity the Fourier series gives back the original function, and at points of discontunity the Fourier series gives back the average of the function at the disconunity neighbourhood.
\end{proposition}
We call functions which properties (i) and (ii) Dirichlet functions. For now on assume all functions are Dirichlet functions so that $ \sim $ means that the series on the RHS coincides with the function on the LHS at points of continuity and to the average at points of discontinuity.\par
\pf We'll prove the proposition only for functions in $ C^\infty(\R) $ (actually $ C^1(\R) $ will do.\\
Assume \textit{wlog} that $ L=2\pi $. Examine $ \lim S_Nf(\theta_0) $ for some $ \theta_0\in[0,2\pi) $. By replacing $ f(\theta) $ with $ f(\theta+\theta_0) $ can assume that $ \theta_0=0 $ \textit{wlog}.
\begin{align*}
	(S_Nf)(\theta)&=\sum_{|n|\le N}\hat f_ne^{in\cdot \theta} \\
		      &= \sum_{|n|\le N}\left(\frac{1}{2\pi}\int_{-\pi}^\pi f(\theta)e^{-in\theta}\mathrm d\theta\right)\\
		 &= \frac{1}{2\pi}\int_{-\pi}^\pi f(\theta)\left[\sum_{|n|\le N}e^{-in\theta}\right]\mathrm d\theta
\end{align*}
We can sum the series as a geometric series, so
\begin{align*}
	e^{-iN\theta}\sum_{n=0}^{2N}e^{-in\theta}=\frac{\sin[(N+\frac 12)\theta]}{\sin(\frac{\theta}{2})}
\end{align*}
when $ \theta\in \R\setminus2\pi\Z $ and the sum is $ 2N+1 $ when $ \theta\in2\pi\Z $.\\
Define the \textit{Dirichlet Kernal} as
\[
  D_N(\theta)=\begin{cases}
	  \frac{\sin[(N+\frac 12)\theta]}{\sin(\frac\theta 2)} & \theta\in \R\setminus 2\pi \Z \\
	  2N+1 & \text{otherwise}
  \end{cases}
\]
For each $ N\ge 0 $,
\begin{enumerate}
	\item $ D_N $ is continiuous, even $ 2\pi $ perioidic
	\item $ \int_{-\pi}^\pi D_N(\theta)\mathrm d\theta=2\pi $
\end{enumerate}
Property $ (ii) $ follows by intergrating $ \sum $ termwise, only $ 1 $ is non-zero. This means that
\[
	f(0)=\frac 1{2\pi}\int_{-\pi}^\pi D_N(\theta)f(\theta)\mathrm d\theta
\]
So
\[
	S_N(f)(0)=f(0)=\frac 1{2\pi}\int_{-\pi}^\pi D_N(\theta)[f(\theta)-f(0)]\mathrm d\theta
\]
now set $ F(\theta)=\frac{\theta}{\sin(\frac \theta 2)}\left[\frac{f(\theta)-f(0)}\theta\right] $
	so we get 
	\[
		(S_Nf)(0)=\frac 1{2\pi}\int_{-\pi}^\pi \sin[(N+\frac 12)\theta]F(\theta)\mathrm d\theta
	\]
	Note that $ \theta\to F(\theta) $ is smooth since
	\[
		\frac{f(\theta)-f(0)}{\theta}=\frac 1\theta\int_0^\theta f'(t)\mathrm dt=\frac 1\theta \int_0^1 f'(\tau\theta)\theta\mathrm d\tau
	\]
	Hence integrating by parts gives that
\begin{align*}
	(S_Nf)(0)-f(0)&=\frac{1}{N+\frac 12}\frac{1}{2\pi}\int_{-\pi}^\pi \cos[(N+\frac 12)\theta]F'(\theta)\mathrm d\theta \\
	&\to 0 \text{ as } N\to \infty
\end{align*}
For an example consider the function
\[
  f(\theta)=\begin{cases}
	  +1 & 0\le \theta < \pi \\
	  -1 & -\pi \le \theta < 0
  \end{cases}
\]
Since $ f $ is odd, $ a_n=0 $ for each $ n $ and
\begin{align*}
	b_n&=\frac{2}{2\pi}\int_{-\pi}^\pi f(\theta)\sin(n\theta)\mathrm d\theta \\
	   &= \frac 2\pi \int_0^\pi \sin(n\theta)\mathrm d\theta\\
	   &= \frac 2{n\pi}[1-(-1)^n]
\end{align*}
Thus
\[
	f(\theta)\sim \frac 4\pi \sum_{n \text{ odd}} \frac{\sin(n\theta)}n
\]
\subsection{Peridoic extensions: Cosine and sine series}
Given a function $ f:[0,L)\to \C $ we can define $ 2L $-periodic even/odd extensions called $ f_{even},f_{odd} $. Define,
\[
	f_{even}(\theta)=\begin{cases}
		f(\theta) & \theta\in[0,L)\\
		f(-\theta) & \theta\in[-L,0)\\
	\end{cases}
\]
and\[ 
f_{odd}(\theta)=\begin{cases}
		f(\theta) & \theta\in[0,L)\\
		-f(-\theta) & \theta\in[-L,0)\\
	\end{cases}
\].
Note that $ f(\theta)=f_{even}(\theta)=f_{odd}(\theta) $ if $ \theta\in [0,L) $.
\begin{align*}
	f_{even}(\theta)&\sim \frac 12 A_0+\sum_{n=1}^\infty A_n\cos\left(\frac{2\pi n \theta}{2L}\right)
	\\
	A_n&=\frac2{2L}\int_{-L}^Lf_{even}(\theta)cos\left(\frac{2\pi n\theta}{2L}\right)\mathrm d\theta\\
	   &= \frac 2L\int_0^Lf(\theta)cos\left(\frac{2\pi\theta}L\right)\mathrm d\theta
\end{align*}
simiarly we have that 
\begin{align*}
	f_{odd}(\theta)&\sim\sum_{n=1}^\infty B_n\sin\left(\frac{2\pi n\theta}{2L}\right)\\
	B_n&=\frac 2L\int_0^Lf(\theta)\sin\left(\frac{n\pi\theta}L\right)\mathrm d\theta
\end{align*}
\begin{definition}
	For $ f: [0,L)\to \C $ define its \textit{cosine} and \textit{sine} series by
	\begin{align*}
		\frac{1}2 A_0+\sum_{n=1}^\infty A_n\cos\left(\frac{n\pi\theta}L\right),\quad \sum_{n=1}^\infty B_n\sin\left(\frac{n\pi\theta}L\right)
	\end{align*}
	where $ A_n $ and $ B_n $ defined as before.
\end{definition}
For an example consider $ f(\theta)=1 $ on $ [0,\pi) $. For the sine series,
\begin{align*}
	B_n=\frac 2\pi \int_0^\pi \sin(n\theta)\mathrm d\theta=\frac2{n\pi}(1-(-1)^n)
\end{align*}
On the interval $ (0,\pi) $ we get that $ 1=f(\theta)=f_{odd}(\theta)=4\sum_{n\in \N}\frac{\sin(n\theta)}{n\pi} $. Whereas for the cosine series we get that 
\[
A_0=2,\quad A_n=0\quad n\ge 1
\]
So for $ \theta\in[0,\pi) $ we get that $ f_{even}=\frac 12\cdot 2 =1 =f(\theta) $.
\subsection{Regularity and decay of Fourier coefficients}
A true but non-examinable fact is that if $ g:[a,b]\to \C $ is integrable on $ [a,b] $ and $ \lambda\in \R $ then 
\[
	\int_a^be^{-i\lambda\theta}g(\theta)\mathrm d\theta\to 0 \text{ as } |\lambda|\to \infty. 
\]
IF $ f:\R\to \C $ is a $ L $-periodic function and integrable on $ [0,L) $ then
\begin{align*}
	\hat f = \frac 1L\int_0^Le^{-2\pi i n \theta/L}f(\theta)\mathrm d{\theta}
\end{align*}
so taking $ \lambda=\frac{2\pi n}L $ gives that $ \hat f_n\to 0 $ as $ n\to \infty $ by the Riemann-Lebesgue lemma. Also\[
a_n=\hat f_n+\hat f_{-n} \quad b_n=i(\hat f_n -\hat f_{-n}),\] 
both go to zero as $ n\to \infty $.\par
Suppose that $ f $ is $ L $-periodic and $ f\in C^k(\R) $.
\begin{align*}
	\hat f_n&=\frac 1L\int_0^Le^{-2\pi in\theta/L}f(\theta)\mathrm d\theta\\
		&=-\frac 1L\left(\frac{L}{2\pi in}\right)f(\theta)e^{-2\pi in\theta/L}\mathhuge|^L_{\theta=0}+\left(\frac{L}{2\pi in}\right)\frac 1L \int_0^Le^{-2\pi i n \theta/L}f'(\theta)\mathrm d\theta \\
		&= -\frac{L}{2\pi in}\left[\frac{f(L^-)-f(0^+)}L\right]+\frac{L}{2\pi in}\frac 1L\int_0^Le^{-2\pi in \theta}f'(\theta)\mathrm d\theta
\end{align*}
Since $ f $ is periodic and continuously differentiable we have that 
\[
  f(0^+)=f(L^+)=f(L^-)
\]
hence the boundary term cancels so repeating we get that 
\[
	\hat f_n=\left(\frac{L}{2\pi in}\right)^k\frac 1L\int_0^Le^{-2\pi in\theta/L}f^{(k)}(\theta)\mathrm d\theta
\]
and the integral is $ o(1) $ by the Rieman-Lebesgue lemma.\par
So we get that if $ f $ is $ C^k(\R) $ then $ \hat f_n = o\left(\frac 1{n^k}\right) $ as $ |n|\to \infty $.
\subsection{Termwise differentiation}
Suppose $ f $ is $ L $-periodic continuously differentiable on $ [0,L) $ with $ f'=g $ thne $ g $ is continuous on $ [0,L) $ so
\begin{align*}
	\hat g_n&=\frac 1L\int_0^Le^{-2\pi in\theta/L}f'(\theta)\mathrm d\theta\\
		&=\frac{f(L^-)-f(0^+)}L+\left(\frac{2\pi in}{L}\right)\frac 1L\int_0^Le^{-2\pi in\theta/L}f(\theta)\mathrm d\theta
\end{align*}
If $ f $ is continuous on $ \R $ then by periodicity we have that 
\begin{align*}
  f(0^+)=f(L^+)=f(L^-)
\end{align*}
so that 
\[
	\hat g_n=\left(\frac{2\pi in}L\right)\hat f_n
\]
i.e.
\[
	f'(\theta)=g(\theta)\sim \sum_n\left(\frac{2\pi in}L\right)\hat f_ne^{2\pi in\theta/L}
\]
\subsection{Parseval's theorem}
	If we have that \[f(\theta)\sim \sum_n\hat f_n e_n(\theta) \]
	and
	\[
	  g(\theta)\sim \sum_n\hat g_n e_n(\theta)
	\]
	then taking the inner product of both function we get that
	\begin{align*}
		\langle f,g\rangle &= \sum_{n,m}\hat f_n\overline{\hat g_n}\langle e_n,e_m\rangle\\
				   &= L\sum_n\hat f_n \overline{\hat g_n}
	\end{align*}
	finally that
	\begin{align*}
		\frac 1L\int_0^L f(\theta)\overline{g(\theta)}\mathrm d\theta=\sum_n \hat f_n \overline{\hat g_n}
	\end{align*}
	 and when $ f $ and $ g $ are the same we get that
	 \[
	   \frac 1L\int_0^L|f(\theta)|^2\mathrm d\theta=\sum_n|\hat f_n|^2
	 \]
	 \section{Sturm-Liouville Theory}
	 \subsection{Abstract eigenvalues problem}
	 Recall from IA Vectors and Matrices that a linear map $ A:V_N\to V_n $ was called \textit{Hermitian} if $ A^\dagger = A $ or equivalently we have that
	 \[
	   \mathbf x \cdot(A\mathbf y)=(A\mathbf x)\cdot \mathbf x
	 \]
	 for all $ \mathbf x,\mathbf y\in V_N $.\par
	 They had properties where all eigenvalues are real, eigenvectors with distinct eigenvalues were orthogonal, and that we could pick an porthogonal set of eigenvectors $ \{\mathbf v_i\}^N_{i=1} $ such that for each $ \mathbf x\in V_n $ we have that
	 \[
		 \mathbf x = \sum_{i=1}^N\hat{\mathbf x}_i\mathbf v_i
	 \]
	 where
	 \[
		 \hat{\mathbf x}_i=\frac{\mathbf x\cdot \mathbf y}{|\mathbf v_i|^2}.
	 \]
	 But now we're in $ N=\infty $, we can't assume everything we've learnt so far.\par
	 Use a vector space of nice functions, $ f:[a,b]\to \C $ with an inner product
	 \[
		 \langle f,g\rangle_w=\int_a^bf(x)\overline{g(x)}w(x)\mathrm dx.
	 \]
	 where $ w $ is real valued and $ w>0 $ on $ (a,b) $. We call $ w $ the \textit{weight function} associated with the inner product. This gives an associated norm
	 \[
		 ||f||=\sqrt{\langle f,f\rangle_w}
	 \]
	 when $ w(x)=1 $ we just write $ \langle \cdot, \cdot \rangle $.
	 \begin{definition}
		 (Self-adjoint) A linear differential operator, $ L $, is said to be \textit{self-adjoint} on $ (V,\langle \cdot,\cdot\rangle_w) $ if
		 \[
		   \langle Ly_1,y_2\rangle_w=\langle y_1,Ly_2\rangle_w\quad\forall y_1,y_2\in V.
		 \]
	 \end{definition}
	 \begin{definition}
		 (Eigenfunction/value) For $ (y,\lambda) \in (V\setminus\{0\}\times \C $ is an \textit{eigenfunction, eigenvalue} pair for $ L $ if $ Ly=\lambda y $.
	 \end{definition}
	 \begin{proposition}
	  If $ L $ is self-adjoint on $ (V,\langle \cdot,\cdot,\rangle_w) $ then:
	  \begin{enumerate}
		  \item Eigenvalues are real,
		  \item eigenfunctions with distinct eigenvalues are orthogonal,
		  \item there exists a complete orthogonal set of eigenfunctions $ \{y_n\}_{n=1}^\infty $ i.e. for each $ f \in V $ we can write,
			  \[
				  f=\sum_{n=1}^\infty \hat f_ny_n
			  \]
			  where
			  \[
				  \hat f_n=\frac{\langle f, y_n\rangle_w}{||y_n||_w^2}
			  \]
	  \end{enumerate}
	 \end{proposition}
	 \pf (For (i)) If $ Ly=\lambda y $ with $ y\ne 0 $ then
	 \begin{align*}
		 (\lambda-\overline\lambda)||y||_w^2&=\langle \lambda y,y\rangle_w-\langle y,\lambda y\rangle_w \\
						    &= \langle Ly, y\rangle_w - \langle y,Ly\rangle_w \\
						    &= 0 \implies \lambda = \overline \lambda
	 \end{align*}
	 (For (ii)) If $ Ly_1=\lambda_1y_1, Ly_2=\lambda_2y_2 $ with $ \lambda_1\ne \lambda_2 $,
	 \begin{align*}
		 (\lambda_1-\lambda_2)\langle y_1,y_2\rangle_w &= \langle \lambda_1y_1,y_2\rangle_w-\langle y_1,\lambda_2y_2\rangle _w\\
							       &= \langle Ly_1,y_2\rangle - \langle y_1,Ly_2\rangle_w \\
							       &= 0 \implies \langle y_1,y_2\rangle_w=0
	 \end{align*}
	 The third statement is too hard to prove for this course.\qed
\par
We will study problems of the form
\begin{align}
  \begin{cases}
	  Ly=\lambda y a<x<b \\
	  y \text{ satisfies some boundary conditions at } x=a,b
  \end{cases}
\end{align}
\begin{definition}
	(Sturm-Liouville operator) We say that $ L $ is a \textit{Sturm-Liouville operator} on $ (a,b) $ if it has the form
	\begin{align*}
		L&=\frac 1w\left[-\frac{\mathrm d}{\mathrm d x}\left(p\frac{\mathrm d \mathhuge\cdot}{\mathrm dx}\right)+q\mathhuge\cdot\right]\\
		 &= \frac 1w\left[-p\frac{\mathrm d^2\mathhuge \cdot}{\mathrm dx^2}-p^2\frac{\mathrm d \mathhuge \cdot}{\mathrm dx}+q\mathhuge\cdot\right]
		\end{align*}
		where $ p,q,w $ are real valued and $ p,w>0 $ on $ (a,b) $. We call $ w $ the \textit{weight function}.
\end{definition}
See that $ Ly=\lambda y $ is equvialent to
\[
	-\frac{\mathrm d}{\mathrm dx}\left(p\frac{\mathrm dy}{\mathrm dx}\right)+qy=\lambda wy\quad a<x<b.
\]
We will enforce boundary conditions by stipulating that $ y $ belongs to a suitable vector space of functions that appropriate behaviour at the boundaries.
\begin{definition}
	(Singular) For a Sturm-Liiouville operator on $ (a,b) $ say an endpoint $ c\in \{a,b\} $ is \textit{singular} if $ p(c)=0 $ and \textit{non-singular} otherwise. 
\end{definition}
We will impose real homogeneous boundary conditions of the form
\[
	c\in \{a,b\}\quad \alpha_cy(c)+\beta_cy'(c)=0
\]
at each non-singular endpoint, :w
for $ \alpha_c,\beta_c\in \R $ and $ \alpha_c^2+\beta_c^2\ne 0 $.\par
We will work on generic vector spaces of the form
\[
	V=\mathhuge{\{}
	  y\in C^2[a,b]: y \text{ satisfies real homogeneous boundary conditions at each non-singular endpoint }
  \mathhuge{\}}
\]
Let's look at the example
\[
	-\frac{\mathrm d}{\mathrm dx}\left[(1-x^2)\frac{\mathrm dy}{\mathrm dx}\right]=\lambda y\quad -1<x< 1.
\]
So we have that $ p=(1-x^2),q=0,w=1 $. Then $ x=\pm 1 $ both singular. Take $ V=\{y\in C^2[a,b]\} $ then
\[
	\langle f,g\rangle_w=\int_{-1}^1f(x)\overline{g(x)}\mathrm dx.
\]
\begin{proposition}
	If $ L $ is a Sturm-Lionville operator on $ (a,b)  $ with weight function $ w $ then if $ y_1,y_0\in C^2[a,b] $ we have that 
	\[
		\langle Ly_1,y_2\rangle_w-\langle y_1,Ly_2\rangle_w=p(x)W(y_1,\overline{y_2})(x)\mathhuge{|}^b_a
	\]
	where $ W $ is the Wronskian.
\end{proposition}
So if $ y_1,y_2\in V $ then $ L $ is self-adjoint on $ (V,\langle\cdot,\cdot\rangle_w) $.\par
\pf
\begin{align*}
	&\int_a^b\frac 1w\left[-(py')'+qy_1\right]\bar{y_2}w\mathrm dx-\int_a^by_1\frac 1w\left[-(p\bar{y_2})'+q\bar{y_2}\right]w\mathrm dx\\
&= \int_a^b\left[y_1(p\bar{y_2})'-\bar{y_2}(py'_1)'\right]\mathrm dx\\
	&= \int_a^b\frac{\mathrm d}{\mathrm dx}\left[p(x)W(y_1,\bar{y_2})(x)\right]\mathrm dx\\
	&= p(x)W(y_1,\bar{y_2})(x)\mathhuge{|}_a^b.
\end{align*}
Now assume that $ y_1,y_2\in V $. If $ x=c \in \{a,b\} $ is singular then $ p(c)=0 $ hence $ p(c)W(y_1,\bar {y_2})(c)=0 $. If $ c\in \{a,b\} $ non-singular then $ y_1,y_2 $ satisfy boundary conditions of the form
\[
  \alpha_cy(c)+\beta_cy'(c)=0,\quad \alpha_c,\beta_c\in \R, \alpha_c^2+\beta_c^2\ne 0.
\]
Since $ \alpha_c,\beta_c\in \R $ we know that $ \bar y $ also satisfies the same boundary conditions hence
\[
  \begin{pmatrix}
	  y_1(c) & y'_1(c) \\
	  \bar{y_2}(c) & \bar{y_2}'(c) \\
  \end{pmatrix}
  \begin{pmatrix}
    \alpha_c \\
    \beta_c \\
  \end{pmatrix}
  =0
\]
So the determinate of the matrix on the left is zero because $ \alpha_c $ and $ \beta_c $ don't both equal zero hence $ W(y_1,\bar y_2)(c) = \det(\cdots)=0 $ Hence we have that
 \begin{align*}
   \langle Ly_1,y_2\rangle_w-\langle y_1,Ly_2\rangle_w=0
 \end{align*}
 for all $ y_1,y_2\in V $.\qed
 \subsection{Sturm-Lioville Eigenvalue problems}
 We'll be studying problems of the form
 \[
	 -\frac{\mathrm d}{\mathrm dx}\left[p\frac{\mathrm dy}{\mathrm dx}\right] + qy = \lambda y\quad y\in V
 \]
 where
 \[
	 V=\left\{y\in C^2[a,b]:y\text{ satisfies real homogeneous BCs at each non-ingular end point} \right\}
 \]
 Equip $ V $ with an inner product with a weight function as before. Assume elements of $ V $ are real-valued \textit{wlog} since if $ y=u+iv $ and $ Ly=\lambda y $ then we can split up into
 \[
   Lu=\lambda u, \quad Lv=\lambda v
 \]
 since $ p,q,w,\lambda\in \R $. So
 \[
	 \langle y_1,y_2\rangle_w=\int_a^by_1(x)y_2(x)\mathrm dx.
 \]
 Since $ L $ is self-adjoint, we know there exists $ (y_n,\lambda_n)\in (V\setminus \{0\})\times \R $ such that $ Ly_n=\lambda_ny_n $ with $ \langle y_n,y_m\rangle_w=0 $ if $ \lambda_n\ne \lambda_m $ and for $ f\in V $ we have
 \begin{align*}
	 f(x)&=\sum_{n=1}^\infty \hat f_n y_n(x)\\
	 \hat f_n&=\frac{\langle f,y_n\rangle_w}{||y_n||^2_w}
 \end{align*}
 are the generalised Fourier coefficients of $ f $. It will also be the cases that $ \lambda_1<\lambda_2<\cdots $ with $ \lambda_n\to \infty $ as $ n\to \infty $. Let's look at an example. Take
 \[
   \begin{cases}
	   -y''=\lambda y & 0<x<L \\
	   y(0)=y(L)=0
   \end{cases}
 \]
 so $ p=w=1 $ and $ q=0 $ and $ V=\{y\in C^2[0,L]: y(0)=y(L)=0\} $.\par
 Solving $ y''+\lambda y= 0 $ then $ y=A\sin(\sqrt{\lambda}x)+B\cos(\sqrt{\lambda}x) $. If $ \lambda\le 0 $ we only get the trivial solution, so we must have that $ \lambda>0 $. If we use $ y(0)=0\implies B=0 $ and $ y(L)=0\implies A\sin(\sqrt{\lambda}L)=0 $ so other than the trivial solution, we have that
 \[
	 \sqrt\lambda = \frac{n\pi}L,\quad n=1,2,\dots
 \]
 So
 \[
	 \lambda = \left(\frac{n\pi}L\right)^2,\quad y_n(x)=\sin\left(\frac{n\pi x}L\right).
 \]
We can see that $ \lambda_n\to \infty $ and $ \lambda_1<\lambda_2<\cdots $ and
\begin{align*}
	\langle y_n,y_m\rangle &= \int_0^L\sin\left(\frac{n\pi x}L\right)\sin\left(\frac{m\pi x}L\right)\mathrm dx \\
			       &= \frac L2\delta_{nm}
\end{align*}
For $ f\in V $,
\begin{align*}
	f(x)&=\sum_{n=1}^\infty \hat f_n\sin\left(\frac{n\pi x}L\right)\\
	\hat f_n&=\frac{\langle f,y_n\rangle}{||y_n||^2}\\
		&= \frac 2L\int_0^Lf(x)\sin\left(\frac{n\pi x}L\right)\mathrm dx
\end{align*}
We have re-derived the Fourier sine series from the previous section.
\subsection{Reduction to Sturn-Lionville form}
Consider a general eigenvalue problem of the form
\begin{align*}
	\alpha(x)\frac{\mathrm d^2y}{\mathrm dx^2}+\beta(x)\frac{\mathrm dy}{\mathrm dx}+\gamma(x)y+\lambda y=0
\end{align*}
with $ \alpha(x)>0 $. Divide the equation by $ \alpha(x) $ and multiply by
\[
	I(x)=\exp\left[\int^x\frac{\beta(t)}{\alpha(t)}\mathrm dt\right]
\]
\begin{proposition}
  The equation
  \[
    \alpha(x)\frac{\mathrm d^2y}{\mathrm dx^2}+\beta(x)\frac{\mathrm dy}{\mathrm dx}+\gamma(x)y+\lambda y=0
  \]
  is equivalent to
  \[
	  -\frac{\mathrm d}{\mathrm dx}\left[p\frac{\mathrm dy}{\mathrm dx}\right] + qy=\lambda wy
  \]
  where
  \begin{enumerate}
	  \item $ p(x)=I(x) $,
	  \item $ q(x)=-\frac{I(x)\gamma(x)}{\alpha(x)} $,
	  \item $ w(x)=\frac{I(x)}{\alpha(x)} $.
  \end{enumerate}
\end{proposition}
\pf 
\begin{align*}
  -\frac{\mathrm d}{\mathrm dx}\left[p\frac{\mathrm dy}{\mathrm dx}\right] + qy-\lambda wy &= I\left[-\frac{\mathrm d^2y}{\mathrm dx^2}-\frac{\beta(x)}{\alpha(x)}\frac{\mathrm dy}{\mathrm dx}-\frac{\gamma(x)}{\alpha(x)}y-\frac{\lambda y}{\alpha(x)}\right]\\
\end{align*}
since $ I>0 $ we get that the equation is zero if and only if $ LHS=0 $.\qed\par
For an example consider
\begin{align*}
  \begin{cases}
	  y''=2y'+\lambda y= 0 & 0<x<1\\
	  y(0)=y'(1)=0
  \end{cases}
\end{align*}
So we have that 
\[
	I(x)=\exp\left[\int^x -\frac 21\right]=e^{-2x}
\]
So the ODE becomes
\[
	-\frac{\mathrm d}{\mathrm dx}\left[e^{-2x}\frac{\mathrm dy}{\mathrm dx}\right]=\lambda e^{-2x}y.
\]
So we get $ e^{-2x} $ as our weight function.\par
To solve put $ y\propto e^{-\alpha x} \implies \alpha=1\pm\sqrt{1-\lambda} $ So if $ \lambda \ne 1 $ we'll get solutions of the form
\[
	y=e^x\left[Ae^{x\sqrt{1-\lambda}}+Be^{-x\sqrt{1-\lambda}}\right]
\]
We need $ 1-\lambda < 0 $ for non-trivial solutions.\par
We can see $ y(0=0\implies B=0 $ and $ y'(1)=0\implies Ae\left[\sin \mu+\mu\cos\mu\right]=0 $ where $ \mu^2=\lambda -1 $ and $ \mu>0 $ \textit{wlog}. So $ \tan\mu=-\mu $. By plotting the graph we can see we have infinitely many solutions for the equation. Call $ \mu_1,\mu_2,\dots $ so we have $ \lambda_n=1+\mu_n^2 $. From the graph we have that $ \mu_n\to\infty $ hence $ \lambda_n\to \infty $. The corresponding eigenfunctions are
\[
	y_n(x)=e^x\sin(\mu_nx),\quad n=1,2,\dots
\]
Check that $ \langle y_n,y_m\rangle\propto \delta_{nm} $. For $ n\ne m $
\begin{align*}
	\langle y_n,y_m\rangle_w &= \int_0^1e^x\sin(\mu_n x)e^x\sin(\mu_m x)e^{-2x}\mathrm dx\\
				 &= \int_0^1 \sin(\mu_n x)\sin(\mu_m x)\mathrm dx\\
				 &=\frac 12\int_0^1\left[\cos((\mu_n-\mu_m)x)-cos((\mu_n+\mu_m)x)\right]\mathrm dx\\
				 &\quad\vdots\\
				 &= 0
\end{align*}
(ommitting a large amount of the algebra.)
\subsection{Legendre's Equation}
Consider an eigenvalue problem defined as
\[
	-\frac{\mathrm d}{\mathrm dx}\left[(1-x^2)\frac{\mathrm d y}{\mathrm dx}\right]=\lambda y\quad -1<x<1
\]
So $ p=1-x^2 $, $ q=0 $, $ w=1 $. Since both endpoints are singular, work on $ V=C^2[-1,1] $. Since $ x=0 $ is a regular point we can look for solutions in the form
\[
	y=\sum_{n=0}^\infty a_nx^n.
\]
By subbing in, we get that
\[
	a_{n+2}=\left[\frac{n(n+1)-\lambda}{(n+1)(n+2)}\right]a_n
\]
Which gives two linearly independent solutions.
\begin{align*}
	y_0&=a_0 \left[1+\frac{(-\lambda)x^2}{2!}+\frac{(-\lambda)(6-\lambda)x^3}{4!}+\cdots\right]\\
	y_1&=a_1 \left[x+\frac{(2-\lambda)x^3}{3!}+\frac{(2-\lambda)(12-\lambda)x^5}{5!}+\cdots\right].
\end{align*}
Note that $ y_0 $ collapses if $ \lambda=0,6 $. In general if $ \lambda=k(k+1) $ for $ k=0,1,2,\dots $ either $ y_0 $ or $ y_1 $ gives a polynomial. What if $ \lambda\ne k(k+1) $? Since the ratio $\left|\frac{a_{n+2}}{a_n}\right|\to 1 $ we know that both series will converge on $ |x|<1 $. This doesn't tell us about $ y(\pm 1) $. Let's look at $ y_0 $ only, $ y_1 $ is treated similiar. Let $ A_n=a_{2n} $, so
\[
	\frac{A_n}{A_{n+1}}=\frac{(2n+1)(2n+2)}{2n(2n+1)-\lambda}=1+\frac 1n+\varepsilon_n
\]
where $ |\varepsilon_n|\le M/n^2 $, $ M=M(\lambda)>0 $. In particular the RHS true for $ n $ sufficiently large, say $ n\ge N $. So $ \{A_n\} $ have the same sign for $ n\ge N $. Using $ e^x > 1+x $ for all $ x\in \R $ we get that
\begin{align*}
	\frac{|A_n|}{|A_{n+1}|}&\le e^{1/n}+|\varepsilon_n|\\
	\implies |A_{n+1}|&\ge \frac{e^{-1/n}|A_n|}{1+e^{-1/n}|\varepsilon_n|}\\
	\ge \frac{e^{-1/n}|A_n|}{1+|\varepsilon_n|}&\ge e^{-1/n}|A_n|e^{-|2n}
\end{align*}
So for $ n\ge N $ we can repeat to get that
\[
	|A_{n+1}|\ge |A_n|\exp\left[-\left(\frac 1n+ \frac 1{n-1}+\cdots + \frac 1N\right)-\left(|\varepsilon_n|+\cdots + |\varepsilon_N|\right)\right]
\]
hence we have that
\begin{align*}
	|A_{n+1}|\ge |A_N|e^{-H_n+H_{N-1}}e^{-M\pi^2/6}.
\end{align*}
Since $ H_n\le \log n +2\gamma $
\begin{align*}
	|A_{n+1}|&\ge |A_N|e^{H_{N-1}-M\pi^2/6}e^{-\log n-2\gamma}\\
		 &> \frac{c}{n+1}
\end{align*}
Hence we have that
\begin{align*}
	y_0=\sum_{n\le N }A_nx^{2n}+\sum_{n>N}A_nx^{2n}
\end{align*}
and since $ \{A_n\} $ have the same sign, assume all positive \textit{wlog}. Note that
\begin{align*}
	\sum_{n>N}A_nx^{2n}&>c\sum_{n=1}^\infty\frac{x^{2n}}n-\sum_{n\le N}\frac{x^{2n}}n.\\
			   &=C\left[\log\left(\frac1{1-x^2}\right)-\text{ (some polynomial in x)}\right]\to \infty\quad \text{as }x\to \pm 1.
\end{align*}
So $ y_0\notin V $ so we must have that $ \lambda_k=k(k+1) $. This gives an even polynomial of degree $ k $ from $ y_0 $. Make normalisation so $ y(1)=1 $ choosing $ a_0 $ and $ a_1 $ accordingly then the solutions are called Legendre polynomials.
\subsection{Bessel's Equation}
Fix an integer $ n\ge 0 $. Consider the eigenvalue problem
\[
	-\frac{\mathrm d}{\mathrm dr}\left[r\frac{\mathrm dy}{\mathrm dx}\right]+\frac {m^2}ry=\lambda ry
\]
with $ 0<r<1 $ and $ y(1)=0 $. We have $ p=r,q=\frac{m^2}r,w=r $. Expanding out derivatives gives that
\[
  r^2y''+ry'+(\lambda r^2-m^2)y=0.
\]\smallskip
Set $ z=\sqrt{\lambda}r $ (we can show that $ \lambda>0 $). Set $ R(z)=y(r) $. This gives that
\[
	z^2R''+zR'+(z^2-m^2)R=0 \quad 0<z<\sqrt{\lambda}, R(\sqrt{\lambda})=0
\].
This is \textit{Bessel's equation of order} $ m $.
Since $ x=0 $ is a regular singular point, get can get solutions in the form \[
	z\to z^\sigma\sum_{n=0}^\infty a_nz^n
\]
by Fuch's theorem. We get two linearly independent solutions only one of which is non-singular as $ z\to 0 $. Label the cooresponding solution $ R=J_m(z) $. We can show that
\[
	J_m(z)=\left(\frac z2\right)^m\sum_{n=0}^\infty\frac{(-1)^n}{n!(n+m)!}\left(\frac z2\right)^{2n}.
\]
These are \textit{Bessel functions of the first kind} of order $ m $. We can show that $ J_n(z) $ has infinitely many zeros on the $ z $ axis, we label them $ j_{mk} $. Since we require that $ J_m(\sqrt{\lambda})=0 $, solutions to the equation are
\[
	y_k(r)=J_m(j_{mk}r),\quad \lambda_k=j_{mk}^2
\]
for $ k=1,2,3,\dots $
\subsection{Inhomogeneous Problems}
Let $ L $ be a Sturm-Liouville operator. Consider problems of the form
\begin{align*}
	\text{find } y\in V : Ly=f\in V.
\end{align*}
\textit{wlog}, $ w=1 $. Let $ \{y_k\} $ be normalised eigenfunctions of $ L $. Be completeness we can write that
\begin{align*}
	y&=\sum A_ky_K,\quad f=\sum B_ky_k\\
	 &\implies \sum_{k=1}^\infty (\lambda_kA_k-B_k)y_k=0\\
	 &\implies \lambda_kA_k=B_k\quad k=0,1,2,\dots
\end{align*}
So if $ \lambda_k\ne 0 $, $ A_k=B_k/\lambda_k $, we get have
\[
	y(x)=\sum_{k=1}^\infty\frac{B_k}{\lambda_k}y_k(x),\quad B_k=\int_a^bf(\xi)y_k(\xi)\mathrm d\xi
\]
Putting the $ B_k $ into $ y $ and changing sums and integrals we get that
\[
	y(x)=\int_a^bG(x;\xi)f(\xi)\mathrm d\xi
\]
where
\[
	G(x;\xi)=\sum_{k=1}^\infty\frac{y_k(\xi)y_k(x)}{\lambda_k}
\]
is called the Green's function.
\section{Linear PDEs and Seperation of Variables}
\subsection{Superposition}
We will be interested in solving boundary value problems (BVP) and initial boundary value problems (IBVP)
\[
	(\dagger) \begin{cases}
	  P\psi(\mathbf x)=0&\mathbf x\in\Omega\\
	  \text{some B.Cs} & \mathbf x\in\partial\Omega
  \end{cases}
\]
\[
	(\dagger\dagger) \begin{cases}
		Q\phi(\mathbf x,t)=0&(\mathbf x,t)\in \Omega\times (0,T)\\
		\text{some I.Cs} & (\mathbf x,t)\in \Omega\times\{t=0\}\\
		\text{some B.Cs} & (\mathbf x,t)\in\partial\Omega\times (0,T)
	\end{cases}
\]
where $ P,Q $ are \textit{linear} partial differentiable operators and $ \Omega $ will be bounded on an open subset of $ \R^n $ for $ n=1,2,3 $.
\begin{remark}
  We can split $ (\dagger\dagger) $ into
  \[
    \begin{cases}
	    Q\phi_1(\mathbf x,t)=0 \quad Q\phi_2(\mathbf x,t)=0 & (\mathbf x,t)\in\cdots \\
	    \text{I.Cs} =0 \quad \text{ some I.Cs} & \vdots \\
	    \text{Some B.Cs} \quad \text{B.Cs} = & (\mathbf x,t)\in \cdots 
    \end{cases}
  \]
\end{remark}
In this course, $ \Omega $ will always be (possibly after a change of variables) a line/rectangle/cuboid. So \textit{wlog} we can always deal with B.Cs that are zero everywhere apart from on one endpoint/edge/face. For example
\[
	(\dagger\dagger\dagger)=\begin{cases}
		P\psi(\mathbf x)=0& \mathbf x\in (0,1)\times (0,1) \\
		\psi = f_1 & \text{ on side }i\text{ for } i = 1,2,3,4
	\end{cases}
\]
We could look at 4 problems for $ \{\psi_i\}_{i=1}^4 $
\[
  \begin{cases}
	  P\psi_i(\mathbf x)=0 & \mathbf x\in (0,1)\times (0,1) \\
	  \psi_i=0 & \text{on side } \ne i \\
	  \psi_i=f_i & \text{on side } = i
  \end{cases}
\]
Then $ \psi=\psi_1+\cdots+\psi_4 $ solves $ (\dagger\dagger\dagger) $.
\subsection{Laplace's Equation}
Recall for $ \varphi:\R^n\to \R $, Laplace's equation is
\[
  \Delta \varphi = 0
\]
where $ \Delta=\nabla\cdot\nabla=\nabla^2 $. So on the Cartesian coordinates,
\[
	\Delta = \frac{\partial^2}{\partial x^2}+\cdots + \frac{\partial^2}{\partial z^2}.
\]
We say that $ \varphi $ is \textit{harmonic} if $ \Delta\varphi=0 $. Harmonic functions are always infinitely differentiable. Let's look at an example.\par
If $ \mathbf u:\R^3\to \R^3 $ is the velocity of an incompressible fluid (so $ \nabla\cdot \mathbf u=0 $) that is irrotational, i.e. $ \nabla\times \mathbf u = 0 $ then we can solve Laplace's equation. Since $ \mathbf u $ is irrotational on the whole of $ \R^3 $ there exists a scalar potential such that $ \mathbf u =\nabla \varphi $. Then $ \Delta\varphi=\nabla\cdot(\nabla \varphi)=\nabla\cdot\mathbf u= 0$.\par
We will consider  BVPs of the following
\[
  \begin{cases}
	  \Delta\varphi(\mathbf x)=0&\mathbf x \in \Omega\\
	  B\varphi=f(\mathbf x)& \mathbf x \in \partial\Omega
  \end{cases}
\]
where $ B\varphi\equiv \varphi $ (Dirichlet) or $ B\varphi=\frac{\partial \varphi}{\partial \mathbf n} $ (Neumann), or even $ B\varphi = \varphi + \frac{\partial \varphi}{\partial\mathbf n} $ (Robin).
\subsubsection{Serpation of variables on the square}
Consider
\[
  \begin{cases}
	  \varphi_{xx}+\varphi_{yy}=0 & (x,y)\in(0,1)\times (0,1) \\
	  \varphi(x,y)=0 & \text{on } x=0,x=1,y=0\\
	  \varphi(x,1)=f(x) & \text{otherwise}
  \end{cases}
\]
Try a separable solution of the form $ \varphi=X(x)Y(y) $. Then we get that
\[
  X''(x)Y(y)+X(x)Y''(y)=0.
\]
Dividing through by $ XY $ gives that
\begin{align*}
	\frac{X''(x)}{X(x)}+\frac{Y''(y)}{Y(y)}=0,
\end{align*}
hence there exists a $ \lambda\in\R $ such that
\[
	\frac{X''}X = -\lambda,\quad \frac{Y''}Y=\lambda.
\]
Since $ \varphi=0 $ at $ x=0,1 $ looking just at the $ X $-equation we get that
\[
  \begin{cases}
	  X''+\lambda X=0 & 0< x< 1\\
	  X(0)=0\\
	  X(1)=0
  \end{cases}.
\]
This is a Sturm-Liouville problem. So there exists $ (X_n,\lambda_n)^\infty_{n=1} $ solutions with $ \lambda_1<\lambda_2<\cdots $ and $ \langle X_n,X_m\rangle=\int_0^1X_n(x)X_m(x)\mathrm dx\propto \delta_{nm} $.\par
We check that we require $ \lambda>0 $ for non-trivial solutions. General solutions are
\[
	X(x)=A\sin(\sqrt{\lambda}x)+B\cos(\sqrt{\lambda}x)
\]
with $ X(0)=0\implies B=0 $ and $ X(1)=0\implies A\sin(\sqrt{\lambda})=0\implies \lambda=\lambda_n=(n\pi)^2 $. So we get that $ X_n(x)=\sin(n\pi x) $ with eigenvalues $ \lambda_n=(n\pi)^2 $ and that $ \langle X_n,X_m\rangle =\frac 12\delta_{nm} $. The $ Y $ problem becomes $ Y''-(n\pi)^2Y=0 $ with $ Y(0)=0 $ which gives that
\[
  Y=A\sinh(n\pi y)+B\cosh(n\pi y).
\]
Now $ Y(0)=0\implies B=0 $, so $ Y_n(y)=A_n\sinh(n\pi y) $. So we have functions $ \{\varphi_n\}_{n=1}^\infty $ with
\[
  \varphi_n(x,y)=A_n\sin(n\pi x)\sinh(n\pi y)
\]
and each satisfies $ \Delta\varphi_n=0 $ in $ (0,1)\times (0,1) $ and $ \varphi_n=0 $ on $ x=0,x=1,y=0 $. So same is true for their sum
\[
	\varphi(x,y)=\sum_{n=1}^\infty A_n\sin(n\pi x)\sinh(n\pi y).
\]
We still want that $ \varphi(x,1)=f(x) $ so we set
\[
	f(x)=\sum_{n=1}^\infty A_n\sin(n\pi x)\sinh(n\pi)
\]
we need to find the $ A_n $s to use get equality.\par
By orthogonality
\begin{align*}
	\langle f,X_n\rangle &= \sum_{n=0}^\infty A_n{\langle X_n,X_m\rangle}\sinh(m\pi)\\
	&= \frac{A_n}2\sinh(n\pi)
\end{align*}
So our final solution is
\[
	\varphi(x,y)=\sum_{n=1}^\infty A_n\sin(n\pi x)\sinh(n\pi y)
\]
where
\begin{align*}
	A_n &=\frac 2{\sinh(n\pi)}\langle f,X_n\rangle\\
	    &= \frac2{\sinh(n\pi)}\int_0^1f(x)\sin(n\pi x)\mathrm dx
\end{align*}
\subsubsection{Seperation of variables in a disc/annulus}
We want to solve Laplace's equation in the region $ r_1<|\mathbf x|<r_2 $ in the $ (x,y) $ plane. Use plane polar coordinates $ (r,\theta) $ so that Laplace's equation becomes
\[
	\Delta\varphi=\frac 1r\frac{\partial}{\partial r}\left(r\frac{\partial\varphi}{\partial r}\right)+\frac1{r^2}\frac{\partial ^2 \varphi}{\partial \theta^2}=0.
\]
Look for seperable solutions in the form $ \varphi=R(r)\Theta(\theta) $. So
\[
	\frac{r(rR')'}R+\frac{\Theta''}\Theta=0
\]
so there exists a $ \lambda\in \R $ such that
\begin{align*}
	r(rR')' &= \lambda R\\
	\Theta'' + \lambda \Theta = 0.
\end{align*}
The solution to the $ \Theta $-equation is going to be
\[
  \Theta(\theta)=\begin{cases}
	  A\cos(\sqrt\lambda\theta) + B\sin(\sqrt\lambda\theta) & \lambda >0 \\
	  A\theta +B & \lambda =0 \\
	  A\cosh(\sqrt\lambda\theta) + B\sinh(\sqrt\lambda\theta) & \lambda <0
  \end{cases}.
\]
We need $ \Theta(\theta+2\pi)=\Theta(\theta) $ for all $ \theta $. This forces $ \Theta $ to be a constant or $ \lambda>0 $ with $ \lambda = n^2 $ for $ n\in \N \cup \{0\} $. So
\begin{align*}
	\Theta_0(\theta)&=A\\
	\Theta_n(\theta)&=C_n\cos(n\theta)+D_n\sin(n\theta)
\end{align*}
Plug $ \lambda=n^2 $ into the $ R $-equation. So
\begin{align*}
  r(rR')'=n^2R
\end{align*}
For $ n=0 $ we have that $ rR' $ is constant, integrating gives that
\[
  R_0(r)=A+B\log r.
\]
And for $ n>0 $ we try a solution in the form $ R(r)=r^\alpha $. Plugging this gives that $ \alpha^2=n^2 $ so $ \alpha=\pm n $,
\[
	R_n(r)=A_nr^n+B_nr^{-n}
\]
so we have the general solution
\[
	\varphi(r,\theta)=A+B\log r+\sum_{r=1}^\infty\left[A_nr^n+B_nr^{-n}\right]\left[C_n\cos(n\theta)+D_n\sin(n\theta)\right].
\]
If the point $ r=0 $ belongs to our domain we have to throw out the $ r^{-n} $ and $ \log r $ terms since they're not defined. So we must take $ B=B_n=0 $ for each $ n $.\par
Let's see an example.\\
Solve
\[
  \Delta\varphi=0\quad r_1<r<r_2
\]
with $ \varphi=0 $ on $ r=r_1 $ and $ \varphi=f(\theta) $ on $ r=r_2 $. We can repeat analysis but require $ R_n(r)=0 $ where $ r=r_1 $ for $ n=0,1,2,\dots $. Then $ R_0(r)=\log\left(\frac rr_1\right) $ and $ R(r)=\left(\frac r{r_1}\right)^n-\left(\frac{r_1}r\right)^n $ to get solutions of the form
\[
	\varphi(r,\theta)=C_0\log\left(\frac r{r_1}\right)+\sum_{n=1}^\infty \left[\left(\frac r{r_1}\right)^n-\left(\frac{r_1}r\right)\right]\left[A_n\cos(n\theta)+B_n\sin(n\theta)\right].
\]
The boundary condition $ r=r_2 $ gives that
\[
	f(\theta)=C_0\log\left(\frac {r_2}{r_1}\right)+\sum_{n=1}^\infty \left[\left(\frac {r_2}{r_1}\right)^n-\left(\frac{r_1}{r_2}\right)\right]\left[A_n\cos(n\theta)+B_n\sin(n\theta)\right].
\]
which can be written as
\[
	f(\theta)=\frac 1na_0+\sum_{n=0}^\infty \left[ a_n\cos(n\theta) + b_n\sin(n\theta)\right]
\]
i.e. the right hand side should be a Fourier series for $ f $. So $ a_n $ and $ b_n $ are given by
\begin{align*}
	a_n&=\frac 1\pi\int_0^{2\pi} f(\theta)\cos(n\theta)\mathrm d\theta \\
	b_n&= \frac 1\pi\int_0^{2\pi} f(\theta)\sin(n\theta)\mathrm d\theta.
\end{align*}
For a more general problem, where $ \varphi $ takes the values of a function $ g(\theta) $ on the interior $ r=r_1 $, we write that $ \varphi=\varphi_1+\varphi_2 $ where $ \varphi_i=0 $ on $ r_i $ and use superposition to split our task up for two Fourier series computations.
\subsubsection{Seperation of variables on a ball/shell (anti-symmetric case)}
We want to solve $ \Delta\varphi=0 $ in the region $ a<|\mathbf x|<b  $ in $ \R^3 $ but under the restriction that the problem is symmetric about the $ z $-axis. We'll work in spherical polar coordinates, so
\begin{align*}
	\frac 1{r^2}\frac\partial{\partial r}\left[r^2\frac{\partial \varphi}{\partial r}\right]+\frac 1{r^2\sin\theta}\frac{\partial}{\partial \theta}\left[\sin\theta\frac{\partial \varphi}{\partial\theta}\right]+\frac{1}{r^2\sin^2\theta}\frac{\partial^2\varphi}{\partial \phi^2}=0.
\end{align*}
Here the last term vanishes using symmetry about the $ z $-axis. Look for a solution in the form $ \varphi=R(r)\Theta(\theta) $ so
\[
	\frac{[r^2R']'}R+\frac1 {\Theta\sin\theta}[\sin\theta\cdot\theta']'=0
\]
so there exists a $ \lambda\in\R $ such that
\begin{align*}
	[r^2R']'=\lambda R-[\sin\theta\cdot \Theta']'=\lambda\sin\theta\cdot\Theta
\end{align*}
which then can be solved using the techniques discussed earlier.
\subsection{Wave Equation}
Consider a taut string, under constant tension $ \tau $ clamped at ends $ x=0 $ and $ x=L $. Let $ y =y(x,t)$ denote vertical displacement of the string. Assume that oscillations are transverse and that the slope $ |y_x|\ll 1 $. Note that
\begin{align*}
|y(x,t)|&=\left|\int_0^x y_x(s,t)\mathrm ds\right|\\
	&\le \int_0^L |y_x|\mathrm ds \ll 1.
\end{align*}
Take a diagram of a string with tension $ \tau $ arcing upwards. Let there be two points $ x_A $ and $ x_B $ with midpoint $ x $. Let $ \theta_A $ and $ \theta_B $ be the subtended angles from the string to the horizontal respectively. Then resolving forces horizontally we get that (given no transverse motion) that
\[
  \tau\cos\theta_B-\tau\cos\theta_A=0.
\]
This is consistant since that $ \tan\theta_A=\left(\frac{\partial y}{\partial x}\right)_A $ so we get that $ \cos\theta_A=\frac1{\sqrt{1+(\frac{\partial y}{\partial x})_A^2}}\approx 1 $. Also the mass of the string is proportional to its length so
\[
	\int_{x_A}^{x_B}\mathrm ds=\int_{x_A}^{x_B}\sqrt{1+y_x^2}\mathrm dx\approx \delta x.
\]
So reasonable to assume that mass is $ \mu\delta x $ where $ \mu>0 $ is density. Resolving vertically and using Newton's second law gives that
\[
	\frac{\mu\delta x}{\tau}\frac{\partial^2y}{\partial x^2}=\frac{\tau\sin\theta_B}{\tau\cos\theta_B}-\frac{\tau\sin\theta_A}{\tau\cos\theta_A}=\left(\frac{\partial y}{\partial x}\right)_B-\left(\frac{\partial y}{\partial x}\right)_A.
\]
Divide by $ \tau $ to ge that $ \tau=\tau\cos\theta_A=\tau\cos\theta_B $ to leading order. Hence by MVT we get that
\[
	\frac{\mu}\tau\delta x\frac{\partial ^2y}{\partial^2t}=\frac{\partial^2y}{\partial x^2}\delta x.
\]
where the first partial is evaluated at $ x $ and the second is evaluated at some $ \xi\in (x_A,x_B) $. Divide by $ \delta x $ and take $ \delta x\to 0 $ and $ \xi\to x $ giving that 
\[
	\frac 1{c^2}\frac{\partial^2y}{\partial t^2}=\frac{\partial ^2y}{\partial x^2},\quad c^2=\frac \tau\mu.
\]
call $ c $ the wave speed. We have boundary conditions given by $ y(0,t)=y(L,t)=0 $ and Newton's second law gives initial conditions that $ y(x,0)=f(x) $ and $ y_t(x,0)=g(x) $.
\subsubsection{Waves on a string}
Solve IBVP
\[
  \begin{cases}
	  y_tt -c^2y_xx=0 & (x,t)=(0,L)\times (0,\infty) \\
	  y(0,t)=0 & t\in (0,\infty) \\
	  y(L,t)=0 & t\in (0,\infty)\\
	  y(x,0)=f(x) & x\in (0,L) \\
	  y_t(x,0)=g(x) & x\in (0,L)
  \end{cases}
\]
Try a solution in the form $ y=X(x)T(t) $ with $ X(0)=0 $ and $ X(L)=0 $. We get that
\[
	\frac{\ddot{T}}{c^2T}=\frac{X''}X
\]
so there must exists some $ \lambda\in \R $ such that we have that
\begin{align*}
  X''+\lambda X=0\\
  X(0)=X(L)=0
\end{align*}
and
\begin{align*}
  \ddot T+\lambda c^2T=0.
\end{align*}
The solutions to the $ X $ equation is a S-L problem, which has known solutions with
\[
	X_n(x)=\sin(\frac{n\pi x}L),\quad \lambda_n=\left(\frac{n\pi}L\right)^2,\quad n=1,2,\dots
\]
The $ T $-equation becomes
\[
	\ddot T+\left(\frac{n\pi c}L\right)^2T=0
\]
which we solve as
\[
	T_n(t)=A_n\cos(\frac{n\pi ct}L)+B_n\sin(\frac{n\pi ct}L)
\]
so by superposition,
\begin{align*}
	y(x,t)=\sum_{n=1}^\infty \sin(\frac{n\pi x}L)\left[A_n\cos(\frac{n\pi ct}L)+B_n\sin(\frac{n\pi ct}L)\right]
\end{align*}
solves the wave equation with $ y(0,t)=y(L,t)=0 $. The initial conditions gives that
\[
	f(x)=\sum_{n=1}^\infty A_n\sin(\frac{n\pi x}L),\quad g(x)=\sum_{n=1}^\infty \left(\frac{n\pi c}L\right)B_n\sin(\frac{n\pi x}L).
\]
which look like the sine series. By orthogonality $ \langle X_n,X_m\rangle=\frac L2\delta_{nm} $, so
\begin{align*}
	A_n&=\frac 2L\int_0^Lf(x)\sin(\frac{n\pi x}L)\mathrm dx\\
	B_n&=\left(\frac{L}{n\pi c}\right)\frac 2L\int_0^Lg(x)\sin(\frac{n\pi x}L)\mathrm dx
\end{align*}
\subsubsection{Waves on a drum}
The higher dimensional analogue of the wave equation is
\[
	\frac 1{c^2}\frac{\partial^2\varphi}{\partial t^2}=\Delta \varphi
\]
Solving the IBVP on a drum,
\[
	\Omega=\{(r,\theta): 0\le r < 1, 0\le \theta < 2\pi\}
\]
\[
  \begin{cases}
	  \varphi_{tt}-c^2\Delta \varphi=0 & \Omega\times (0,\infty)\\
	  \varphi=0 & \partial\Omega\times(0,\infty)\\
	  \varphi=f & \Omega\times{t=0}\\
	  \varphi_t=g & \Omega\times {t=0}
  \end{cases}
\]
For simplicity assume that $ f=f(r) $, $ g=g(r) $. So that $ \varphi=\varphi(r,t) $. Wave equation becomes
\begin{align*}
	\frac 1{c^2}\frac{\partial^2 \varphi}{\partial t^2}=\frac 1r\frac{\partial}{\partial r}\left(r\frac{\partial \varphi}{\partial r}\right)+\frac1{r^2}\frac{\partial^2 \varphi}{\partial \theta^2}.
\end{align*}
Where the last term is zero since the solution is independent of $ \theta $. So we can try a solution in the form $ \varphi=R(r)T(t) $. Hence
\[
	\frac{\ddot T}{c^2 T}=\frac{[rR']'}{rR}.
\]
So there exists a $ \lambda\in \R $ such that
\begin{align*}
	-\frac{\mathrm d}{\mathrm dr}\left[r\frac{\mathrm R}{\mathrm r}\right]&= \lambda rR \qquad 0<r < 1, R(1)=0\\
	\ddot T+c^2\lambda T &= 0.
\end{align*}
So the $ R $-equation is the Bessel problem with $ m=0 $. This gives solutions
\[
	R_k(r)=J_0(j_{0k}r),\quad\lambda_k=j_{0k}^2.
\]
The $ T $-equation becomes
\[
	\ddot T+(cj_{0k})^2T=0
\]
so we get a solution in the form
\[
	\varphi(r,t)=\sum_{k=1}^\infty J_0(j_{0k}r)\left[A_k\cos(j_{0k}ct)+B_k\sin(j_{0k}ct)\right]
\]
which solves the PDE and boundary conditions. The initial conditions are
\begin{align*}
	f(r)=\sum_{k=1}^\infty A_kJ_0(j_{0k}r)\\
	g(r)=\sum_{k=1}^\infty B_kj_{0k}cJ_0(j_{0k}r)
\end{align*}
Recall that $ \langle R_k,R_\ell\rangle_w=\int_0^1 J_0(j_{0k}r)J_0(j_{0\ell r})r\mathrm dr = \frac 12 J_0'(j_{0k})^2\delta_{k\ell} $, so by orthogonality we get that
\begin{align*}
	A_k&=\frac 2{J_0'(j_{0k})}^2\int_0^1f(r)J(j_{0k}r)r\mathrm dr\\
		B_k&=\frac 2{J_0'(j_{0k})^2}\frac 1{cj_{0k}}\int_0^1g(r)J_0(j_{0k}r)r\mathrm dr
\end{align*}
\par
Missed Lecture - 03.11.25
\par
\subsection{The Heat Equation}
The temperature of a conductive material, $ \varphi(\mathbf x,t) $ satisfies the \textit{heat equation} which is
\[
	\frac{\partial \varphi}{\partial t}=\kappa\Delta\varphi
\]
where $ \kappa>0 $ is a constant. We are interested in the IBVP
\[
	(\dagger\dagger) \begin{cases}
	  \varphi_t-\kappa\Delta\varphi=0 & \Omega\times (0,\infty) \\
	  \varphi = 0 & \partial\Omega\times (0,\infty) \\
	  \varphi = f & \Omega \times \{t=0\}
  \end{cases}.
\]
We'll try a solution in the form $ \varphi=T(t)\psi(\mathbf x) $. Plugging in, for some $ \mu \in \R $ we get that 
\[
  \dot T+\kappa\mu = 0
\]
and \[ -\Delta\psi = \mu \psi \].
We get that $ T(t)=e^{-\kappa\mu t} $. We can see that $ T $ does not vanish on the boundary (unless $ T $ is trivial), hence we can impose the vanishing boundary condition onto $ \psi $. So we're now solving
\[
  \begin{cases}
	  -\Delta\psi = \mu\psi & \mathbf x\in \Omega\\
	  \psi = 0 & \mathbf x\in \partial \Omega
  \end{cases}.
\]
Solutions to this depend on the geometry of $ \Omega $.
\subsubsection{Heat conduction on a square sheet}
Take $ \Omega=\{(x,y)\in (0,L)\times (0,L)\} $. Try a solution seperable in $ x $ and $ y $, $ \psi=X(x)Y(y) $. So we get that
\[
	\frac{X''}X+\frac{Y''}Y+\mu = 0.
\]
So there exists a $ \lambda\in \R $ such that $ X''+\lambda X=0 $ and $ Y''+(\mu-\lambda)Y=0 $. We also have the requirement that $ X(0)=X(L)=Y(0)=Y(L)=0 $. The $ X $-equation gives that
\[
	X_n(x)=\sin\left(\frac{n\pi x}L\right),\quad \lambda_n=\left(\frac{n\pi}L\right)^2
\]
and the $ Y $-equation gives that
\[
	Y=A\sin(y\sqrt{\mu-\lambda_n})+B\cos(y\sqrt{\mu-\lambda_n})
\]
So $ Y(0)=0\implies B=0 $ and $ Y(L)=0 $ gives that
\[
	\mu_{mn}-\lambda_n=\left(\frac{m\pi}L\right)^2
\]
so
\[
	Y_n(y)=\sin(\frac{m\pi y}L), \quad \mu_{mn}=\left(\frac{n\pi}L\right)^2+\left(\frac{m\pi}L\right)^2
\]
so by superposition we get that
\[
	\varphi(x,y,t)=\sum_{m,n=1}^\infty A_{mn}e^{-\kappa \mu_{mn}t}\sin(\frac{n\pi x}L)\sin(\frac{m\pi y}L)
\]
satisfies the heat equation and the boundary condition $ \varphi=0 $ on $ \partial \Omega $. Now to impose the initial condition,
\[
	f(x,y)=\sum_{n,m=1}^\infty A_{mn}\sin(\frac{n\pi x}L)\sin(\frac{m\pi y}L).
\]
So using $ \langle X_n,X_k\rangle =\frac 12 \delta_{nk} $ we get that
\[
	A_{mn}=\frac 4{L^2}\int_0^L\mathrm dx\int_0^L\mathrm dy f(x,y)\sin(\frac{n\pi x}L)\sin(\frac{m\pi y}L).
\]
\subsubsection{Heat flow down a pipe}
In cylindrical polars,
\[
	\Omega=\{(\rho,\phi,z):0\le \rho < 1, 0\le \phi<2\pi, 0<z<L\}
\]
Want to solve
\[
	(\dagger\dagger) \begin{cases}
	  \varphi_t-\kappa\Delta\varphi=0 & \Omega\times (0,\infty) \\
	  \varphi = 0 & \partial\Omega\times (0,\infty \\
	  \varphi = f & \Omega \times \{t=0\}
  \end{cases}.
\]
For simplicity we'll assume that $ f=f(\rho, z) $. We'll look for solutions $ \varphi=T(t)\psi(\rho,z) $. So
\begin{align*}
  &-\Delta\psi = \mu \varphi \\
	\implies &\frac 1\rho\frac{\partial}{\partial \rho}\left(\rho\frac{\partial \psi}{\partial \rho}\right)+\frac{\partial ^2 \psi}{\partial z^2}+\mu \psi=0
\end{align*}
We'll try $ \psi(\rho,z)=P(\rho)Z(z) $. So there exists a $ \lambda\in \R $ such that
\[
	-\frac 1{\rho P}[\rho P']'=\lambda,\quad Z''+(\mu-\lambda)Z=0.
\]
i.e.
\[
	-\frac{\mathrm d}{\mathrm d\rho}\left[\rho\frac{\mathrm dP}{\mathrm d\rho}\right]=\lambda\rho P\quad 0<p<1,\quad P(1)=0
\]
This is Bessel's problem with $ m=0 $.
\begin{align*}
	P_k(\rho)=J_0(j_{0k}\rho),\quad k=1,2,\dots\\
	\lambda_k=j_{0k}^2.
\end{align*}
This $ Z $-equation becomes
\begin{align*}
  Z''+(\mu-\lambda_k)Z=0,\quad Z(0)=Z(L)=0
\end{align*}
Hence we get that
\[
	Z_n(t)=\sin(\frac{n\pi z}L)
\]
with $ \mu_{kn}-\lambda_k=\left(\frac{n\pi}L\right)^2 $ for $ n=1,2,\dots $. By superposition we get that
\[
	\varphi(\rho,z,t)=\sum_{n,k=1}^\infty B_{nk}e^{-\kappa \mu_{kn}t}J_0(j_{0k}\rho)\sin(\frac{n\pi z}L)
\]
is a solution to the heat equation, and the boundary condition $ \varphi=0 $ on $ \partial \Omega $. For the initial conditions we need,
\[
	f(\rho,t)=\sum_{n,k=1}^\infty B_{nk}J_0(j_{0k}\rho)\sin(\frac{n\pi z}L).
\]
Recall that
\[
	\int_0^1J_0(j_{0k}\rho)J_0(j_{0\ell}\rho)\rho\mathrm d\rho=\frac 12J_0'(j_{0k})^2\delta_{k\ell}
\]
We get that
\[
	B_{nk}=\frac 4{LJ_0'(j_{0k})^2}\int_0^L\mathrm d\rho \int_0^L\mathrm dz\rho f(\rho z)J_0(j_{0k}\rho)\sin(\frac{n\pi z}L)
\]
\subsubsection{Heat loss and uniqueness}
Recall again our problem
\[
	(\dagger\dagger) \begin{cases}
	  \varphi_t-\kappa\Delta\varphi=0 & \Omega\times (0,\infty) \\
	  \varphi = 0 & \partial\Omega\times (0,\infty \\
	  \varphi = f & \Omega \times \{t=0\}
  \end{cases}.
\]
is the solution to $(\dagger\dagger) $ unique? Define the energy $ Q $ of the system as
\[
	Q(t)=\frac 12\int_\Omega \varphi(\mathbf x,t)^2\mathrm dV.
\]
Then
\begin{align*}
	Q'(t)&=\int_\Omega \varphi_t\varphi\mathrm dV\\
	     &=\int_\Omega \kappa\varphi\Delta\varphi\mathrm dV\\
	     &=\kappa\int_\Omega\left[\nabla\cdot(\varphi\nabla\varphi)-|\nabla\varphi|^2\right]\mathrm dV\\
	     &= \kappa\int_{\partial \Omega}\varphi\frac{\partial \varphi}{\partial \mathbf n}\mathrm dS-\kappa\int_\Omega|\nabla \varphi|^2\mathrm dV
\end{align*}
The first integral is zero by the boundary condition, and the second integral is clearly non-negative, so $ Q'(t)\le 0 $. So $ Q(t)\le Q(0)=\frac 12 \int_\Omega f(\mathbf x)^2\mathrm dV $.
\begin{proposition}
  The solution to the problem in $ (\dagger\dagger) $ is unique.
\end{proposition}
\pf Suppose $ \varphi_1,\varphi_2 $ satisfy $ (\dagger\dagger) $ and set $ \varphi=\varphi_1-\varphi_2 $. Then $ \varphi $ satisfies $ (\dagger\dagger) $ with $ f=0 $. So
\[
  \int_\Omega\varphi(\mathbf x,t)^2\mathrm dV \le Q(0)=0
\]
hence $ \varphi(\mathbf x,t)=0 $ on $ \Omega\times(0,\infty) $ hence $ \varphi_1=\varphi_2 $.\qed
\section{Inhomogeneous Problems and Green's Functions}
\subsection{The Dirac Delta Function}
We say this mysterious function $ \delta(x) $ in IA Differential Equations. It has the properties that
\begin{align*}
	\delta(x)=0 &\quad\text{for } x\ne 0\\
	\forall \varepsilon>0 &\quad \int_{-\varepsilon}^\varepsilon\delta(x)\mathrm dx=1
\end{align*}
We can make this (slightly) rigorous by definiting a sequence of functions
\[
  \delta_n(x)=\begin{cases}
	  \frac n\alpha\exp\left[-\frac 1{1-n^2x^2}\right]& |x|<\frac 1n\\
	  0 & |x|\ge \frac 1n
  \end{cases}
\]
where
\[
	\alpha=\int_{-1}^1\exp\left[-\frac 1{1-y^2}\right]\mathrm dy.
\]
Then we have that
\begin{enumerate}
	\item $ x\to \delta_n(x) $ is a smooth function for each $ n $.
	\item $ \delta_n(x)=0 $ on $ |x|\ge \frac 1n $.
	\item $\forall\varepsilon>0,\ \exists N>0, \ \forall n>N $ we have that
		\[
			\int_{-\varepsilon}^\varepsilon\delta_n(x)\mathrm dx=1
		\]
\end{enumerate}
We'll prove (iii) since (i) and (ii) are obvious. Given some $ \varepsilon>0 $ take $ N=\frac 1\varepsilon $, so if $ n>N $ we have that $ \frac 1n<\varepsilon $, hence
\begin{align*}
	\int_{-\varepsilon}^\varepsilon\delta_n(x)\mathrm dx&=\int_{-\frac1n}^{\frac1n}\delta_n(x)\mathrm dx\\
							    &= \frac1\alpha\int_{-1}^1\exp\left[-\frac1{1-y^2}\right]\mathrm dy=1
\end{align*}
We can see that
\begin{align*}
	\lim_{n\to\infty}\delta_n(x)=0\quad\text{if } x\ne 0.
\end{align*}
And for all $ \varepsilon>0 $,
\[
	\lim_{n\to\infty}\int_{-\varepsilon}^\varepsilon\delta_n(x)\mathrm dx=1
\]
So it almost looks like $ \delta(x)=\lim_{n\to\infty}\delta_n(x) $. But the problem is that the pointwise limit doesn't converge at $ 0 $. But the limit does exist in a weaker sense. Note that $ \delta(x) $ rarely appears in isolation (we usually see it inside an integral or as part of a forcing term in a differential equation). We can interpret these as a sequence of statements that we will eventually take the limit of once the limiting behaviour becomes well-defined using the surrounding terms which makes the limit behave nicely. For example if we have the differential equation
\[
  \ddot y + \omega^2y=\delta(t)
\]
we can solve for $ y_n $, the solution to
$ \ddot y+\omega^2y=\delta_n(t) $, and then assert that $ y=\lim_{n\to\infty}y_n(t) $.\\
We could even look at the derivative of $ \delta(x) $ via
\begin{align*}
	\int_{-\infty}^\infty\delta'(x)f(x)\,\mathrm dx &= \lim_{n\to\infty}\int_{-\infty}^\infty \delta_n'(x)f(x)\,\mathrm dx\\
																											 &= -\lim_{n\to\infty}\int_{-\infty}^\infty \delta_n(x)f'(x)\,\mathrm dx\\
																											 &= -\int_{-\infty}^\infty \delta(x)f'(x)\,\mathrm dx = -f'(0)
\end{align*}
\subsubsection{Periodic delta functions}
Let $ \delta^L(x) $ denote the $ L $-periodic extension of $ \delta(x) $ outside $ [-\frac L2, \frac L2] $. It has Fourier coefficients given by
\begin{align*}
	\frac 1L\int_{-\frac L2}^{\frac L2}\delta(x)e^{-2\pi inx/L}\mathrm dx=\frac 1L
\end{align*}
So we have that
\[
	\delta^L(x)\sim \frac 1L\sum_n e^{2\pi inx/L}
\]
which also doesn't converge, which is good since now neither side makes sense. Let's try and chuck our Fourier series into where we would usually use a delta function.
\begin{align*}
	f(0)&=\int_{-\frac L2}^{\frac L2}\delta^L(x)f(x)\mathrm dx\\
	    &=\sum_n\left[\frac 1L\int_{-\frac L2}^{\frac L2}e^{2\pi in x/L}f(x)\mathrm dx\right]\\
	    &=\sum_n\hat f_{-n}=\sum_n\hat f_n=\sum_n \hat f_ne^{2\pi i n(0)/L}\\
	    &=f(0)
\end{align*}
Note that the Fourier series for each $ \delta_n^L(x) $ each converge (rapidly) since $ \delta_n^L\in C^\infty(\R) $.
\subsubsection{Eigenfucntion expansion of $ \delta(x) $}
Let $ L $ be a Sturm-Liouville operator on
\[
  V=\mathhuge\{
	  y\in C^2[a,b]: \text{ y satisfies real homogeneous B.Cs at each non-singular endpoint}
  \mathhuge\}
\]
Let $ \{Y_k\}_{k=1}^\infty $ be normalised eigenfunctions and fix $ \xi\in(a,b) $ and consider functions $ x\to \delta_n(x-\xi) $. For $ n $ sufficiently large $ \delta_n(x-\xi)\in V $. By completeness of the space $ V $ we should have
\begin{align*}
	\delta_n(x-\xi)&=\sum_{k=1}^\infty \langle \delta_n(\cdot -\xi),Y_k\rangle_wY_k(x)\\
		       &= \sum_{k=1}^\infty \left[\int_a^b\delta_n(t-\xi)Y_k(t)w(t)\mathrm dt\right]Y_k(x).
\end{align*}
Formally, letting $ n\to\infty $,
\begin{align*}
	\delta(x-\xi)=\sum_{k=1}^\infty w(\xi)Y_k(\xi)Y_k(x).
\end{align*}
Let's do a sanity check for this result. For $ f\in V $ we should have
\begin{align*}
	f(x)&=\int_a^b\delta(x-\xi)f(\xi)\mathrm d\xi\\
	    &= \sum_{k=1}^\infty\left[\int_a^bf(\xi)Y_k(\xi)w(\xi)\mathrm d\xi\right]Y_k(x)\\
	    &=\sum_{k=1}^\infty\hat f_kY_k(x),\quad \hat f_k=\langle f,Y_k\rangle_w
\end{align*}
\subsection{Green's Functions}
We want to solve
\[
	(\dagger)\begin{cases}
	  Ly=f(x) & a< x < b\\
	  y(a) = 0\\
	  y(b) = 0
  \end{cases}.
\]
Where $ L=\alpha(x)\frac{\mathrm d^2}{\mathrm dx^2}+\beta(x)\frac{\mathrm d}{\mathrm dx}+\gamma(x) $, with $ \alpha(x)\ne 0 $ on $ (a,b) $.
Fix $ \xi\in (a,b) $. Suppose we can find a function $ G=G(x,\xi) $ such that \[
  \begin{cases}
	  L_x[G(x,\xi)]=\delta(x-\xi) & a< x < b\\
	  G(a,\xi ) = 0\\
	  G(b,\xi ) = 0
  \end{cases}.
\]
Consider
\begin{align*}
	y(x)=\int_a^b G(x,\xi)f(\xi)\mathrm d\xi.
\end{align*}
Clearly we have that $ y(a)=y(b)=0 $ and
\begin{align*}
	L_x[y(x)]&=\int_a^bL_x[G(x,\xi)]f(\xi)\mathrm d\xi\\
		 &=\int_a^b\delta(x-\xi)f(\xi)\mathrm d\xi\\
		 &=f(x).
\end{align*}
So $ y $ solves $ (\dagger) $. We call $ G $ the Green's function for $ L $ with Dirichlet boundary conditions.\par
Let's look at some properties of $ G=G(x,\xi) $. Note on $ a<x<\xi $, $ \xi<x<b $, $ G $ satisfied $ L_x[G(x,\xi)]=0 $ which is a nice second order homogeneous ODE so we expect $ x\to G(x,\xi) $ to be well-behaved. In the neighbourhood of $ x=\xi $ the worst term on the LHS is
\[
	\alpha(x)\frac{\mathrm d^2 G}{\mathrm dx^2}
\]
and on the RHS the worst term is $ \delta(x-\xi) $. So we expect 
\[
	\alpha(x)\frac{\mathrm d}{\mathrm dx}\left[\frac{\mathrm d G}{\mathrm dx}\right]=\delta(x-\xi)+\text{(more regular terms)}
\]
What do we need for
\[
	\frac{\mathrm d}{\mathrm dx}\left[\cdots\right]\sim \delta(x-\xi)?
\]
Recall the sequence $ \{\delta_n\} $ were such that
\[
	\lim_{n\to \infty}\delta_n(x)=0\ x\ne 0,\quad \forall \varepsilon >0\ \lim_{n\to\infty}\int_{-\varepsilon}^\varepsilon \delta_n(x)\mathrm dx=1.
\]
So it's natural to look at
\[
	H_n(x)=\int_{-\infty}^x \delta_n(t)\mathrm dt,
\]
since $ H'_n(x)=\delta_n(x) $.\par
If we fix $ x>0 $ then for $ n $ sufficiently large so $ \frac1n < x $, hence
\[
	H_n(x)=\int_{-\frac1n}^{-\frac1n}\delta_n(t)\mathrm dt=1
\]
Similarly if we have $ x<0 $ fixed, then for $ n $ sufficiently large we get that
\[ H_n(x)=0. \]
If we take
\[
	\lim_{n\to\infty} H_n(x)=H(x)=\begin{cases}
		1 & x> 0\\
		0 & x< 0
	\end{cases}
\]
we get the Heaviside function. i.e. the unique function such that $ H'(x)=\delta(x) $ and $ H(x)=0 $ for $ x<0 $. We expect to need $ \frac{\mathrm dG}{\mathrm dx} $ to behave like $ H(x-\xi) $ near $ x=\xi $, i.e. we expect a jump discontinuity at $ x=\xi $. Since
\begin{align*}
	G(x,\xi)=\int_a^x\frac{\mathrm dG(t,\xi)}{\mathrm dt}\mathrm dt
\end{align*}
and
\begin{align*}
  \int_a^xH(t-\xi)\mathrm dt=\begin{cases}
	  x-\xi & x\ge \xi \\
	  0 & x < \xi
  \end{cases}
\end{align*}
which is a continuious function. To conclude, expect
\begin{itemize}
	\item $ x\to G(x,\xi) $ to be continuous at $ x=\xi $;
	\item $ x\to \frac{\mathrm dG(x,\xi)}{\mathrm dx} $ to have a jump disconuinuity at $ x=\xi $.
\end{itemize}
To fix the jump disconuinuity, integrate
\begin{align*}
	\frac{\mathrm d^2 G}{\mathrm dx^2}+\frac{\beta(x)}{\alpha(x)}\frac{\mathrm dG}{\mathrm dx}+\frac{\gamma(x)}{\alpha(x)}G=\frac{\delta(x-\xi)}{\alpha(x)}
\end{align*}
over $ (\xi-\varepsilon,\xi+\varepsilon $ with $ \varepsilon >0 $,
\begin{align*}
	\frac{\mathrm dG}{\mathrm dx}|_{x=\xi-\varepsilon}^{x=\xi+\varepsilon}+\int_{\xi-\varepsilon}^{\xi+\varepsilon}\left[\frac \beta\alpha\frac{\mathrm dG}{\mathrm dx}+\frac{\gamma}{\alpha}G\right]\mathrm dx=\int_{\xi-\varepsilon}^{\xi+\varepsilon}\frac{\delta(x-\xi)}{\alpha(x)}\mathrm dx=\frac1{\alpha(\xi)}.
\end{align*}
Taking $ \varepsilon\to 0 $ we get that
\[
	\left[\frac{\mathrm dG}{\mathrm dx}\right]_{x=\xi^-}^{x=\xi^+}=\frac1{\alpha(\xi)}.
\]
In summary, $ G $ satisfies
\begin{enumerate}
	\item $ G(\xi^+,\xi)=G(\xi^-,\xi) $;
	\item $ \left[ \frac{\mathrm dG}{\mathrm dx}\right]_{x=\xi^-}^{x=\xi^+}=\frac1{\alpha(\xi)}$.
\end{enumerate}
Let's look at an example. Take,
\[
	L=\frac{\mathrm d^2}{\mathrm dx^2}+\omega^2,\quad (a,b)=(0,1).
\]
We want to solve
\[
  \begin{cases}
	  \frac{\mathrm d^2G}{\mathrm dx^2}+\omega^2 G=\delta(x-\xi) & 0<x<1 \\
	  G(0,\xi)=G(1,\xi)=0
  \end{cases}.
\]
Solve the ODE on either side of $ x=\xi $.
\[
  G(x,\xi)=\begin{cases}
	  A(\xi)\sin(\omega x)+B(\xi)\cos(\omega x), & 0<x< \xi \\
	  C(\xi)\sin[\omega(x-1)]+D(\xi)\cos[\omega(x-1)] & \xi < x < 1
  \end{cases}.
\]
By the boundary conditions, $ G(0,\xi)=G(1,\xi)=0 $, hence
\[
  \implies B(\xi)=0,\ D(\xi) = 0.
\]
From continuity at $ x=\xi $ we get that 
\[
	A(\xi)=C(\xi)\frac{\sin(\omega(\xi-1))}{\sin(\omega\xi)}.
\]
From jump of $ \frac{\mathrm dG}{\mathrm dx} $ at $ x=\xi $ we get that
\[
	\frac{C(\xi)}{\sin(\omega\xi)}\omega\left[\cos[\omega(\xi-1)]\sin(\omega\xi)-\cos(\omega \xi)\sin[\omega(\xi-1)\right]=1
\]
So the inside expression becomes $ \sin[\omega\xi-\omega(\xi-1]=\sin \omega $.
Hence
\[
	C(\xi)=\frac{\sin(\omega\xi)}{\omega\sin(\omega)}
\]
and
\[
	A(\xi) = \frac{\sin(\omega(\xi-1))}{\omega\sin\omega}.
\]
Finially we get,
\begin{align*}
	G(x,\xi)&=\frac{1}{\omega\sin\omega}\times \begin{cases}
		\sin(\omega x)\sin(\omega(\xi-1)) & 0< x < \xi \\
		\sin(\omega\xi)\sin(\omega(x-1)) & \xi < x < 1
	\end{cases}\\
		&\equiv \text{const} \times \begin{cases}
			y_1(x)y_2(\xi) & 0< x < \xi\\
			y_1(\xi)y_1(x) & \xi < x < 1
		\end{cases}
\end{align*}
Note that $ y_1,y_2 $ are L.I solutions to $ Ly=0 $ with $ y_1(0)=0 $ and $ y_2(1)=0 $.
\subsection{A General Result}
We want to solve
\[
	(\dagger)\begin{cases}
	  Ly=f(x) & a< x < b\\
	  y(a)=y(b)=0
  \end{cases}
\]
where $ L=\alpha(x)\frac{\mathrm d^2}{\mathrm dx^2}+\beta(x)\frac{\mathrm d}{(\mathrm dx}+\gamma(x) $.
\begin{proposition}
  Let $ y_1,y_2 $ be solutions to $ Ly=0 $ with $ y_1(a)=0 $ and $ y_2(b)=0 $ linearly independent. Then,
  \[
	  G(x,\xi)=\frac1{\alpha(xi)W(y_1,y_2)(\xi)}\times \begin{cases}
		    y_1(x)y_2(\xi) & a <x <\xi\\
		    y_1(\xi)y_2(x) & \xi< x < b
    \end{cases}
  \]
satisfies 
\[
	L_x[G(x,\xi)]=\delta(x-\xi),\quad G(a,\xi)=G(b,\xi)=0.
\]
\end{proposition}
\pf We have that
\begin{align*}
	L_x[G(x,\xi)]=0
\end{align*}
for $ x\ne \xi $. Clearly $ G(\xi^+,\xi)=G(\xi^-,\xi) $ and
\begin{align*}
	\frac{\mathrm dG}{\mathrm dx}|^{x=\xi^+}-\frac{\mathrm dG}{\mathrm dx}|^{x=\xi^-}&=\frac1{\alpha(\xi)W(y_1,y_2)(\xi)}\left[y_1(\xi)y_2'(\xi)-y_1'(\xi)y_2(\xi)\right]\\
											 &=\frac1{\alpha(\xi)}.
\end{align*}
So $ G(x,\xi) $ has all desired properties for the Dirichlet Green's function.\qed\par
So the solution to $ (\dagger) $ is
\begin{align*}
	y(x)&=\int_a^b G(x,\xi)f(\xi)\mathrm d\xi\\
	    &=\left[\int_a^x+\int_x^b\right]G(x,\xi)\mathrm d\xi\\
	    &=y_2(x)\int_a^x\frac{y_1(\xi)f(\xi)}{\alpha(\xi)W(y_1,y_2)(\xi)}\mathrm d\xi+y_1(x)\int_a^b\frac{y_2(\xi)f(\xi)}{\alpha(\xi)W(y_1,y_2)(\xi)}\mathrm d\xi
\end{align*}
\begin{remark}
  Take care with the definition of $ G(x,\xi) $ on $ x>\xi $, $ x<\xi $.
\end{remark}
Take the example
\[
	L=\frac{\mathrm d^2}{\mathrm dx^2}+\omega^2,\quad (a,b)=(0,1).
\]
Use $ y_1(x)=\sin(\omega x) $ and $ y_2=\sin(\omega(x-1)) $, and $ \alpha(\xi)=1 $. We can compute
\begin{align*}
	W(y_1,y_2)(\xi)&=\omega\sin(\omega\xi)\cos(\omega(\xi-1))-\omega\cos(\omega \xi)\sin(\omega(\xi-1))\\
		       &= \omega\sin(\omega \xi-\omega(\xi-1))\\
		       &=\omega\sin(\omega).
\end{align*}
which gives
\[
	G(x,\xi)=\frac1{\omega\sin\omega}\times\begin{cases}
		\sin(\omega x)\sin(\omega(\xi-1)) & 0< x < \xi \\
		\sin(\omega\xi)\sin(\omega(x-1)) & \xi < x < 1
	\end{cases}
\]
same as before.
\subsection{Green's functions for Sturm-Liouville operators}
Suppose $ L $ has form
\[
	L=\frac1{w(x)}\left[-\frac{\mathrm d}{\mathrm dx}\left(p(x)\frac{\mathrm d}{\mathrm dx}\right)+q(x)\right].
\]
Since we're interested in solutions to $ Ly=f $, we can take $ w=1 $ \textit{wlog.} (by just changing $ f $ to $ wf $). Note for Sturm-Liouville operators we have that $ \alpha(x)\equiv -p(x) $.
\begin{proposition}
  If $ L $ is a Sturm-Lioville operator and $ y_1,y_2 $ satisfy $ Ly_1=Ly_2=0 $, then
  \[
	  p(x)W(y_1,y_2)(x)
  \]
  is constant.
\end{proposition}
\pf We have that
\begin{align*}
	y_2Ly_1-y_1Ly_2 &= y_2(-(py_1')'+qy_1)-y_1(-(py_2')'+qy_2)\\
			&= y_1(py_2')'-y_2(py_1')'\\
			&= (p(x)[y_1y_2'-y_1'y_2])'
\end{align*}
So if $ Ly_1=Ly_2=0 $ we get that $ p(x)=W(y_1,y_2)(x) $ is constant.\qed
\par
So for $ L $, a Sturm Liouville operator, we get that 
\[
	G(x,\xi)=\text{const.} \times \begin{cases}
		y_1(x)y_2(\xi) & a<x<\xi \\
		y_1(\xi)y_2(x) & \xi< x < b
	\end{cases}.
\]
And the solution to $ Ly=f $ with $ y(a)=y(b)=0 $ is
\begin{align*}
	y(x)=\left[-\frac{1}{p(c)W(y_1,y_2)(c)}\right]\times\left[y_2(x)\int_a^x y_1(\xi)f(\xi)\mathrm d\xi+y_1(x)\int_x^by_2(\xi)f(\xi)\mathrm d\xi\right]
\end{align*}
where $ c\in (a,b) $ is constant.
\subsection{Eigenfunction Expansions Revisited}
Recall that if $ L $ is a Sturm-Liouville operator on
$ V=\{y\in C^2([a,b]): y(a)=y(b)=0\} $,
then there exists \textit{normalised} eigenfunctions $ \{Y_k\}_{k=1}^\infty $ such that $ LY_k=\lambda_kY_k $ and $ \langle Y_k,Y_\ell\rangle =\delta_{k\ell} $. And for any $ f\in V $ with have that
\[
	f(x)=\sum_{k=1}^\infty \hat f_k Y_k(x), \quad \hat f_k=\langle f,  Y_k\rangle=\int_a^b f(x)Y_k(x)\mathrm dx.\tag{$ \star $}
\]
Note that we have that $ G(a,\xi)=G(b,\xi)=0 $, but $ G(x,\xi)\notin V $ since it has a jump discontinuity so it's not twice continuously differentiable. Instead, consder the sequence $ \{G_n(x,\xi)\}_{n=1}^\infty $ such that
\begin{align*}
	L_x[G_n(x,\xi)]=\delta_n(x-\xi)\\
	G_n(a,\xi)= G_n(b,\xi)=0.
\end{align*}
Note that each $ G_n(\cdot, \xi)\xi V $ for any $ \xi\in (a,b) $ so $ \star $ holds for each $ G_n(x,\xi) $ with $ f(x)\equiv G_n(x,\xi) $. Then take the limit to get that
\[
	G(x,\xi)=\sum_{k=1}^\infty \hat G_k(x)Y_k(x).
\]
Instead pretend that $ G(x,\xi)\in V $.
\begin{proposition}
	If $ \{Y_k\} $ are as above then the Dirichlet Green's function for the Sturm Liouville operator $ L $ satisfies,
	\[
		G(x,\xi)=\sum_{k=1}^\infty \frac{Y_k(x)Y_K(\xi)}{\lambda_k}
	\]
	where $ LY_k=\lambda_k Y_k $.
\end{proposition}
By completeness of $ V $ we know that
\[
	G(x,\xi)=\sum_{k=1}^\infty \hat G_k(\xi)Y_k(x)
\]
with
\begin{align*}
	\hat G_k(\xi)&=\langle G(\cdot ,\xi),Y_k\rangle\\
		     &= \frac 1{\lambda_k}\langle G(\cdot, \xi),LY_k\rangle\\
		     &=\frac1{\lambda_k}\langle LG(\cdot, \xi), Y_k\rangle \quad\text{since}\ L\ \text{is self-adjoint}\\
		     &= \frac1{\lambda_k}\langle\delta(\cdot-\xi),Y_k\rangle\\
		     &=\frac1{\lambda_k}\int_a^b\delta(x-a)Y_k(x)\mathrm dx\\
		     &=\frac{Y_k(\xi)}{\lambda_k}.
\end{align*}
i.e. we have that
\[
	G(x,\xi)=\sum_{k=1}^\infty \frac{Y_k(x)Y_k(\xi)}{\lambda_k}\qed
\]
\begin{remark}
  If we have $ \lambda_k=0 $ for some $ k $, suppose $ \tilde y\in V $ with $ L\tilde y=0 $. We will have $ y=(\inv Ly)(x) $ so if $ y\in V $ satisfies $ Ly=f $ so does $ L(y+\tilde y)=f $ so $ L $ is not invertiable, hence we can't find a Green's function.
\end{remark}
Let's look an example now. Take $ L=-\frac{\mathrm d^2}{\mathrm dx^2} $ with $ (a,b)=(0,1) $. So $ V=\{y\in C^2([0,1]): y(0)=y(1)=0\} $. We have that
\[
  Y_k(x)=\sqrt 2\sin(k\pi x),\quad \lambda_k=(k\pi)^2.
\]
So
\begin{align*}
	G(x,\xi)=\sum_{k=1}^\infty \frac{2\sin(k\pi x)\sin(k\pi \xi)}{(k\pi)^2}
\end{align*}
and we can check all properties.\par
We have
\[
	G(x,\xi)=\sum_{k=1}^\infty \frac{Y_k(x)Y_k(\xi)}{\lambda_k}
\]
and we apply $ L $ to both sides, we get that
\begin{align*}
	\delta(x-\xi)&=L[G(x,\xi)]\\
		     &=\sum_{k=1}^\infty \frac{Y_k(\xi)L_x[Y_k(x)]}{\lambda_k}\\
		     &= \sum_{k=1}^\infty Y_k(\xi)Y_k(x)\\
\end{align*}
so
\[
	\delta(x-\xi)=\sum_{k=1}^\infty Y_k(x)Y_k(\xi).
\]
This is a similar form to the Kronecker delta expansion as the outer product of two vectors.
\subsection{Initial Value Problems}
We want to solve
\[
	(\dagger)\begin{cases}
		Ly=f(t) & t>0 \\
		y(0)=\dot y(0)=0
	\end{cases}.
\]
where $ L =\alpha(t)\frac{\mathrm d^2}{\mathrm dx^2}+\beta(t)\frac{\mathrm d}{\mathrm dx}+\gamma(t) $, with $ \alpha(t)\ne 0 $.\par
For each $ \tau\in (0,\infty) $, suppose we can find $ G=G(t,\tau) $ such that
\[
	L_t[G(t,\tau)]=\delta(t-\tau)\quad t>0
\]
and also that
\[
	G(0,\tau)=\frac{\mathrm dG}{\mathrm dt}(0,\tau)=0.
\]
Then if we set
\[
  y(t)=\int_0^\infty G(t,\tau)f(\tau)\mathrm d\tau,
\]
we get the solution to $ (\dagger) $.
\begin{proposition}
  The Green's function for $ (\dagger) $ is characterised by
  \begin{enumerate}
	  \item $ G(t,\tau)=0 $ on $ t< \tau $;
	  \item $ L_t[G(t,\tau)]=0 $ on $ t>\tau $ with $ G(t^+,\tau)=0 $ and $ \frac{\mathrm dG}{\mathrm dt}(\tau^+,\tau)=\frac1{\alpha(\tau)} $.
  \end{enumerate}
\end{proposition}
\pf We know that (i) guarantees that $ L_t[G(t,\tau)]=0 $ for $ t<\tau $, and $ G(0,\tau) =\frac{\mathrm dG}{\mathrm dt}(0,\tau)=0 $. Part (ii) guarantees that $ L_t[G(t,\tau)]=\delta(t-\tau) $, since $ G(\tau^+,\tau)=G(\tau^-,\tau)=0 $ and
\[
	\frac{\mathrm dG}{\mathrm dt}(\tau^+,\tau)-\frac{\mathrm dG}{\mathrm dt}(\tau^-,\tau)=\frac1{\alpha(\tau)}\qed
\]
Note that since $ G(t,\tau)=0 $ on $ \tau>t $,
\begin{align*}
	y(t) &= \int_0^\infty G(t,\tau)f(\tau)\mathrm d\tau)=\int_0^\tau G(t,\tau)f(\tau)\mathrm d\tau.
\end{align*}
i.e. causality is intact.\par
Let's see an example. Take $ L=\frac{\mathrm d^2}{\mathrm dt^2} $. Then $ G(t,\tau)=0 $ when $ t<\tau $ and for $ t>\tau $, we have
\[
\frac{\mathrm d^2 G}{\mathrm dt^2}(t,\tau)=0,\quad G(\tau^+,\tau)=0,\quad \frac{\mathrm dG}{\mathrm dt}(\tau^+,\tau)=1.
\]
We get that $ G(t,\tau)=A(\tau)(t-\tau)+B(\tau) $. This gives that $ B=0 $ and differentiating with resepct to $ t $ gives that $ A=1 $. So our solution to $ (\dagger) $ gives that
\[
  y(t)=\int_0^\tau (t-\tau)f(\tau)\mathrm d\tau.
\]
Note that this is consistant with Taylor's theroem.
\section{The Fourier Transform}
\subsection{Definitions and simple properties}
The Fourier transform is the $ L\to \infty $ limit of the theory of Fourier series.
\begin{definition}
	(Fourier Transform) For $ f:\to \R\to \C $ define its \textit{Fourier Transform} by
	\[
		\hat f(\lambda)=\allint e^{-i\lambda x}f(x)\mathrm dx,\quad \lambda\in \R.
	\]
\end{definition}
We will use $ \mathcal F $ to denote the relevant linear map, i.e. $ \hat f=\mathcal F[f] $. We will occasionally write that $ \mathcal F_{x\to \lambda}[f(x)]=\hat f(\lambda) $.\par
Also if $ g $ is integrable on $ \R $ then
\[
	\allint  e^{-i\lambda x}g(x)\mathrm dx\to0
\]
as $ |\lambda|\to\infty $, by the Riemann Lebsegue lemma.
\begin{proposition}
	We'll make some statements about the arthimetic of Fourier transformations.
	\begin{enumerate}
		\item 
			\[
				\mathcal F_{x\to \lambda}\left[\left(\frac{\mathrm d}{\mathrm dx}\right)^kf(x)\right]=(i\lambda)^k\hat f(\lambda)
			\]
			\[
				\mathcal F_{x\to \lambda}\left[x^kf(x)\right]=\left(i\frac{\mathrm d}{\mathrm d\lambda}\right)^k\hat f(\lambda)
			\]
		\item 
			\[
				\mathcal F_{x\to \infty}[f(x-a)]=e^{-i\lambda a}\hat f(\lambda)
			\]
			\[
				\mathcal F_{x\to\infty}[e^{-iax}f(x)]=\hat f(\lambda +a)
			\]
	\end{enumerate}
\end{proposition}
\pf We'll just prove part (i) and leave (ii) as an exercise.
\begin{align*}
	\allint f^{(k)}(x)e^{-i\lambda x}\mathrm dx&=\allint f(x)\left(-\frac{\mathrm d}{\mathrm dx}\right)^ke^{-i\lambda x}\mathrm dx\\
	&=(i\lambda)^k\hat f(\lambda).\\\\
	\left(i\frac{\mathrm d}{\mathrm d\lambda}\right)^k\allint e^{-i\lambda x}f(x)\mathrm dx
	&= \allint e^{-i\lambda x}x^kf(x)\mathrm dx\\
	&= \mathcal F_{x\to\lambda}[x^kf(x)].
\end{align*}
Property (i) is important in solving ODEs.
\[
	P\left(\frac{\mathrm d}{\mathrm dx}\right)y=F(x),\quad x\in \R,
\]
where $ P $ is a polynomial. Take the Fourier transform, so
\[
  P(i\lambda)\hat y(\lambda)=\hat F(\lambda),
\]
i.e. $ \hat y(\lambda)=\frac{\hat F(\lambda)}{P(i\lambda)} $.
\par
But can we get $ y $ back from $ \hat y $?
\begin{proposition}
  We can reconstruct $ f $ from $ \hat f $, using the inversion formula for the Fourier transformation,
  \[
	  f(x)=\frac1{2\pi}\allint e^{i\lambda x}\hat f(\lambda)\mathrm d\lambda.
  \]
\end{proposition}
\pf Write
\[
	\allint e^{i\lambda x}\hat f(\lambda)\mathrm d\lambda=\lim_{n\to\infty} \int_{-n}^n  e^{i\lambda x}\hat f(\lambda)\mathrm d\lambda.
\]
So the integral is the limit of
\begin{align*}
	\frac1{2\pi}\int_{-n}^ne^{i\lambda x}\left[\allint e^{-i\lambda y }f(y)\mathrm dy\right]\mathrm d\lambda&= \allint f(y)\left[\frac1{2\pi}\int_{-n}^ne^{i\lambda(x-y)}\mathrm d\lambda\right]\mathrm dy\\
													       &=\allint f(y)\frac{\sin[n(x-y)]}{\pi(x-y)}\mathrm dy\\
													       &= \allint f(x+y)\frac{\sin(ny)}{\pi y}\mathrm dy.
\end{align*}
Recall from IA Differential Equations Example Sheet 1 Question 13. We get that for all $ n $
\[
	\allint \frac{\sin(ny)}{\pi y}\mathrm dy=1.
\]
So
\begin{align*}
	\frac{1}{2\pi}\int_{-n}^{n}e^{i\lambda x}\hat{f}(\lambda)\mathrm dy-f(x)&= \allint \sin(ny)\frac{f(x+y)-f(x)}{\pi y}\mathrm dy\\
									   &= \allint \sin(ny)F(y,x)\mathrm dy\\
									   &=\frac{1}{n}\allint \cos(ny)F_y(y,x)\mathrm dy\to 0\qed
\end{align*}
From previous removing limits we get that
\[
	f(x)=\allint f(y)\left[\frac1{2\pi}\allint e^{i\lambda(x-y)}\mathrm d\lambda\right]\mathrm dy
\]
i.e
\[
	\delta(x-y)=\frac1{2\pi}\allint e^{i\lambda(x-y)}\mathrm d\lambda.
\]
We can use this to prove a proposition.
\begin{proposition}
	For $ f,g:\R\to \C $,
	\[
		\allint f(x)\overline{g(x)}\mathrm dx=\frac1{2\pi}\allint \hat f(\lambda)\overline{\hat g(\lambda)}\mathrm d\lambda
	\]
	hence
	\[
		\allint|f(x)|^2\mathrm dx=\frac1{2\pi}\allint|\hat f(\lambda)|^2\mathrm d\lambda.
	\]
\end{proposition}
\pf 
\begin{align*}
	\frac {1}{2\pi}\allint\left[\allint e^{-i\lambda y}f(y)\mathrm dy\right]\left[e^{i\lambda x\overline{g(x)}}\mathrm dx\right]\mathrm d\lambda&=\allint\allint f(y)\overline{g(x)}\left[\frac1{2\pi}\allint e^{i\lambda(x-y)}\mathrm dy\right]\mathrm dx\mathrm dy\\
	&=\allint f(x)\overline{g(x)}\mathrm dx\qed
\end{align*}
\begin{definition}
	(Convolution) For $ f,g:\R\to\C $ define the \textit{convolution} by,
	\[
	  f*g(x)=\allint f(x-y)g(y)\mathrm dy.
	\]
\end{definition}
\begin{proposition}
  \[
	  \mathcal F_{x\to \lambda}[f*g(x)]=\hat f(\lambda)\hat g(\lambda).
  \]
\end{proposition}
\pf 
\begin{align*}
	\text{LHS} &=\allint e^{-i\lambda x}\left[\allint f(x-y)f(y)\mathrm dy\right]\mathrm dx\\
	   &=\allint\allint e^{-i\lambda x}f(x-y)g(y)\mathrm dx\mathrm dy\\
	   &=\allint\allint e^{-i\lambda(X+Y)}f(X)g(Y)\mathrm dX\mathrm dY\quad\text{where}\  X=x-y,\ Y=y\\
	   &= \hat f(\lambda)\hat g(\lambda).
\end{align*}
\subsection{Important Examples}
We'll first look at exponentials.\par
Let $ \sigma\in \C $ with $ \Re(\sigma)>0 $. Let \[
	f(x)=H(x)e^{-\sigma x} = \begin{cases}
		e^{-\sigma x} & x \ge 0\\
		0 & x < 0
	\end{cases}.
\].
Then
\[
	\hat f(x)=\allint f(x)e^{-i\lambda x}\mathrm dx=\int_0^\infty e^{-(\sigma +i\lambda)x}\mathrm dx=\frac 1{\sigma+i\lambda}.
\]
Note that $ |\hat f(\lambda)|\to 0 $ as $ |\lambda|\to \infty $.\par
From the inversion formula, we have that
\begin{align*}
	H(x)e^{-\sigma x}&=\frac {1}{2\pi}\allint \frac{e^{i\lambda x}}{\sigma +i\lambda}\mathrm{d}\lambda\\
			 &=\frac {1}{2\pi i}\allint \frac{e^{i\lambda x}}{\lambda-i\sigma}\mathrm{d}\lambda
\end{align*}
Differentaion with respect to $ \sigma $ gives that
\[
	H(x)x^ke^{-\sigma x}=\left(-\frac{\partial}{\partial \sigma}\right)^k\frac {1}{2\pi}\allint \frac{e^{i\lambda x}}{i\lambda +\sigma} \mathrm{d}\lambda=\frac{k!}{2\po}\allint \frac{e^{i\lambda x}}{(\sigma+i\lambda)^{k+1}}\mathrm {d}\lambda.
\]
Let's now consider $ f(x)=\frac 1{\sqrt{2\pi}}e^{-x^2/2} $. So
\begin{align*}
	\hat {f}(\lambda)=\frac {1}{\sqrt{2\pi}}\allint e^{-i\lambda x-x^2/2}\mathrm {d}\lambda.
\end{align*}
This is tough without IB Complex Analysis/Methods.\\
Instead we'll notice the following,
\begin{align*}
	\left(\frac{\mathrm d}{\mathrm dx}\right) f=-xf\\
	\implies (i\lambda)\hat f=-\left(i\frac{\mathrm d}{\mathrm dx}\right)\hat f.
\end{align*}
i.e. we have that
\[
	\left(\frac{\mathrm d}{\mathrm dx}\right)\hat f=-\lambda \hat f.
\]
Hence
\[
	\hat f(\lambda)=\hat f(0)e^{-\lambda^2/2},
\]
so from the definition,
\[
  \hat f(0)=\allint f(x)\mathrm dx=1
\]
i.e.
\[
	\hat f(\lambda)=e^{-\lambda^2/2}.
\]
So Gaussians are eigenfunctions of the operator $ \mathcal F $ (with eigenvalue $ \sqrt{2\pi} $ in this case). We also have that
\[
	e^{-\lambda^2/2}=\frac 1{\sqrt{2\pi}}\allint e^{-i\lambda x-x^2/2}\mathrm dx.
\]
\par
Now we look at the Dirac delta function. From the definition of $ \delta(x) $ we have that
\begin{align*}
	\hat \delta(\lambda)=\allint e^{-i\lambda x}\delta(x)\mathrm dx=1
\end{align*}
From the inversion formula we get that
\[
	\delta(x)=\frac 1{2\pi}\allint e^{i\lambda x}\mathrm d\lambda.
\]
\subsection{Initial Value Problems revisited}
Recall from Section 4.6 we have the initial value problem
\[
	(\dagger)\begin{cases}
		Ly=f(t) & t>0 \\
		y(0)=\dot y(0)=0
	\end{cases}.
\]
Now assume that $ L $ has constant coefficients, so $ L=\alpha\frac{\mathrm d^2}{\mathrm dx^2}+\beta\frac{\mathrm d}{\mathrm dx}+\gamma $. Without loss of generality we can assume that $ \alpha=1 $. Extend definition of $ t\to y(t) $ to all of $ \R $ by setting $ y(t)=0 $ on $ t<0 $. We'll do the same for $ f $. It is customary to write \[ \mathcal F_{t\to\omega}[y(t)]=\hat y(\omega) \] when dealing without time-like variables.\par
Taking the Fourier transformation we get that
\[
	[(i\omega)^2+\beta(i\omega)+\gamma]\hat y(\omega)=\hat f(\omega).
\]
i.e. we have that
\[
	\hat y(\omega)=\frac{\hat f(\omega)}{P(\omega)},\quad P(\omega)=(i\omega)^2+\beta(\omega)+\gamma.
\]
Write $ P(\omega) $ as
\[
	P(\omega)=(i\omega+\sigma_1)(i\omega+\sigma_2).
\]
Assume that $ \beta $ and $ \gamma $ are such that $ \Re(\sigma_i)>0 $ for $ i=1,2 $, and additionally that $ \sigma_1\ne \sigma_2 $. Note that
\[
	\frac 1{P(\omega)}=\frac1{\sigma_2-\sigma_1}\left[\frac{1}{i\omega+\sigma_1}-\frac 1{i\omega+\sigma_2}\right]
\]
and by Section 5.2 we get that the right hand side of the equation is
\[
	=\frac{1}{\sigma_2-\sigma_1}\left[\mathcal F_{t\to \omega}[H(t)e^{-\sigma_1t}]-\mathcal{F}_{t\to \omega}[H(t)e^{-\sigma_2t}]\right].
\]
If we define
\[
	R(t)=H(t)\left[\frac{e^{-\sigma_1t}-e^{-\sigma_2t}}{\sigma_2-\sigma_1}\right],
\]
then
\begin{align*}
	\hat R(\omega)=\frac 1{P(\omega)},
	\hat y(\omega)=\hat R(\omega)\hat f(\omega).
\end{align*}
We call $ R=R(t) $ the response function for $ L $. If $ \sigma_1=\sigma_2 $, set $ \sigma_1=\sigma $, and $ \sigma_2=\sigma+\varepsilon $, and take the limit as $ \varepsilon\to 0 $, to get
\[
	R(t)=H(t)te^{-\sigma t}.
\]
In the other case, by the convolution theorem we get that
\begin{align*}
	y(t) &= R*f(t) \\
	     &= \allint R(t-\tau)f(\tau)\mathrm d\tau\\
	     &= \int_0^\tau R(t-\tau)f(\tau)\mathrm d\tau.
\end{align*}
So $ R(t-\tau)=G(t,\tau) $. If $ f=\delta(\tau) $, then $ y(t)=R(t) $.\par
However it's not clear where we used the initial conditions of the problem to get our solution. We actually used them implicitly to ensure that the boundary terms vansih when we integrated the differential equation when we first took the Fourier transformation, to ensure the function is continuous at $ 0 $ and the first derivative is continuous at $ 0 $.\par
Let's see an example. Consider the damped simple harmonic oscillator.
\[
  \begin{cases}
	  \ddot y + 2\gamma \dot y+y=f(t) & t>0\\
	  y(0) = \dot y(0)=0
  \end{cases}
\]
with $ \gamma>0 $. By the Fourier transformation we ge that
\[
	\hat y(\omega)=\frac{\hat f(\omega)}{P(\omega)},\quad P(\omega)=[i\omega+\gamma+\sqrt{\gamma^2-1}][i\omega+\gamma-\sqrt{\gamma^2-1}].
\]
Note that
\[
	\Re[\gamma\pm \sqrt{\gamma^2-1}]\equiv \Re[\omega_\pm(\gamma)]>0.
\]
So in the case $ \gamma>1 $,
\[
	R(t)=H(t)\left[\frac{e^{-\sigma_-(t)}-e^{-\sigma_+t}}{\sigma_+-\sigma_-}\right]=H(t)e^{-\gamma t}\frac{\sinh[t\sqrt{\gamma^2-1}]}{\sqrt{\gamma^2-1}}
\]
This is \textit{overdamping}. In the case that $ 0<\gamma<1 $, we get that
\[
	R(t)=H(t)e^{-\gamma t }\frac{\sin[t\sqrt{1-\gamma^2}]}{\sqrt{1-\gamma^2}}
\]
This is \textit{underdamping}. When $ \gamma=1 $, take $ \gamma=1+\varepsilon $ and send $ \varepsilon\to 0 $, to get
\[
	R(t)=H(t)te^{-t}.
\]
This is \textit{critical damping}.
\subsection{Some Neat Examples}
\subsubsection{Poisson Summation Formula}
\begin{proposition}
	(Poisson summation formula) For a "nice" function $ f $ we have that
  \[
	  \sum_nf(x+n)=\sum_n\hat f(2\pi n)e^{2\pi in x}
  \]
\end{proposition}
\pf Notice that the LHS and RHS functions are periodic with period $ 1 $. So they have a Fourier series. Say that the LHS is $ F(x) $ and the RHS is $ G(x) $, so $ \hat G_n=\hat f(2\pi n) $. $ F $ has Fourer coefficients given by
\begin{align*}
	\hat F_n&=\int_0^1e^{-2\pi in x}F(x)\mathrm dx \\
		&=\sum_m\int_0^1 e^{-2\pi inx}f(x+m)\mathrm dx\\
		&= \sum_m\int_m^{m+1}e^{-2\pi i nx}f(x)\mathrm dx\\
		&=\allint e^{-2\pi inx}f(x)\mathrm dx\\
		&= \hat f(2\pi n).
\end{align*}
So $ \hat F_n=\hat G_n $, so $ F(x)=G(x) $.\qed
\par
Let's see an example of this in action. Try $ f(x)=H(x)e^{-\sigma(x)} $ with $ \Re(\sigma)>0 $. Then $ \hat f(\lambda)=(\sigma+i\lambda)^{-2} $ using the Poisson summation formula with $ x=0 $ we get that
\begin{align*}
	\sum_n\frac1{(2\pi in +\sigma)^2}=\sum_{n=1}^\infty n^{-\sigma n}=\frac{e^\sigma}{(1-e^{\sigma})^2}=\frac 4{\sinh^2(\sigma/2)}.
\end{align*}
Let's take
\[
	f(x)=\frac1{\sqrt{2\pi}}e^{-x^2/2},\quad \hat f(\lambda)=e^{-\lambda^2/2}
\]
which gives that
\[
	\frac 1{\sqrt{2\pi}}\sum_ne^{-(x+n)^2/2}=\sum_ne^{-(2\pi n)^2/2}e^{2\pi inx}
\]
\subsubsection{Heisenberg's Uncertainty Principle}
If $ \allint |\psi(x)|^2\mathrm dx=1 $ then we claim that
\[
	\left(\allint x^2|\psi(x)|^2\mathrm dx\right)\left(\allint \lambda^2|\hat \psi(\lambda)|^2\mathrm d\lambda\right)\ge \frac \pi 2
\]
Integrate by parts to get that
\begin{align*}
	I &= -\allint x\frac{\mathrm d}{\mathrm dx}[\psi\bar\psi]\mathrm dx\\
	  &= -\allint \left(x\psi\bar\psi'+x\psi'\bar\psi\right)\mathrm dx.\\
\end{align*}
Then by the triangle inequality we get that
\begin{align*}
	1&\le 2\allint x|\psi||\psi'|\mathrm dx\\
	 &=2\left(\allint x^2|\psi|^2\mathrm dx\right)^{\frac 12}\left(\allint |\psi'|^2\mathrm dx\right)^{\frac 12}.
\end{align*}
Recall that $ \mathcal F_{x\to \lambda}[f'(x)]=(i\lambda)\hat f(\lambda) $, and $ \allint |f(x)|^2\mathrm dx=\frac 1{2\pi}\allint |\hat f(\lambda)|^2\mathrm d\lambda $. So
\[
	\allint |f'(x)|^2\mathrm dx=\frac1{2\pi}\allint \lambda^2|\hat f(\lambda)|^2\mathrm d\lambda.
\]
Hence
\begin{align*}
	1\le \left(\allint x^2|\psi|^2\mathrm dx)^{\frac 12}\right)\left(\frac 1{2\pi}\allint \lambda^2 |\hat \psi(\lambda)|^2\mathrm d\lambda)^{\frac 12}\right)
\end{align*}
which gives the result after moving constants and squaring.
\par
Replacing $ \psi(x) $ with $ e^{-i\lambda_0 x}\psi(x+x_0) $ we get the new Fourier transform as $ e^{i\lambda x_0}\hat \psi(\lambda+\lambda_0) $. So 
\begin{align*}
  \left(\allint x^2|\psi(x+x_0)|^2\mathrm dx\right)\cdot\left(\allint \lambda^2|\hat\psi(\lambda+\lambda_0)|^2\mathrm d\lambda\right)\ge \frac \pi 2,
\end{align*}
so we get that 
\begin{align*}
    \left(\allint x^2|\psi(x)|^2\mathrm dx\right)\cdot\left(\allint \lambda^2|\hat\psi(\lambda)|^2\mathrm d\lambda\right)\ge \frac \pi 2.
\end{align*}
In quantum mechanics the probability that a particle's momentum is in $ (A,B) $ is (up to a constant) $ \int_A^B |\hat \psi(\lambda)|^2\mathrm d\lambda $. If we choose
\begin{align*}
	x_0&=\allint x|\psi(x)|^2\mathrm dx\\
	\lambda_0&=\allint \lambda|\hat \psi(\lambda)|^2\mathrm \lambda,
\end{align*}
then our equation becomes a statement about variance i.e the variance of position multiplied by the variance of momentum must be greater than some constant.
\subsubsection{From Fourier Series to Fourier Transforms}
Consider the smooth function $ f:\R\to \C $, and assume that $ f(x) $ has compact support i.e. that $ f(x)=0 $ for all $ |x|> L $. On our interval $ (-L,L) $ we have that
\[
	f(x)=\sum_n\hat f_ne^{i\pi nx/L},
\]
so
\begin{align*}
	\hat f_n&=\frac1{2L}\int_{-L}^Le^{-i\pi nx/L}f(x)\mathrm dx\\
		&= \frac1{2L}\allint e^{-\pi n x/L}f(x)\mathrm dx
\end{align*}
since $ f(x)=0 $ on $ |x|>L $. Write $ \lambda_n=\frac{n\pi}L $ and $ \delta \lambda_n=\lambda_{n+1}-\lambda_n=\frac \pi L $. Then
\begin{align*}
	\hat f_n=\frac 1{2\pi}\delta \lambda_n\hat f(\lambda_n).
\end{align*}
So for $ x\in (-L,L) $ we have that
\[
	f(x)=\frac 1{2\pi} \sum_ne^{i\lambda_n x}\hat f(\lambda_n)\delta \lambda_n.
\]
Take $ L\to \infty $ i.e. $ \delta\lambda_n\to 0 $, we get that 
\[
	f(x)=\frac 1{2\pi}e^{i\lambda x}\hat f(\lambda)\mathrm d\lambda.
\]
This is the Fourier inversion formula we saw before!\par
Also from Parseval's theorem we have that 
\begin{align*}
	\int_{-L}^L |f(x)|^2\mathrm dx &=2L\sum_n|\hat f_n|^2\\
				       &= \frac 1{2\pi}\sum_n|\hat f(\lambda_n)|^2\delta \lambda_n
\end{align*}
So as $ L\to\infty $ get the that
\begin{align*}
	\allint|f(x)|^2\mathrm dx=\frac 1{2\pi}\allint |\hat f(\lambda)|^2\mathrm d \lambda.
\end{align*}
This is Parseval's theorem for the Fourier transform from the equivalent theorem from the Fourier series theorem.
\subsubsection{Central Limit Theorem}
Let $ X_1,\dots, X_n $ be i.i.d random variables with $ \mathbb E(X_1)=\mu=0 $ and $ \Var{X_1}=\sigma^2 $. Define
\[
	S_n=\frac 1{\sqrt n}(X_1+\cdots + X_n)
\]
and denote the pdf of $ S_n $ by $ f_n(x) $. The characteristic function of $ S_n $ is
\begin{align*}
	\hat f_n(\lambda)&=\mathbb E[e^{-i\lambda S_n}]\\
			 &= \mathbb E[e^{-i\lambda(X_1+\cdots X_n)/\sqrt n}]\\
			 &= \mathbb E[e^{-i\lambda X_1/\sqrt n}]\cdots \mathbb E[e^{-i\lambda X_n/\sqrt n}]\\
			 &=\mathbb E[e^{-i\lambda X_1/\sqrt n}]^n\\
			 &=\varphi_{X_1}(\frac \lambda{\sqrt n})^n
\end{align*}
Notice
\begin{align*}
	\varphi_{X_1}(0)&=1\\
	\varphi_{X_1}'(0)&=-i\mu=0\\
	\varphi_{X_2}''(0)&=-\sigma^2.
\end{align*}
Take $ \sigma^2 = 1 $, so
\begin{align*}
	\hat f_n(\lambda)&=\left(1-\frac 12\frac{\lambda^2}n+o\left(\frac 1nright)\right)^n\\
			 &= e^{-\lambda^2/2}+o(1)\quad\text{as}\ n \to\infty.
\end{align*}
Recall that
\[
	\mathcal F_{x\to \lambda}\left[\frac 1{\sqrt{2\pi}}e^{-x^2/2}\right]=e^{-\lambda^2/2}.
\]
So by  the inverse Fourier transform we get that 
\begin{align*}
	\lim_{n\to \infty} f_n(x)&=\lim_{n\to \infty}\frac 1{2\pi}\allint e^{i\lambda x}\hat f_n(\lambda)\mathrm d\lambda \\
				 &= \frac 1{\sqrt{2\pi}}e^{-x^2/2}
\end{align*}
i.e.
\[
	\frac{X_1+\cdots + X_n}{\sqrt n}\to \mathcal N(0,1).
\]
\subsection{Discrete Fourier Transform}
Given a sequence $ \{X_n\}_{n=0}^{N-1} $, define a new sequence $ \{\hat X_{k}\}_{k=0}^{N-1} $ by the discrete Fourier Transform (DFT)
\[
	\hat X_k=\sum_{n=0}^{N-1}X_ne^{-i(2\pi n/N)k}.
\]
Let $ f:[0,2\pi)\to \C $ be given and suppose that $ X_n=f(x_n) $, where $ x_n=\frac{2\pi n}N \in [0,2\pi) $. Set $ \delta x_n=x_{n-1}-x_n=\frac {2\pi}N $.
\begin{align*}
	\hat X_k=\frac N{2\pi}\sum_{n=0}^{N-1}f(x_n)e^{-ikx_n}\delta x_n.
\end{align*}
If we extend $ f(x)=0 $ when $ x\notin [0,2\pi) $, we get that
\[
	\hat X_K\approx \frac {N}{2\pi} \allint e^{-ikx}f(x)\mathrm dx=\frac N{2\pi}\hat f(k)\quad \text{for}\ k=0,1,\cdots,N-1.
\]
i.e. we have that for $ N $ large,
\[
	\hat f(k)\approx \frac{2\pi}N\hat X_k.
\]
\begin{proposition}
	Given $ \{\hat X_k\}_{k=0}^{N-1} $ we can recover $ \{X_n\}_{n=0}^{N-1} $ via 
	\[
		X_n=\frac 1N\sum_{k=0}^{N-1}\hat X_ke^{i(2\pi k/N)n}.
	\]
\end{proposition}
\pf By direct computation,
\begin{align*}
	\sum_{k=0}^{N-1}\hat X_ke^{i(2\pi n/N)k} &= \sum_{k=0}^{N-1}\sum_{m=0}^{N-1} X_me^{2\pi i(n-m)k/N}.
\end{align*}
Write $ \omega_N=e^{2\pi i/N} $, we get that
\begin{align*}
  &=\sum_{m=0}^{N-1}X_m\sum_{k=0}^{N-1}\omega_N^{(n-m)k}.
\end{align*}
Note that
\[
	\sum_{k=0}^{N-1}\omega_N^{(n-m)k}=\begin{cases}
		N & n=m \\
		\frac{1-\omega_N^{(n-m)N}}{1-\omega_N^{(n-m)}}=0 & n\ne m
	\end{cases}.
\]
So our sum becomes
\[
	=\sum_{m=0}^{N-1}X_m(N\delta_{nm})=NX_n.\qed
\]
Instead with vectors, if we have $ \mathbf x\in \C^N $ we have
\[
  \mathbf x=\begin{pmatrix}
	  X_0 \\ \vdots \\ X_{N-1}
  \end{pmatrix},\ \text{so}\quad \hat {\mathbf X}=\begin{pmatrix}
  \hat X_0 \\ \vdots \\ \hat X_{N-1}
  \end{pmatrix} = \mathcal F\mathbf X
\]
where $ \mathcal F $ is an $ N\times N $ matrix, where $ \mathcal F_{kn}=e^{-i(2\pi n/N)k} $. From previous,
\[
  \mathcal F\mathcal F^\dagger = \mathcal F^\dagger \mathcal F =N I_N,
\]
hence if we define $ U=\frac 1{\sqrt N}\mathcal F $ we have that $ U U^\dagger = U^\dagger U $ so $ U $ is unitary. i.e $ (U\mathbf x)\cdot (U\mathbf y)=\mathbf x \cdot \mathbf y $.\par
For an example take $ X_n=\begin{pmatrix}
  N-1 \\ n
\end{pmatrix} $ for $ n=0,\cdots, N-1 $. Then we have that
\begin{align*}
\hat X_k=\sum_{n=0}^{N_1} \binom{N-1}{n} e^{-i(2\pi n/N)k}=\left(1+e^{-2\pi ik/N}\right)^{N-1}.
\end{align*}

\begin{proposition}
	(Parseval's theorem) We have that,
	\[
		\sum_{n=0}^{N-1}X_n\overline Y_n=\frac 1N\sum_{k=0}^{N-1} \hat X_k\overline{\hat Y}_k.
	\]
	This also means that
	\[
		\sum_{n=0}^{N-1}|X_n|^2=\frac 1N\sum_{k=0}^{N-1}|\hat X_k|^2.
	\]
\end{proposition}
\pf
\begin{align*}
	\text{RHS} &= \frac 1N(\mathcal F\mathbf x)\cdot(\mathcal F\mathbf y)\\
		   &= (U\mathbf x)\cdot(U\mathbf y)\\
		   &=\mathbf x\cdot \mathbf y\\
		   &= \text{LHS}.
\end{align*}
And we can take $ \mathbf x=\mathbf y $ to get latter part.\qed
\par
Let's see an example. Take $ X_n=\begin{pmatrix}
  N-1 \\ n
\end{pmatrix} $ as before. So
\begin{align*}
	\sum_{n=0}^{N-1}\binom{N-1}{n}^2 &=\frac 1N=\sum_{k=0}^{N-1}(1+e^{-2\pi ik/N})^{N-1}(1+e^{2\pi ik/N})^{N-1}\\
					 &=\frac 1N\sum_{k=0}^{N-1}(e^{i\pi k/N}+e^{-i\pi k/N})^{2N-2}\\
					 &=\frac 1N \sum_{k=0}^{N-1}\left(2\cos\left(\frac{\pi k}N\right)\right)^{2N-2}
\end{align*}
From IA Numbers and Sets we get that
\[
	\sum_{n=0}^{N-1}\binom{N-1}n\binom{N-1}n=\sum_{n=0}^{N-1}\binom{N-1}n\binom{N-1}{N-1-n}=\binom{2N-2}{N-1}.
\]
This gives the result
\[
\frac 1{2^{2M}}\binom{2M}M=\frac 1{M+1}\sum_{k=0}^M\cos^{2M}\left(\frac{\pi k}{M+1}\right)=\mathbb P(\text{getting } M \text{ heads tossing a coin } 2M \text{ times}).
\]
Just like with the usual Fourier transformation, we can define a conovuolution of sequences $ \{X_n\}_{n=0}^{N-1} $, $ \{Y_n\}_{n=0}^{N-1} $, defined as
\[
	(X*Y)_n=\sum_{m=0}^{N-1}X_{(n-m)\text{ mod } N}Y_m.
\]
\begin{proposition}
  \[
	  [\mathcal F(X*Y)]=\hat X_k\hat Y_k.
  \]
\end{proposition}
\pf
\begin{align*}
	\text{LHS} &= \sum_{n=0}^{N-1}\left(\sum_{m=0}^{N-1}X_{(n-m)\text{ mod } N} Y_m\right)e^{-i\left(\frac{2\pi n}N\right)k}\\
		   &= \sum_{m=0}^{N-1}Y_me^{-i(2\pi m/N)k}\sum_{n=0}^{N-1}X_{(n-m)\text{ mod } N}e^{-i(2\pi(n-m)/N)k}\\
		   &=\sum_{m=0}^{N-1} Y_me^{-i(2\pi m/N)k} \hat X_k\\
		   &=\hat X_k\hat Y_k.\qed
\end{align*}
Let's look at the computation complexity of the discrete Fourier transformation. From the sum, the time complexity of the discrete Fourier transformation computed naively is $ O(n^2) $. The Fast Fourier Transform (FFT) can be used to speed up if $ N=2^M $, then we can compute the Fourier transform in time complexity $ O(N\log N) $.
\section{PDEs on unbounded domains}
\subsection{Well-posedness}
Hadamard declared that a problem is well-posed if
\begin{enumerate}
	\item A solution exists;
	\item The solution is unique;
	\item The solution depends continuously on the given initial conditions and boundary data.
\end{enumerate}
(i) and (ii) are obvious, but (iii) is subtle. to maintain continuity we implicity mean we have certain norms or metrics inducing a topology we have to keep in mind. For example if we have some initial boundary value problem (IBVP) whose initial data and boundary data lie in $ X $ and time $ t>0 $ the solution $ u(\cdot, t) $ belongs to some $ Y $. For each $ t>0 $ we get some abstract map, $ S_t:X\to Y $. Continuity depends on the metrics $ d_X $ and $ d_Y $. Intuitively if we start close, we stay close.\par
For an example consider the IVP
\[
	\frac{\mathrm dx}{\mathrm dt}=-x,\quad x(0)=x_0.
\]
Clearly (i) and (ii) are satisfied, we have a solution
\[
  X_0(t)=x_0e^{-t}.
\]
Solution with initial data $ x(0)=x_1 $ is $ X_1(t)=x_1e^{-t} $. So
\begin{align*}
	|X_1(t)-X_0(t)|&=e^{-t}|x_1-x_0|\\
		       &\le |x_1-x_0|.
\end{align*}
So problem is well-posed for the usual metric on $ \R $. Let's look an ill-posed example. Consider the backwards heat equation IBVP on $ \Omega=(0,\pi) $.
\[
  \begin{cases}
	  \varphi_t+\varphi_{xx}=0 & \Omega\times (0,\infty) \\
	  \varphi = 0 & \partial\Omega\times (0,\infty)\\
	  \varphi = f & \Omega\times \{t=0\}
  \end{cases}.
\]
If $ f(x)=0 $ we get $ \varphi(x,t)=0 $.\\
If $ f(x)=f_n(x)=\frac 1n\sin(nx) $.\\
Then $ \varphi_n(x,t)=\frac 1ne^{n^2t}sin(nx) $, so
\[
	||f-f_n||_\infty=\sup_{(0,\infty)}|f(x)-f_n(x)|=\frac 1n\to 0.
\]
However
\[
	||\varphi-\varphi_n||_\infty=\frac 1ne^{n^2t}\to \infty.
\]
So the problem is ill-posed even locally in time.
\subsection{The Method of Characteristics}
We want to solve 
\[
	a(x,y)\frac{\partial u}{\partial x}+b(x,y)\frac{\partial u}{\partial y}=c(x,y,u).
\]
This equation is quasi-linear, since the complicated part (the left-hand side) is linear. We also have that $ u $ satisfies some initial data on some curve $ C $ in $ \R^2 $, i.e we want $ u(x,y)=\phi(x,y) $ where $ (x,y)\in C $.
\par
Consider curves $ (x,y)=(x(t),y(t)) $ defined by
\begin{align*}
	\frac{\mathrm dy}{\mathrm dx}=a(x,y), \quad \frac{\mathrm dy}{\mathrm dx}=b(x,y)
\end{align*}
and $ (x(0),y(0))\in C $. We call these the \textit{characteristic curves} for the PDE. We get a whole family of solutions determined by starting point $ (x(0),y(0)) $. Consider evolution of $ u(x,y) $ along a given characteristic, i.e set 
\[
  z(t)=u(x(t),y(t)).
\]
By the chain rule, we get that (writing $ x=x(t) $ and $ y=y(t) $),
\begin{align*}
	\frac{\mathrm dz}{\mathrm dt}&=\frac{\mathrm dx}{\mathrm dt}\frac{\partial y}{\partial x}+\frac{\mathrm dy}{\mathrm dt}\frac{\partial u}{\partial y}\\
				     &= a(x,y)\frac{\partial u}{\partial x}+b(x,y)\frac{\partial u}{\partial y}\\
				     &=c(x,y,z).
\end{align*}
Since $ u(x,y)=z $ on $ x=x(t) $ and $ y=y(t) $. Note that $ z(0)=u(x(0),y(0))=\phi(x(0),y(0)) $ since $ (x(0),y(0))\in C $. So
\[
	\frac{\mathrm dz}{\mathrm dt}=c(x,y,z)
\]
and $ z(0)=\phi(x(0),y(0)) $. We need to invert the relationship between $ (x,y) $ and $ (s,t) $ where $ s $ is the arc along $ C $ to the characteristic curve we need. Let's see an example
\[
	\frac{\partial u}{\partial x}+\frac{\partial u}{\partial y}=u,\quad u(x,0)=f(x), C=\{(s,0), s\in \R\}.
\]
The characteristic curves are
\[
	\frac{\mathrm dx}{\mathrm dt}=1,\quad \frac{\mathrm dy}{\mathrm dt}=1.
\]
hence we get solutions,
\[
  \begin{cases}
    x=t+x_0 \\
    y=t+y_0
  \end{cases}.
\]
We want $ (x_0,y_0)\in C $, so take $ (x_0,y_0)=(s,0) $ for $ s\in \R $, hence our characteristic curves are $ x=t+s $ and $ y=t $.
\[
	\frac{\mathrm dz}{\mathrm dt}=c(x,y,z)=z.
\]
This has a solution $ z(t)=z_0e^t $. Where $ z_0=z(0)=u(x_0,y_0)=u(s,0)=f(s) $. Therefore $ z(t,s)=f(s)e^t $.\par
We invert using the characteristic curves to get that
\[
  \begin{cases}
    t=y\\
    s=x-y
  \end{cases}.
\]
Hence finially, $ u(x,y)=z(t(x,y),s(x,y))=f(x-y)e^y $.\par
Now let's see an harder example. Take
\[
	(1+x^2)\frac{\partial u}{\partial x}+\frac{\partial u}{\partial y}=u+1,\quad u(0,y)=f(y), \quad C=\{(0,s): s\in \R\}.
\]
The characteristic curves are
\[
	\frac{\mathrm dx}{\mathrm dt}=1+x^2,\quad \frac{\mathrm dy}{\mathrm dt}=1.
\]
Solving gives $ x(t)=\tan(t+\arctan x_0) $ and $ y(t)=t+y_0 $. We want $ (x_0,y_0)\in C $ so $ (x_0,y_0)=(0,s) $. Therefore
\[
  \begin{cases}
    x=\tan t \\
    y=t+s=\arctan x + s
  \end{cases}
\]
To find $ z $, solve
\[
	\frac{\mathrm dz}{\mathrm dt}=z+1
\],
which has solution $ z(t)=-1+[z_0+1]e^t $. Recall that $ z_0=u(x_0,y_0)=u(0,s)=f(s) $, hence
\[
  z(t,s)=-1+(f(s)+1)e^t.
\]
Inverting this \textit{flow map} we get that
\begin{align*}
  t=\arctan x\\
  s=y-\arctan x.
\end{align*}
So 
\[
	u(x,y)=-1+[f(y-\arctan x)+1]e^{\arctan x}.
\]
\subsection{Classification of second order linear PDEs in 2 variables}
Consider the generic problem $ Lu=y $ where
\[Lu=a(x,y)\frac{\partial^2 u}{\partial x^2}+2b(x,y)\frac{\partial^2 u}{\partial x\partial y}+c(x,y)\frac{\partial^2u}{\partial y^2}+d(x,y)\frac{\partial u}{\parital x}+e(x,y)\frac{\partial u}{\partial y}+f(x,y)u \].
We want to introduce $ \xi=\xi(x,y) $ and $ \eta=\eta(x,y) $ such that the PDE simplifies. Write $ (x,y)=(x_1,x_2) $ and $ (\xi,\eta)=(\xi_1,\xi_2) $. Then
\[
	Lu=\sum_{i,j=1}^2a_{ij}(x,y)\frac{\partial^2u}{\partial x\partial y}+[\text{lower order stuff}],
\]
where
\[
	\{a_{ij}\}=\begin{pmatrix}
		a(x,y) & b(x,y) \\
		b(x,y) & c(x,y)
	\end{pmatrix}.
\]
\begin{proposition}
  If $ (x,y)\to (\xi,\eta) $ and $ U(\xi,\eta)=u(x,y) $ then $ Lu $ becomes
  \[
	  \tilde LU=\sum_{p,q=1}^2 A_{pq}\frac{\partial^2 U}{\partial \xi_p\partial \xi_q}+[\text{lower order terms}]
  \]
  where 
  \[
	  A_{pq}=\sum_{i,j=1}^2a_{ij}\frac{\partial \xi_p}{\partial x_i}\frac{\partial \xi_q}{\partial x_j}.
  \]
\end{proposition}
\pf By the chain rule and using summation convention, we get
\[
	\frac{\partial y}{\partial x_i}=\frac{\partial \xi_p}{\partial x_i}\frac{\partial U}{\partial \xi_p}.
\]
Hence
\[
	\frac{\partial^2 u}{\partial x_i\partial x_j}=\frac{\partial \xi_p}{\partial x_i}\frac{\partial \xi_q}{\partial x_j}\frac{\partial^2 U}{\partial \xi_p\partial \xi_q} + \frac{\partial^2 \xi_p}{\partial x_i\partial x_j}\frac{\partial U}{\partial \xi_p}.
\]
This means that
\[
	a_{ij}\frac{\partial^2u}{\partial x_i\partial x_j}=A_{pq}\frac{\partial^2U}{\partial \xi_p\partial \xi_q}+[\text{lower order terms}].\qed
\]
We can read of that
\begin{align*}
	A_{11}&=a\xi_x^2+2b\xi_x\xi_y+c\xi_y^2\\
	A_{12}&=a\xi_x\eta_x+b(\xi_x\eta_y+\xi_y\eta_x)+c\xi_y\eta_y\\
	A_{22}&=a\eta^2_x+2b\eta_x\eta_y+c\eta^2_y. 
\end{align*}
We can make $ A_{11}=0 $ and $ A_{22}=0 $ if $ M=\frac{\xi_x}{\xi_y} $ and $ N=\frac{\eta_x}{\eta_y} $ satifying $ az^2+2bz+c=0 $. This has roots
\[
	z=\left[\frac{-b\pm \sqrt{b^2-ac}}a\right].
\]
If $ M $ and $ N $ are chosen to satisfy this quadratic, we call the curves $ \xi(x,y)=\text{const.} $ and $ \eta(x,y)=\text{const.} $ the \textit{characteristic curves} of the PDE. On the characteristic curves
\[
	\frac{\mathrm d}{\mathrm dx}[\xi(x,y(x))]=\xi_x+\frac{\mathrm dy}{\mathrm dx}\xi_y=0.
\]
Similarly we have that
\[
	\eta_x+\frac{\mathrm dy}{\mathrm dx}\eta_y=0.
\]
This gives that
\[
	\frac{\mathrm dy}{\mathrm dx}=-\frac{\xi_x}{\xi_y}=-M,\quad \frac{\mathrm dy}{\mathrm dx}=-\frac{\eta_x}{\eta_y}=-N.
\]
So on characteristic curves,
\[
	\frac{\mathrm dy}{\mathrm dx}=-\left[\frac{-b\pm \sqrt{b^2-ac}}a\right].\tag{\star}
\]
This leads to a classification.
\begin{itemize}
	\item If $ b^2-ac<0 $ we say the PDE is \textit{elliptic} and has no real characteristic curves.
	\item If $ b^2-ac=0 $ we say the PDE is \textit{parabolic} and has a single family of characteristic curves.
	\item If $ b^2-ac>0 $ we say the PDE is \textit{hyperbolic} and has two families of characteristic curves.	
\end{itemize}
In hyperbolic regions, we can integrate up equation $ (\star) $ to get coordinates $ \xi=\xi(x,y) $, $ \eta=\eta(x,y) $ for which $ A_{11}=A_{22}=0 $. Our partial differential operator becomes
\[
	2A_{12}\frac{\partial^2 U}{\partial \xi \partial \eta}+[\text{lower order terms}].
\]
We call this the \textit{canonical form} of the PDE>.\par
Let's see an example of this on the wave equation with $ c=1 $.
\[
	\frac{\partial^2 u}{\partial x^2}-\frac{\partial ^2 u}{\partial y^2}=0.
\]
Thus, $ a=1, b=0, c=-1 $, so $ b^2-ac = 1 > 0 $, so the PDE is \textit{globally hyperbolic}. To get characteristic curves, we set
\[
	\frac{\mathrm dy}{\mathrm dx}=\mp 1.
\]
So integrating we get curves $ x\pm y=\text{const.} $ i.e we can take
\begin{align*}
  \xi(x,y)=x+y \\
  \eta(x,y)= x- y.
\end{align*}
In these coordinates we get that
\[
	4\frac{\partial^2 U}{\partial\xi\partial \eta}=0.
\]
Hence $ U(\xi,\eta)=A(\xi)+B(\eta) $. This gives solution
\[
  u(x,y)=A(x+y)+B(x-y).
\]
We can solve
\[
  \begin{cases}
	  u_{tt}-u_{xx}=0 & \R\times (0,\infty) \\
	  u=f & \R\times \{t=0\} \\
	  u_t=g & \R\times \{t=0\}
  \end{cases}.
\]
Set $ A(x)+B(x)=f(x) $ and $ A'(x)-B'(x)=g(x) $. So we get the solution to the IVP as
\[
	u(x,t)=\frac 12[f(x+t)+f(x-t)]+\frac 12 \int_{x-t}^{x+t}g(s)\mathrm ds.
\]
Let's see the example
\[
	xy\frac{\partial^2 u}{\partial^2x^2}-\frac{\partial^2 u}{\partial y^2}=0
\]
so $ a=xy, b=0, c=-1 $. Hence $ b^2-ac=xy $, so the hyperbolic region is the region $ \{x,y>0\} \cup \{x,y<0\} $. On the characteristic curves we have that
\[
	\frac{\mathrm dy}{\mathrm dx}=\mp\frac1{\sqrt{xy}}.
\]
Solving this in $ \{x,y>0\} $ region we get that
\[
	\frac 13 y^{3/2}\pm x^{1/2}=\text{const.}
\]
So this gives
\begin{align*}
	\xi(x,y)&=\frac 13 y^{3/2} + x^{1/2}\\
	\eta(x,y)&=\frac 13 y^{3/2} - x^{1/2}.
\end{align*}
So the PDE becomes
\[
	-\frac 12 y\frac{\partial^2 U}{\partial \xi\partial \eta}+[\text{lower order terms}]=0.
\]
Note that
\[
	y=\left[\frac 32(\xi+\eta)\right]^{2/3},
\]
so we can write our PDE in canonical form.








\end{document}

