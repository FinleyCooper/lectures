\documentclass{article}
\usepackage{../header}
\title{Analysis II}
\author{Notes made by Finley Cooper}
\newcommand{\eps}{\varepsilon}
\begin{document}
  \maketitle
  \newpage
  \tableofcontents
  \newpage
  \section{Uniform Convergence}
For a subset $ E\subseteq \R $, have a sequence $ f_n:E\to \R $. What does it mean for the sequence $ (f_n) $ to converge? The most basic notion for any $ x \in E $ require that the sequence of real numbers $ f_n(x) $ to converge in $ \R $. If this holds we can defined a new function $ f: E\to \R $ by setting each value to the limit of the function.
\begin{definition}
	(Pointwise limit) We say that $ (f_n) $ converges \textit{pointwise} if for all $ x $ in its domain we have that
	\[
		f(x)=\lim_{n\to\infty}f_n(x)
	\]
	converges. We write that $ f_n\to f $ pointwise.
\end{definition}
Are properties such as continuity, differentiability integrability, preserved in the limit? We'll use an example to show that continuity is not preserved.\par
We can see this by taking a sequence of functions which converge to a step function by taking tighter and tighter curvers which get steeper and steeper. For example take,
\[
	f_n:[-1,1]\to \R,\quad f_n(x)=x^{\frac 1{2n+1}}.
\]
So in the limit we get that
\[
  f_n(x)\to f(x)=\begin{cases}
	  1 & 0< x \le 1 \\
	  0 & x = 0 \\
	  -1 & -1\le x < 0 
  \end{cases}
\]
which is not continious.\par
For an example where integability is not preserved, let $ q_1,q_2,q_3,\dots $ be an enumeration of $ \Q\cap [0,1] $ and define
\[
  f_n(x)=\begin{cases}
	  1 & x\in\{q_1,\dots, q_n\} \\
	  0 & \text{otherwise}
  \end{cases}
\]
so we get $ f_n(x) $ continious everywhere on $ [0,1] $ apart from a finite number of points, then $ f_n $ is integrable on $ [0,1] $ (IA Analysis I). But,
\[
	\lim_{n\to\infty}f_n(x)=\boldsymbol{1}_\Q(x)
\]
which we know is not integrable.\par
If $ f_n\to f $ pointwise, $ f_n $ integrable, $ f $ integrable, does it follow that $ \int f_n\to\int f $? (Spoiler: No)
For example take $ f_n $ to be a 'spike' with height $ n $ and width $ \frac 2n $, concretely,
\[
  f_n(x)=\begin{cases}
	  n^2x& 0\le x \le \frac 1n \\
	  n^2(\frac 2n - x) & \frac 1n \le x \le \frac 2n \\
	  0 & \text{otherwise}
  \end{cases}
\]
So the integral of $ f_n $ over $ [0,1] $ is $ 1 $, but we can see that $ f_n $ converges pointwise to zero. So $ \int_0 ^1 f_n\to 1 $ but $ \int_0 ^1f\to0 $.\par So we need a better (stronger) notion for the convergence of a sequence of functions.
We can't use something too strong, such as $ f_n \to f$ if $ f_n $ is eventually $ f $ for large enough $ n $. We've got to find something inbetween. This is uniform convergence.
\begin{definition}
	(Uniform convergence) Let $ f_n,f: E\to \R $, for $ n\in\N $. We say that $ (f_n) $ converges \textit{uniformly} on $ E $ if the following holds. For all $ \eps>0 $, $ \exists N=N(\eps) $ such that for every $ n\ge N $ and for every $ x\in E $ we have that $ |f_n(x)-f(x)|<\eps $.
\end{definition}
\begin{remark}
  This statement is equivalent to the following,
  \[
	  \forall\eps >0,\exists N=N(\eps), \text{ s.t. } \forall n\ge N, \sup_{x\in E}|f_n(x)-f(x)|<\eps.
  \]
\end{remark}
Comparing this to pointwise convergence, $ \forall x \in E $ and $ \forall \eps>0 $, $ \exists N=N(\eps,x) $ such that $ n\ge N\implies |f_n(x)-f(x)|<\eps $. So we can change our $ N $ value for each individual $ x $. However we can't in uniform convergence, which makes this is stronger statement.\par
Hence we see Uniform convergence $ \implies $ Pointwise convergence.
This gives a nice way to compute uniform limits. If a function doesn't converge pointwise then we know it doesn't converge uniformly. If we know a sequence of functions converges pointwise to some limit function, then this function must be the limit of the uniform limit, if it exists.
\begin{definition}
	(Uniformly Cauchy) Let $ f_n:E\to \R $ be a sequence of functions. We say that $ (f_n) $ is \textit{uniformly Cauchy} on $ E $ if
	\[
		\forall \eps >0, \exists N=N(\eps) \st n,m\ge N\implies \sup_{x\in E}|f_n(x)-f_m(x)|<\eps.
	\]
\end{definition}
\begin{theorem}
	(Cauchy criterion for uniform convergence) Let $ (f_n) $ be a sequence of functions with $ f_n:E\to \R $. The $ (f_n) $ converges uniformly on $ E $ if and only if $ (f_n) $ is uniformly Cauchy on $ E $.
\end{theorem}
\pf Suppose that $ (f_n) $ is a sequence converging uniformly in $ E $ to some function $ f $. Given some $ \eps>0 $, there is a $ N $ such that $ \sup_{x\in E}|f_n(x)-f(x)|<\eps $ for all $ n\ge N $. By the triangle inequality $ \forall x\in E $, picking $ n,m\ge N $,
\begin{align*}
	|f_n(x)-f_m(x) & |\le |f_n(x)-f(x)|+|f_m(x)-f(x)| \\
 & \le \sup_E|f_n-f|+\sup_E|f_m-f| \\
 & < \eps + \eps\\
 & < 2\eps
\end{align*}
hence $ (f_n) $ is uniformly Cauchy.\\
For the converse, suppose that $ (f_n) $ is a sequence uniformly Cauchy in $ E $. Then the sequence of real numbers $ (f_n(x)) $ is Cauchy so by IA Analysis I, this sequence has a limit, call it $ f(x) $. So $ (f_n) $ converges pointwise to $ f $. Now we check that $ f_n\to f $ uniformly on $ E $. Pick any $ \eps >0 $ and note that by the hypothesis that $ (f_n) $ is uniformly Cauchy, there exists a number $ N $ such that for all $ n,m\ge N $ we have $ |f_n(x)-f_m(x)|<\eps $. Fix $ n\ge N $ and let $ m\to\infty $ in this. So since $ f_m(x) $ converges to $ f(x) $ pointwise, we get that
\[
  |f_n(x)-f(x)|\le \eps
\]
hence $ (f_n) $ converges uniformly in $ E $.\qed\par
For an example consider $ f_n:\R\to\R $ defined by $ f_n(x)=\frac x n $. So $ f_n\to 0 $ pointwise on $ \R $. But $ |f_n-0| $ is unbounded so the suprenum doesn't exist so $ f_n $ does not converge uniformly on $ \R $. However if we restrict the domain of $ f_n $ to $ [-a,a] $ then we get uniform convergence.
\begin{theorem}
	(Continuity is preserved under uniform limits) Let $ f_n,f:[a, b]\to \R $. Suppose that $ (f_n) $ converges to $ f $ uniformly on $ [a,b] $. If $ x\in [a,b] $ is such that $ f_n $ is continuous at $ x $ for all $ n\in \N $, then $ f $ is continuous at $ x $.
\end{theorem}
\pf Let $ \eps>0 $ by uniform convergence of $ f_n\to f $ we have some $ N\in \N $ such that for all $ n\ge N $,
\[
	\sup_{y\in[a,b]}|f_n(y)-f(y)|<\eps
\].
By continuity of $ f_N $ at $ x $ we have $ \delta=\delta(N,x,\eps)>0 \st y\in[a,b], |x-y|<\delta\implies |f_N(y)-f_N(x)|<\eps $.\\
Then $ y\in[a,b], |x-y|<\delta $ we ] have
\begin{align*}
	|f(y)-f(x)| & \le |f(y)-f_N(y)|+|f_N(y)-f_N(x)|+|f_N(x)-f(x)|\\
         & < \eps + \eps +\eps\\
	 & < 3\eps
\end{align*}
Hence $ f $ is continuous at $ x $. \qed\par
It is instructive to see where this proof goes wrong if we only assume that $ (f_n) $ converges to $ f $ pointwise.
\begin{corollary}
	(Uniform limits of continuous functions are continuous) If $ f_n,f:[a,b]\to \R $, and $ f_n\to f $ uniformly on $ [a,b] $ and if $ f_n $ is continuous on $ [a,b] $ for every $ n $ then $ f $ is continuous on $ [a,b] $.
\end{corollary}
\pf Immediate from the previous theorem.\qed\par
From now on we will denote $ C([a,b]) = \{f:[a,b]\to \R:f\text{ is continuous on } [a,b]\}. $
\begin{theorem}
	Let $ (f_n) $ be a uniformly Cauchy sequence of functions in $ C([a,b]) $ the it converges to a function in $ C([a,b]) $.
\end{theorem}
\pf Trivial from our theorems earlier proved. \qed
\begin{theorem}
	(Uniform convergence implies convergence of integrals) For $ f_n,f:[a,b]\to \R $ be such that $ f_n,f $ are bounded and integrable on $ [a,b] $. If $ f_n\to f $ uniformly on $ [a,b] $ then
	\[
	  \int_a^bf_n(x)\mathrm dx\to \int_a^bf(x)\mathrm dx
	\]
\end{theorem}
\begin{remark}
  The assumption that $ f $ is integrable is redundant. We will see later that integrability of $ f_n $ implies that $ f $ is integrable if $ f_n\to f $ uniformly
\end{remark}
\pf
\begin{align*}
	\left|\int_a^bf_n(x)\mathrm dx-\int_a^bf(x)\mathrm dx|&=\left|\int_a^bf_n(x)-f(x)\mathrm dx \\
							      &\le \int_a^b |f_n(x)-f(x)|\mathrm dx \\
							      &\le \sup_{x\in [a,b]}|f_n(x)-f(x)|(b-a) \to 0
\end{align*}
by assumption.
\subsection{Differentation and uniform convergence}
This is more subtle if $ f_n\to f $ uniformly on some interval and if $ f_n $ are differentiable it does not follow that
\begin{enumerate}
	\item That $ f $ is differentiable.
	\item Even if $ f $ is differentiable that $ f_n'(x)\to f(x) $.
\end{enumerate}
We can view this in the example of $ f_n:[-1,1]\to \R $ with $ f_n(x)=|x|^{1+\frac 1n} $. Hence we have that 
\[
	\lim_{x\to 0}\frac{f_n(x)-f_n(0)}{x}=\lim_{x\to 0}\mathrm{sgn}(x^{\frac 1n})=0
\]
So $ f_n $ is differentialbe at $ 0 $ with $ f_n(0)=0 $ and clearly $ f_n $ is differentiable everywhere where $ x=0 $ too. We can check that $ f_n\to |x| $ uniformly. But $ |x| $ is not differentiable at $ x=0 $.\par
Now consider the example $ f_n:\R\to\R $ with
\[
	f_n(x)=\frac{\sin(nx)}{\sqrt n}.
\]
So $ f_n\to 0 $ uniformly on $ \R $. So we have a differentiable limit but $ f_n'(x)=\sqrt n \cos(nx) $ which is not convergent as $ n\to\infty $. So we don't have $ f_n'(x)\to f'(x) $ pointwise on $ \R $.
\begin{theorem}
	Let $ f_n:[a,b]\to \R $ be a sequence of differentiable functions (at the end points this means that the one-sided derivative exists). Suppose that:
	\smallskip\begin{enumerate}
		\item $ f_n'\to g $ uniformly for some function $ g:[a,b]\to \R $.
		\item For some $ c\in[a,b] $ the sequence $ (f_n(c)) $ converges.
	\end{enumerate}\smallskip
	Then $ (f_n) $ converges uniformly to some function $ f:[a,b]\to \R $ where $ f $ is differentiable everywhere on $ [a,b] $ and $ f'(x)=g(x) $ for all $ x\in[a,b] $.
\end{theorem}
This proves that
\[
	\left(\lim_{n\to \infty}f_n\right)'=\lim_{n\to\infty}f'_n
\]
i.e. we can exchange the derivative and limit in this case.
\begin{remark}
If we assume that $ f'_n $ are continuous, then the proof is more straightforward and can be based on the fundamental theorem of calculus. 
\end{remark}
\pf By the mean value theorem applied to the difference $ (f_n-f_m) $ we have that for any $ x\in[a,b] $
\begin{align*}
	f_n(x)-f_m(x)&=f_n(c)-f_m(c)+(x-c)(f_n-f_m)'(x_{n,m}) \\
	\implies |f_n(x)-f_m(x)|&\le |f_n(c)-f_m(c)|+(b-a)|f_n'(x_{n,m})-f_m'(x_{n,m})|\\
	\implies \sup|f_n-f_m|&<|f_n(c)-f_m(c)|+(b-a)\sup|f_n'-f_m'|\to 0
\end{align*}
as $ n\to\infty $. So $ (f_n) $ is uniformly Cauchy and hence there is an $ f : [a,b]\to \R \st f_n\to f$ uniformly.\par
For the next part fix some $ y\in [a,b] $. Define
\[
  h(x)=\begin{cases}
	  \frac{f(x)-f(y)}{x-y} & x\ne y \\
	  g(y) & x=y
  \end{cases}
\]
Now we only have to estabilish that $ h $ is continuous at $ y $ to show that $ f $ is differentiable at $ y $ with $ f'(y)=g(y) $. Let
\[
  h_n(x)=\begin{cases}
	  \frac{f_n(x)-f_n(y)}{x-y} & x\ne y\\
	  f_n'(y) & x = y 
  \end{cases}
\]
then since $ f_n $ is differentiable at $ y $ we see that $ h_n $ is continuous on $ [a,b] $. The pointwise limit of $ (h_n) $ is $ h $ almost by definition since $ f'_n\to g $ at $ x=y $. Since the uniform limit of sequence of continuous functions is continuous, we just need to show that $ (h_n) $ is uniformly Cauchy on $ [a,b] $ since the limit must be $ h $ since it converges pointwise to $ h $.
\begin{align*}
  h_n(x)-h_m(x)=\begin{cases}
	  \frac{(f_n-f_m)(x)-(f_n-f_m)(y)}{x-y} & x\ne y \\
	  (f_n'-f_m')(y) & x=y
  \end{cases}.
\end{align*}
By the mean value theorem,
\begin{align*}
	h_n(x)-h_m(x)&=\begin{cases}
	  (f_n-f_m)'(x_{n,m}) \text{ for some } x_{n,m} \text{ between } x \text{ and } y & x\ne y\\
	  (f_n-f_m)'(y) & x=y
  \end{cases}\\
		\sup_{[a,b]}|h_n-h_m|&\le \sup_{[a,b]}|f_n'-f_m'|\to 0
\end{align*}
as $ n,m\to \infty $. So $ (h_n) $ is uniformly Cauchy so we're done. \qed
\begin{remark}
  $ f_n' $ need not be continuous consider
  \[
    f(x)=\begin{cases}
	    x^2\sin \frac 1x & x\ne 0 \\
	    0 & x = 0
    \end{cases}
  \]
  the $ f $ is differentiable on $ [-1,1] $ with f'(x) not continuous at $ x=0 $ and we can take $ f_n(x)=f(x) $ for all $ n $ (or $ f_n(x)=f(x)+\frac xn $.
\end{remark}
We have a shorter proof of the above theorem, assuming that $ (f_n') $ are continuous in addition to the hypothesis. For any $ x\in [a,b] $ we can write
\begin{align*}
  f_n(x)=f_n(c)+\int_c^xf_n'(t)\mathrm dt
\end{align*}
by FTC. Then
\begin{align*}
	|f_n(x)-f_m(x)|&=\left|f_n(c)-f_m(c)+\int_c^x(f_n'(c)-f_m'(c))\mathrm dt\right|\\
		       &\le |f_n(c)-f_m(c)|+\sup_{t\in[a,b]}|f_n'(t)-f_m'(t)|(b-a)\to 0
\end{align*} as $ n,m\to\infty $. So $ (f_n) $ is uniformly Cauchy, hence converges uniformly.\par
Note that 
\[
	\int_c^xf_n'(t)\mathrm dt\to \int_c^xg(t)\mathrm dt
\]
by uniform convergence of $ f_n'\to g $ which implies $ g $ is continuous and hence also integrable. We can let $ n\to\infty $ the first equation for $ f_n(x) $ which gives that
\[
  f(x)=f(c)+\int_c^xg(x)\mathrm dt
\]
So we can take the derivative of both sides giving that $ f'(x)=g(x)=\lim f_n'(x) $.\qed
\begin{proposition}
  If $ f_n,g_n:E\to \R $ with $ f_n\to f $ uniformly on $ E $ and $ g_n\to g $ uniformly on $ E $ then $ f_n+g_n $ converges uniformly to $ f+g $ on $ E $, and if $ h:E\to \R $ is a bounded function then $ hf_n\to hf $ uniformly on $ E $ also.
\end{proposition}
\pf On the example sheet.
\section{Series of functions}
\begin{definition}
	(Convergence of a series of functions) Let $ g_n: E\to \R $ for $ n\in\N $ then write
  \[
	  f_n=\sum_{j=1}^ng_j
  \]
  defined pointwise. Then we say that that,
  \begin{enumerate}
	  \item The series of functions $ \sum_{n=1}^\infty g_n $ is convergent at a point $ x\in E $ if the sequence of partial sums $ (f_n(x)) $ converges.
	  \item The series of functions $ \sum_{n=1}^\infty g_n $ uniformly on $ E $ if the sequence $ (f_n) $ converges uniformly on $ E $.
	  \item $ \sum_{n=1}^\infty g_n $ converges absolutely at $ x\in E $ if the series $ \sum_{n=1}^\infty |g_n(x)| $ converges.
	  \item $ \sum_{n=1}^\infty g_n $ converges absolutely uniformly on $ E $ if $ \sum_{n=1}^\infty |g_n| $ converges uniformly on $ E $.
  \end{enumerate}
\end{definition}
We know from IA Analysis I that absolutely convergence $ \implies $ convergence for a sequences in $ \R $. From this we have that if $ \sum_{n=1}^\infty  g_n $ converges absolutely at a point $ x\in E $ then $ \sum_{n=1}^\infty g_n $ converges at $ x $. Similiar to this we have the following proposition relating absolute uniform convergence and uniform convergence.
\begin{proposition}
	(Absolute uniform convergence implies uniform convergence) If $ g_n:E\to \R $ and if $ \sum_{n=1}^\infty g_n $ converges absolutely uniformly on $ E $ then $ \sum_{n=1}^\infty g_n$ converges uniformly on $ E $.
\end{proposition}
\pf Let $ f_n=\sum_{i=1}^n g_i $ Then
\begin{align*}
	|f_n(x)-f_m(x)|&=\left|\sum_{i=m+1}^ng(i)\right|\\
		       &= \sum_{i=m+1}^n|g_i(x)|=h_n(x)-h_m(x),\quad\text{where } h_n(x)=\sum_{i=1}^n|g_i(x)|\\
	\sup_{x\in E}|f_n(x)-f_m(x)|&\le \sup_{x\in E}|h_n(x)-h_m(x)|\to 0
\end{align*}
as $ n,m\to \infty $ so $ (f_n) $ converges uniformly on $ E $.\qed
\begin{remark}
  Uniform convergence and absolute pointwise convergence aren't enough to conclude that the series convergence absolutely uniformly.
\end{remark}
\begin{theorem}
	(Weierstrass M-test) Let  $ g_n:E\to\R $ be a sequence of functions and suppose that $ \exists M_n $ such that
	\[
		\sup_{x\in E}|g_n(x)|\le M_n
	\]
	and that
	\[
		\sum_{n=1}^\infty M_n
	\]
	converges. Then
	\[
		\sum_{n=1}^\infty g_n
	\]
	converges absolutely uniformly on $ E $.
\end{theorem}
\pf Let
\[
	h_n(x)=\sum_{j=1}^n |g_n(x)|
\]
for $ n>m $,
\begin{align*}
	h_n(x)-h_m(x)=\sum_{j=m+1}^n|g_j(x)|\le \sum_{j=k+1}^nM_j=\sum_{j=1}^nM_j -\sum_{j=1}^m M_j\\
	\implies \sup_{x\in E}|h_n(x)-h_m(x)|\le \left|\sum_{j=1}^n M_j-\sum_{j=1}^m M_j\right|\quad\forall n,m
\end{align*}
by assumption the right hand side $ \to 0 $ since $ \sum_{j=1}^\infty M_j $ is convergent, hence $ (h_n) $ is uniformly Cauchy hence converges uniformly.
\subsection{Power series}
We'll now specialise to the case where $ g_n(x)=c_n(x-a)^n $ for $ a,c_n\in \R $. This gives a real power series.
\begin{theorem}
	(Radius of convergence) Let $ \sum_{n=0}^\infty c_n(x-a)^n $ be a real power series then there exists a $ R\in[0,\infty] $ called the \textit{radius of convergence} of the power series such that
	\begin{enumerate}
		\item If $ |x-a|<R $ then the power series converges absolutely.
		\item If $ |x-a|>R $ then the power series diverges.
		\item R is given by
			\[
				R=\frac{1}{\limsup_{n\rightarrow\infty}|c_n|^{\frac 1n}}
			\]
			where if the limit is zero, then $ R=\infty $.
		\item For any $ r\in (0,R) $ we have the power series converges uniformly on $ [a-r,a+r] $, in particular the function that the power series converges to is continuous on $ (a-R,a+R) $.
	\end{enumerate}
\end{theorem}
\pf The proof for (i), (ii), and (iii) are in IA Analysis I. We'll just prove (iv). Note first that the power series converges absolutely at $ x=a+r $ i.e. we have that
\[
	\sum_{n=0}^\infty |c_n|r^n
\]
is convergent. Since $ |c_n(x-a)^n|\le |c_n|r^n $ for any $ x\in[a-r,a+r] $ we can apply the Weierstrass $ M $-test with $ M_n=|c_n|r^n $ to conclude that the series
\[
	\sum_{n=0}^\infty c_n(x-a)^n\to f
\]
converges absolutely uniformly on $ [a-r,a+r] $. It follows that $ f $ is continuous. at any point in $ (a-R,a+R) $ by picking $ r $ small enough.
\begin{remark}
   (Boundary behaviour. Let
   \[
	   f(x)=\sum_{n=0}^\infty c_n(x-a)^n
   \]
   with power series boundary $ R $ with $ 0< R<\infty $. If the power series converges at one of the boundary points of the interval of convergence, say at $ x=a+R $ i.e. $ \sum_{n=0}^\infty c_nR^n $ is convergent then
   \[
	   \lim_{x\to a+R}f(x)=\sum_{n=0}^\infty c_nR^n
   \]
   so $ f $ extends to $ (a-R,a+R] $ as a continuous function.
\end{remark}
Moreover, under the same conditions that $ \sum_{n=0}^\infty c_nR^n $ converges we have that the series converges uniformly on $ [a-r,a+r] $ for any $ r\in (0,R) $. Same discussion applies at the endpoint $ a-R $.
\begin{theorem}
	(Differentation of power series) Let $ \sum_{n=0}^\infty c_n(x-a)^n $ be a power series with radius of convergent $ R>0 $. Let
	\[
		f(x)=\sum_{n=0}^\infty c_n(x-a)^n
	\]
	defined on $ (a-R,a+R) $. We have the following
	\begin{enumerate}
		\item The derived series \[
		  \sum_{n=1}^\infty nc_n(x-a)^{n-1}
	  \]
	  has radius of convergent $ R $.
  \item $ f $ is differentiable on $ (a-R,a+R) $ with
	  \[
		  f'(x)=\sum_{n=1}^\infty nc_n(x-a)^{n-1}\quad\forall x\in(a-R,a+R)
	  \]
	\end{enumerate}
\end{theorem}
\pf
Before we prove the theorem let's give a definition we've seen slightly before.
\begin{definition}
	If $ (a_n) $ is a sequence of reals let 
	\begin{align*}
		p_n&= \sup\{a_m: m\ge n\}\\
		q_n&=\inf\{a_m:m\ge n\}.
	\end{align*}
        Then we define
	\begin{align*}
		\limsup_{n\to\infty}a_n&=\lim_{n\to\infty}p_n\\
		\liminf_{n\to\infty}a_n&=\lim_{n\to\infty}q_n.
	\end{align*}
	which exists in $ \R\cup\{\infty\} $ since $ (q_n) $ and $ (p_n) $ are monotone.
\end{definition}
\begin{align*}
	\limsup_{n\to\infty}(n|c_n|)^{\frac 1n}=\limsup_{n\to\infty}|c_n|^{\frac 1n}
\end{align*}
since we have that $ \lim_{n\to\infty}n^{\frac 1n} = 1$. So we have $ (i) $.\par
Define $ f_n(x)=\sum_{j=0}^jc_j(x-a)^j $ is clearly differentiable on $ \R $ with $ f_n'(x)=\sum_{j=1}^njc_j(x-a)^{j-1} $. By (i) we have that $ f_n'(x) $ converges uniformly on $ [a-r,a+r] $ for all $ r<R $ and $ f_n(a)=c_0 \forall n $ so $ (f_n(a)) $ converges. So the limit is differentiable in $ [a-r,a+r] $, with \[ f'(x)=\lim_{n\to\infty}f_n'(x) =\lim_{n\to\infty}\sum_{j=1}^njc_j(x-a)^{j-1}\].\qed\par
If we have a power series $ \sum_{n=1}^\infty c_n(x-a)^n $ we say the power series converges \textit{locally uniformly} on the interval of convergence $ (a-R,a+R) $ i.e. for all $ 0<r<R $ the power series converges uniformly on $ [a-r,a+r] $.
\begin{remark}
	By repeatedly applying the above theorem we get that if $f(x)= \sum_{n=1}^\infty c_n(x-a)^n $ has radius of convergence $ R>0 $ then $ f $ is differentiable to any order $ k\in\N $ in $ (a-R,a+R) $ and the $ k $th derivative is given by
	\[
		f^{(k)}(x)=\sum_{n=k}^\infty n(n-1)\cdots(n-k+1)c_n(x-a)^{(n-k)}.
	\]
	Plugging in $ x=a $ we get that 
	\[
		c_n=\frac{f^{(k)}(a)}{k!}.
	\]
	This says that $ f $ is uniquely determined by its values in an arbitrarily small interval around the point $ x=a $ since that's all we need to capture it's derivatives and form its power series.
\end{remark}
\section{Uniform continuity and Riemann integrability}
\subsection{Uniform continuity}
\begin{definition}
	(Uniform continuity) Let $ E\subseteq \R $ and let $ f:E\to \R $. We say that $ f $ is \textit{Uniformly continuous} on $ E $ if $ \forall \eps>0 $ there exists a $ \delta>0 $ such that $ \forall x,y\in E $ we have that
	\[
	  |x-y|<\delta\implies |f(x)-f(y)|<\eps
	\]
\end{definition}
This differs from our usual definition of continuity. We require some $ \delta $ to work for \textit{any} $ x,y\in E $ given some $ \eps $, rather than picking a $ \delta $ for each $ \eps $ and $ x $ value. Clearly uniform continuity implies continuity but the converse is not true. For an example consider $ f(x)=\frac 1x $ on $ (0,1) $. Clearly continuous at each $ x $, but not uniformly continuous since it gets too steep around $ 0 $.\par
Not even boundedness and continuity is enough for uniform continuity, consider $ \sin(\frac 1x) $, take $ x_n=\frac 1{2n\pi} $ and $ y_n=\frac 1{2n+\frac 12)\pi} $ then $ |f(x)-f(y)|=1 $, so no $ \delta $ works, we can always choose an $ n $ large enough.
\begin{theorem}
	Let $ [a,b] $ be a closed, bounded interval and $ f:[a,b]\to \R $ a continuous function. Then $ f $ is uniformly continuous.
\end{theorem}
\pf Argue by contradiction. Suppose that $ f $ is not uniformly continuous, so there exists an $ \eps>0 $ such that for all $ \delta>0 $ there is a pair of points $ x,y\in [a,b] $ such that $ |y-x|<\delta $ but $ |f(x)-f(y)|\ge \eps $. Now let $ \delta_n=\frac 1n $, so we get a sequence of functions $ x_n $ and $ y_n $ satisfying the above for each $ \delta_n $. By Bolzano-Weiestrass, there exists a subsequence $ (x_{n_k}) $ that converges to a point $ x\in [a,b] $.
\begin{align*}
	|x-y_{n_k}|\le |x-x_{n_k}|+|x_{n_k}-y_{n_k}\le |x-y_{n_k}|+\frac 1{n_k}\to 0\text{ as } n\to\infty 
\end{align*}
By the continuity of $ f $ at $ x $ we get $ f(x_{n_k})\to f(x) $ and $ f(y_{n_k})\to f(x) $. But this is contradiction since $ f(x) $ and $ f(y_{n_k}) $ are always seperated by some distance $ \eps $.\qed\par
We can actually strengthen this theorem.
\begin{theorem}
	Let $ f:[a,b]\to \R $ where $ -\infty<a<b<\infty $ be any function. Suppose that there is a collection $ \mathcal C $ of open intevals $ I\subseteq \R $ such that if
	\[
		F=[a,b]\setminus\bigcup_{I\in\mathcal C}I
	\]
	then $ f $ is continuous at every point in $ F $ (i.e. the set of discontinuities is contained in the union). Then $ \forall \eps>0 \exists\delta>0\st $ $ x\in F,y\in [a,b] $, with $ |x-y|<\delta \implies |f(x)-f(y)|<\eps $.
\end{theorem}
\pf Same as above, using the fact that $ F $ is \textit{closed} so it contains all of its limit points.\par
Let's show some applications of uniform continuity.
\subsection{Riemann Integration}
We'll do a quick recap of Riemann integration. For full proofs, look at IA Analysis I. Let $ f: [a,b]\to \R $ be a bounded function. Say that $ m\le f(x)\le M $ for $ m,M\in \R $. Let $ P =\{a_0=a, a_1,a_2,\dots, a_n=b\} $ be a partition of the interval $ [a,b] $ with $ a_0<a_1<\cdots<a_n $. We will write $ P=\{a_0=a<a_1<\cdots<a_n=b \} $ as shorthand.\par
We write that $ I_j=[a_j,a_{j+1}] $ for $ 0\le j<n $. Define the upper sum of $ f $ with $ P $ as
\[
	U(P,f)=\sum_{j=0}^{n-1}(a_{j+1}-a_j)\sup_{I_j}f
\]
and the lower sum of $ f $ with $ P $ as
\[
	L(P,f)=\sum_{j=0}^{n-1}(a_{j+1}-a_j)\inf_{I_j}f.
\]
We can see immediately that $ m(b-a) \le L(P,f)\le U(P,f)\le M(b-a) $. When we refine the partition by adding finitely many new points the uppers sum decreases or stays the same, and the lower sum increases or stays the same. So now we can define the upper and lower Riemann integral as
\begin{align*}
	I^*(f)&=\inf_PU(P,f)\\
	I_*(f)&=\sup_PL(P,f).
\end{align*}
We say that $ f $ is Riemann integrable if $ I^*(f)=I_*(f) $. We denote
\[
  \int_a^bf(x)\mathrm dx
\]
as this common value.
\begin{theorem}
	(Riemann criterion for integrability) For $ f:[a,b]\to \R $ bounded, $ f $ is integrable if and only if for all $ \eps>0 $ there exists a partition $ P $ of $ [a,b] $ such that
	\[
	  U(P,f)-L(P,f)<\eps
	\]
\end{theorem}
\pf In IA Analysis I.
\begin{theorem}
	Let $ f:[a,b]\to[A,B] $ be integrable and $ g:[A,B]\to\R $ continuous. Then the composite function $ g\circ f:[a,b]\to\R $ is integrable.
\end{theorem}
We may ask does this hold is we switch the order? i.e. given the both conditions is $ f\circ g$ always be integrable?\par
\pf Since $ g $ is continuous in a bounded interval, it is uniformly continuous. Given any $ \eps >0 $ there is a $ \delta $ such that $ x,y\in [A,B] $ with $ |y-x|<\delta\implies|g(x)-g(y)|<\eps $. We also have by integrability that there exists a partition $ P $ such that $ U(P,f)-L(P,f)<\eps' $ for all $ \eps' >0 $.
\begin{align*}
	U(P,g\circ f)-L(P, g\circ f)=\sum (a_{j+1}-a_j)\left(\sup_{I_j}g\circ f-\inf_{I_j}g\circ f\right)
\end{align*}
Take $ J=\left\{j: \sup_{I_j}f -\inf_{I_j}f\le \delta \right\}$. For any $ j\in J $ for all $ x,y \in I_J $ we must have that
\[
	|f(x)-f(y)|\le \sup_{z_1,z_2\in I_j}(f(z_1)-f(z_2))=\sup_{I_j}-\inf_{I_j}\le \delta.
\]
Hence we get that
\[
  |g\circ f(x)-g\circ f(y)|<\eps
\]
so
\begin{align*}
	\sup_{I_j}\left(g\circ f(x)-g\circ f(y)\right)\le \eps\\
	\sup_{I_j}g\circ f - \inf_{I_j}g\circ f\le \eps
\end{align*}
which gives that
\begin{align*}
	U(P,g\circ f)-L(P,g\circ f)&=\sum_{j=0}^n(a_{j+1}-a_j)\left(\sup_{I_j}g\circ f-\inf_{I_j}g\circ f\right)\\
				   &=\sum_{j\in J}(a_{j+1}-a_j)\left(\sup_{I_j}g\circ f-\inf_{I_j}g\circ f\right)+\sum_{j\notin J}(a_{j+1}-a_j)\left(\sup_{I_j}g\circ f-\inf_{I_j}g\circ f\right),\\
				   &\le \eps(b-a)+2\sup_{[A,B]}|g|\sum_{j\notin J} (a_{j+1}-a_j)
\end{align*}
hence it suffices to make the sum over the $ j $s not in $ J $ small enough. We know that
\[
	\sum_{j\notin J}(a_{j+1}-a_j)<\frac{\eps'}\delta
\]
so if we pick $ \eps'=\eps \delta $ we get that 
\[
	U(P,g\circ f)-L(P,g\circ f)<\left((b-a)+2\sup_{[A,B]}|g|\right)\eps.\qed
\]
\begin{corollary}
  If $ f $ is continuous then it is integrable
\end{corollary}
\pf Apply the theorem with $ g=\mathrm{id} $ which is clearly integrable.\qed
\begin{theorem}
	(Uniform limits of integrable functions are integrable) Suppose we have $ f_n:[a,b]\to\R $ be a sequence of Riemann integrable functions and $ f_n\to f $ uniformly. Then $ f $ is bounded, Riemann integrable and
	\[
		\int_a^bf_n\to \int_a^bf
	\]
\end{theorem}
\pf 
\[
	\sup_{[a,b]}|f|\le \sup_{[a,b]}|f-f_n|+\sup_{[a,b]}|f_n|\le 1+\sup_{[a,b]}|f_n|
\]
for $ n $ sufficently large (setting $ \eps=1 $). Hence $ f $ is bounded.\par
Let $ P=\{a_0,\dots, a_m\} $ be a partition of $ [a,b] $. Given some $ \eps>0 $ and consider
\begin{align*}
	U(P,f)-L(P,f)&=\sum_{j=0}^{m-1}(a_{j+1}-a_j)\left(\sup_{I_j}f-\inf_{I_j}f\right)\\
	&=\sum_{j=0}^{m-1}(a_{j+1}-a_j)\left(\sup_{I_j}(f-f_n+f_n)-\inf_{I_j}(f-f_n+f_n)\right)\\
	&\le \sum_{j=0}^{m-1}(a_{j+1}-a_j)\left(\sup_{I_j}(f-f_n)+\sup_{I_j}(f_n)-\inf_{I_j}(f-f_n)-\inf_{I_j}(f_n)\right)\\
	&\le U(P,f_n)-L(P,f_n)+2(a-b)\sup_{[a,b]}|f-f_n|
\end{align*}
So for our $ \eps> 0$ choose some $ N $ such that $ 2(b-a)\sup_{[a,b]}|f-f_N|\le\frac \eps 2 $ by uniform convergence. Now also choose a partition $ P $ such that $ U(P,f_N)-L(P,f_N)<\frac \eps2 $ since $ f_N $ is Riemann integrable. Hence $ U(P,f)-L(P,f)<\frac \eps 2+ \frac\eps 2=\eps $ for any $ \eps>0 $ so $ f $ is integrable by the Riemann criterion. The last part have been proved previously in the course.\qed\par
\textit{Non-examinable} We'll now prove an equivalent condition for a function to be Riemann integrable. First we'll set up some frameworks. For a function $ f:[a,b]\to\R $ bounded, we use $ \mathcal D_f $ to denote its set of discontinuities. We know that there are functions with $ \mathcal D_f $ non-empty which are still Riemann integrable, such as Thomae's function which has $ \mathcal D_f=\Q $. We also know that all monotone functions are integrable. What condition on $ \mathcal D_f $ do we need for integrability?
\begin{definition}
	(Null set) A subset $ \mathcal R\subseteq \R $ is said to be a \textit{null set} (or a set of \textit{Lebesgue measure zero}) if $ \forall\eps>0 $ there exists an at most countable collection of open intevals $ I_j=(a_i,b_i) $ such that
	\[
		\mathcal D\subseteq \bigcup_{i=1}^nI_i
	\]
	and
\[
	\sum_{j=1}^\infty|I_j|\le \eps
\]
where $ |I_j|=b_j-a_j $.
\end{definition}
We have a few examples of null sets.
\begin{enumerate}
	\item The empty set and singleton sets are null.
	\item Any subset of small enough sets are null.
	\item Any countable union of null sets is null (namely $ \Q $ is a null set and any other countable set like the algebraic numbers).
	\item The (standard) Cantor set is a null set even though it's uncountable.
	\item However not every set is a null set, every (open or closed) interval is not a null set.
\end{enumerate}
Now for the big theorem completely characterising Riemann integrable functions.
\begin{theorem}
	(Lebesgue's theorem on the Riemann integral) Let $ f:[a,b]\to\R $ bounded. Then $ f $ is Riemann integrable if and only if $ \mathcal D_f $ is a null set.
\end{theorem}
\pf See Part II Probability and Measure.
\begin{remark}
  Many results on Riemann integration are direct corollaries from Lebesgue's thereom. For example from IA Analysis I Example Sheet 3 we know that the set of discontinuities for a monotone function is countable. Hence for a monotone function $ \mathcal D $ is a null set and thus $ f $ is Riemann integrable.\\
  Also if $ f:[a,b]\to [A,B] $ is integrable, and $ g:[A,B]\to \R $ is continuious, then $ g\circ f $ is integrable can be proved too. Clearly, $ g\circ f $ is bounded, since $ g $ is bounded. Since $ f $ is integrable, we know that $ \mathcal D_f $ is null. But $ \mathcal D_{g\circ f}\subseteq \mathcal D_{f} $ (since if $ f $ is continuous at $ x $ then $ g\circ x $ is continuous at $ x $ since $ g $ is continuous. Hence $ \mathcal D_{g\circ f} $ is null, so $ g\circ f $ is integrable by Lebesgue's theorem.\\
  Finially if we have a sequence $ f_n:[a,b]\to \R $ integrable converging uniformly to $ f $, then $ f $ is integrable. We can also prove this using Lebesgue's theorem since $ \mathcal D_{f_n} $ is null by Lebesgue's theorem for all $ n\in \N $, so $ \bigcup_{n\in \N}\mathcal D_{f_n} $ is null too. But $ \mathcal D_f\subseteq \bigcup_{n\in \N}\mathcal D_{f_n} $ (Example Sheet 1), so it's also null, hence $ f $ is integrable.\par
Here's a new result that can be deduced from Lebesgue's theorem.
\end{remark}
\begin{corollary}
	If $ f :[a,b]\to \R $ is integrable, then $ |f| $ is integrable on $ [a,b] $. Moreover
	\[
		\int_a^b|f|=0\iff f=0\ \text{except on a null set}
	\]
	So there exists a null set $ N\subseteq [a,b] $ such that $ f(x)=0 $ for all $ x\in [a,b]\setminus N $. We say that $ f=0 $ \textit{almost everywhere} on $ [a,b] $.
\end{corollary}
\pf \textit{Exercise.}\qed
\par
This concludes our non-examinable interlude.
\section{Metric and Normed Spaces}
\begin{definition}
(Metric Space) Let $ X $ be any set. A \textit{metric} (or distance function) on $ X $ is a function, $ d:X\times X\to \R $ satisfying the following for any $ x,y,z\in X $,
\begin{enumerate}
	\item $ d(x,y)\ge 0 $ and $ d(x,y)=0\iff x=y $;
	\item $ d(x,y)=d(y,x) $;
	\item $ d(x,y)\le d(x,z) + d(z,y) $.
\end{enumerate}
We call such a pair $ (X,d) $ a \textit{metric space}.
\end{definition}
\begin{definition}
	(Normed Space) Let $ V $ be a real vector space. A \textit{norm} on $ V $ is a function $ ||\cdot ||: V\to \R $ satisfying for any $ x,y\in V $, and any $ \lambda \in\R $,
	\begin{enumerate}
		\item $ ||x||\ge 0 $ and $ ||x||=0\iff x=0 $;
		\item $ ||\lambda x||=|\lambda|\cdot||x||$;
		\item $ ||x+y||\le ||x||+||y|| $.
	\end{enumerate}
	We say that $ (V,||\cdot ||) $ is a \textit{normed space}.
\end{definition}
\begin{proposition}
  If $ (V,||\cdot||) $ is a normed space, and if $ d:V\times V\to \R $ is defined by $ d(x,y)=||x-y|| $, then $ (V,d) $ is a metric space.
\end{proposition}
\pf \textit{Exercise.}\qed\par
Let's go over a few examples.
\begin{enumerate}
\item (Finite dimensional normed spaces) The prototypical example of a normed space is the Euclidean space $ \R^n=\{(x_1,x_2,\dots \}: x_i\in \R\} $, with $ n\in \N $ fixed. We know that this defines a vector space and we can define various different norms on the vector space. Taking $ V=\R^n $ with its usual vector space structure, we can define several useful norms on $ V $:
	\begin{enumerate}
		\item The Euclidean norm (or the $ \ell_2 $-norm, defined by \[ ||x||_{\ell_2}=||x||_2=\left(\sum_{i=1}^n |x_i|^2\right)^{\frac 12}. \]
			All the requirements are simple to prove apart from the triangle inequality.
			\begin{align*}
				||x-y||_2^2&=\sum_{i=1}^n(x_i+y_i)^2\\
					   &=||x||^2_2+||y||_2^2+2\sum_{i=1}^nx_iy_i\\
					   &\le ||x||^2_2+||y||^2_2+2||x||_2^2||y||_2^2 \quad\text{by Cauchy-Schwarz}\\
					   &= ||x||^2_2+||y||^2_2
			\end{align*}
		\item The $ \ell_1 $-norm on $ \R^n $ defined as $ ||x||_1=\sum_{i=1}^n|x_i| $.
		\item We can also define the $ \ell_\infty $-norm as $ ||x||_\infty=\sup\{|x_i|:1\le i\le n\} $.
			\begin{remark}
			  More generally, for $ x\in\R^n $ we can define the $ \ell_p $-norm as
			  \[
				  ||x||_p=\left(\sum_{i=1}^n|x_i|^p\right)
			  \]
			  and it turns out that for $ p\le 1 $ this is indeed a norm (however the triangle inequality is non-trivial to proof). Moreover if we let $ p\to\infty $ we recover $ ||x||_\infty $.
			\end{remark}
	\end{enumerate}
\item ($ \R^\N $) Let's look at the infinite sequences of real numbers. We write $ \R^\N $ for the set of real sequences $ (x_k)_{k\in\N} $. This is vector space under addition defined by termwise addition and scalar multiplication.
	\begin{enumerate}
		\item We look at the space $ \ell_1 =\{ (x_k)\in\R^\N: \sum_{k=1}^\infty |x_i|<\infty\} $. This is a linear subspace of $ \R^\N $. We can turn this into a normed space by defining,
			\[
				||x||_{\ell_1}=||x||_1=\sum_{k=1}^\infty|x_k|.
			\]
		\item Likewise, $ \ell_2=\{(x_k)\in \R^\N: \sum_{i=1}^\infty x_i^2<\infty\} $ is a linear subspace of $ \R^\N $ and define the $ \ell_2 $ norm as
			\[
				||x||_{\ell_2}=||x||_2=\left(\sum_{i=1}^\infty |x_i|^2\right)^{\frac 12}.
			\]
		\item We can also define $ \ell_\infty=\{(x_k)\in\R^\N :\sup_{k\ge 1}|x_k|<\infty\} $ i.e. the space of bounded sequences. We can define define the $ \ell_\infty $ norm as
			\[
				||x||_{\ell_\infty}=||x||_\infty=\sup_{k\ge 1}|x_k|.
			\]
		\begin{remark}
			More generally, let $ \ell_p=\{(x_k)\in\R^\N:\sum_{i=1}^\infty|x_i|^p\le \infty $. Then $ \ell_p $ is a subspace of $ \R^\N $, and the $ \ell_p $-norm is defined as
				\[
					||x||_p=\left(\sum_{i=1}^\infty |x_i|^p\right)^\frac1p.
				\]
			And again we can see that the $ \ell_\infty $ norm is the limit of the $ \ell_p $ norm as $ p\to \infty $.
		\end{remark}
	\end{enumerate}
\item (The space of continuous functions in a bounded, closed interval) Define a vector space as
	\[
		V=C([a,b])=\{f:[a,b]\to\R:f\ \text{is continuous}\}.
	\]
	This forms a vector space under pointwise addition and scalar multiplication of functions.
	\begin{enumerate}
		\item The $ L^1 $-norm
			\[
				||f||_{L^1([a,b])}=||f||_{L^1}=||f||_1=\int_a^b|f(x)|\mathrm dx.
		\]
		\item The $ L^2 $-norm
			\[
				||f||_{L^2}=||f||_2=\left(\int_a^b|f(x)|^2\mathrm dx\right)^{\frac 12}.
			\]
		\item The $ L^\infty $-norm or the \textit{uniform norm}
			\[
				||f||_{L^\infty}=||f||_\infty=\sup_{x\in[a,b]}|f(x)|.
			\]
	\end{enumerate}
	It is easy to check that these are norms, we just need to know the Cauchy-Schwarz theorem for integrals to prove the triangle inequality for $ L^2 $.
	\[
		\int_a^b|f\cdot g|\le \left(\int_a^b |f|^2\right)^{\frac 12}\left(\int_a^b |g|^2\right)^{\frac 12}.
	\]
	Which we can prove by considering,
	\[
	  \varphi(t)=\int_a^b(|f|-t|g|)^2\ge 0,
	\]
	which is a quadratic in $ t $, so using the fact that the discriminant is non-positive (with the $ g=0 $ case being trivial).
\begin{remark}
	By the previous proposition, all of these examples are naturally metric spaces. For example the \textit{Euclidean metric} is $ d_E(x,y)=||x-y||_2 $ on $ \R^n $ for $ x,y\in \R^n $ and the \textit{uniform metric} on $ C([a,b]) $, $ d(f,g)=||f-g||_\infty=\sup_{x\in[a,b]}|f(x)-g(x)| $.
\end{remark}
\begin{remark}
	Integral norms such as $ L^1 $, are more naturally defined on the larger space of integrable functions, $ \mathcal R([a,b]) $ which is a vector space under pointwise addition and scalar multiplication, where we have that $ C([a,b])\subseteq \mathcal R([a,b]) $. However we have a problem since there are functions in $ \mathcal R([a,b]) $ which have norm of zero but aren't the zero function. But by a previous corollary of Lebesgue's theorem, we have that \[
		\int_a^b|f|=0\implies f=0\ \text{almost everywhere on } [a,b].
	\]
	So we can turn $ \mathcal R([a,b]) $ into a normed space by defining an equivalence relation on the space by setting $ f\sim g $ if $ f=g $ almost everywhere. Then the space $ \mathcal R([a,b])/\sim $ is now a normed vector space. Everything is well-defined independent of equivalence class representatives by the corollary of Lebesgue's theorem.
\end{remark}
\item (Discrete metric) For any set $ X $ let
	\[
	  d(x,y)=\begin{cases}
		  0 & x=y\\
		  1 & x\ne y
	  \end{cases}
	\]
\item (New metrices from given ones) Let $ (X,d) $ be a mretric space
	\begin{enumerate}
		\item Define $ g:X\times X\to \R $ by $ g(x,y)=\min\{1,d(x,y)\} $.
		\item $ h:X\times X\to \R $ by $ h(x,y)=\frac{d(x,y)}{1+d(x,y)} $.
		\item Take $ X=\R^2 $, then define
			\[
			  d(x,y)=\begin{cases}
				  ||x-y||_2 & \text{if } x=ty\ \text{for some } t\in \R\\
				  ||x||_2+||y||_2 & \text{otherwise}
			  \end{cases}.
			\]
			This is the French railways metric or the SNCF metric.
	\end{enumerate}
\end{enumerate}
\begin{definition}
	(Metric subspaces) If $ (X,d) $ is a metric space, let $ Y\subseteq X $ be any subset. Then the restriction
	\[
		d\mid_{Y\times Y}:Y\times Y\to \R
	\]
	is a metric on $ Y $ called the \textit{induced metric} or the \textit{subset metric}.
\end{definition}
\subsection{Open and closed subsets}
We will now look at the very important definitions of open and closed subsets in a metric space.
\begin{definition}
	(Open ball) Let $ (X,d) $ be a metric space. Then for any $ a\in X $ and any $ r>0 $. The \textit{open ball} with radius $ r $ and centre $ a $ is the set
	\[
		B_r(a)=\{x\in X:d(x,a)<r\}.
	\]
	This is our abstraction of $ \varepsilon $-neighbourhoods on $ \R $ for general metric spaces.
\end{definition}
\begin{definition}
	(Open set) Let $ (X,d) $ be a metric space. Then a subset $ U\subseteq X $ is \textit{open} if for all $ a\in U $ there exists a radius $ r>0 $ such that $ B_r(a)\subseteq U $.
\end{definition}
\begin{definition}
	(Closed set) Let $ (X,d) $ be a metric space. Then a subset $ E\subseteq X $ is \textit{closed} if $ X\setminus E $ is open.
\end{definition}
The property of being open or closed for a subset is relative to the containing ambient space. For example consider $ X=\R $ with the Euclidean metric. Then consider $ Y=[0,1)\cup \{2\} $ with the induced metric. We can see that $ [0,1) $ is neither open or closed in $ X $. Looking at $ Y $ however $ [0,1) $ is both open and closed in $ Y $.
\begin{proposition}
  Let $ (X,d) $ be a metric space. Then
  \begin{enumerate}
	  \item Any open ball $ B_r(a) $ is an open set;
	  \item Any singleton $ \{x\} $, $ x\in X $ is closed.
  \end{enumerate}
\end{proposition}
\pf Let $ y\in B_r(a) $, let $ r_1=r-d(y,a) $. Then $ r_1>0 $ since $ y\in B_r(a) $, so $ d(y,a)<r $. Take some $ z\in B_{r_1}(y) $. So
\[
  d(z,a)\le d(z,y)+d(y,a)<r_1+d(y,a)=r,
\]
hence $ z\in B_r(a) $, so $ B_{r_1}(y)\le B_r(a) $.\par
For the second part, take a point $ z\in X\setminus \{x\} $, then $ r=d(x,z)>0 $. So $ x\notin B_r(z) $, therefore $ B_r(z)\subseteq X\setminus \{x\} $. Hence $ X\setminus \{x\} $ is open, so $ \{x\} $ is closed.\qed
\begin{theorem}
  Let $ (X,d) $ be a metric space. We have the following,
  \begin{enumerate}
	  \item The union of any (possibly uncountable) collection of open set is open.
	  \item The intersection of any finite collection of open sets is open.
	  \item The empty set, $ \emptyset $, and the whole set, $ X $, are both open.
  \end{enumerate}
\end{theorem}
\pf \textit{Exericse.}\qed\par
By taking complements of sets we get th
By taking complements of sets we get the corresponding theorem.
\begin{theorem}
  Let $ (X,d) $ be a metric space. We have the following,
  \begin{enumerate}
	  \item The intersection of any (possibly uncountable) collection of closed sets is closed.
	  \item The union of any finite collection of closed sets is closed.
	  \item The empty set, $ \emptyset $, and the whole set, $ X $, are both closed.
  \end{enumerate}
\end{theorem}
Note that the "finite" is important for part (ii) of both theorems. For example consider the metric space $ \R $ over the Euclidean metric, then
\[
	\bigcap_{n=1}^\infty\left(-\frac 1n,\frac 1n\right)=\{0\}
\]
which is not open. Similarly we have that
\[
	\bigcup_{n=1}^\infty \left(\R\setminus \left(-\frac1n,\frac 1n\right)\right)=\R\setminus \{0\}
\]
which is not closed.
\begin{definition}
	(Convergence of sequences in metric spaces) Let $ (X,d) $ be a metric space. A sequence $ (x_k) $ in $ X $ is said to converge to a point $ x\in X $ if $ d(x_k,x)\to 0 $ as $ k\to\infty $. So $ \forall \varepsilon>0 $, $ \exists N $ such that $ k>N\implies d(x_k,x)<\varepsilon \iff x_k\in B_\varepsilon(x) $.
\end{definition}
It's clear from the $ \varepsilon $ definition if $ x_k\to x$ we must have that $ x_{n_k}\to x $ for any subsequence $ x_{n_k} $. 
\begin{proposition}
	(Uniquness of the limits) Let $ (X,d) $ be a metric space, and $ (x_k) $ be a sequence in $ X $ with $ x_k\to x $ and $ x_k\to y $, then $ x=y $.
\end{proposition}
\pf We have that
\[
  d(x,y)\le d(x_k,y) + d(x_k,x)\to 0
\]
as $ k\to\infty $. Hence $ d(x,y)=0 $, so $ x=y $.\qed
\begin{remark}
  It is possible that the same sequence in $ X $ has different limits with respect to different metrices on $ X $. For example take $ X=\R $ and let $ f:\R\to\R $ with
  \[
    f(x)=\begin{cases}
	    1 & x=0 \\
	    0 & x = 1\\
	    x & \text{otherwise}
    \end{cases}.
  \]
Then if we take $ d_1(x,y)=|f(x)-f(y)| $. Then the sequence $ x_k=\frac 1k\to 0 $ in the Euclidean metric, but $ x_k\to 1 $ in the $ d_1 $ metric. This example seems a bit contrived, but this can happen with respect to two different norms (only if the normed space is infinite dimensional). This is because any two norms on a finite dimensional vector space are Lipschitz equivalent (more to come later).
\end{remark}
\begin{proposition}
	(Convergence in $ \R^n $ with the $ \ell_2 $ norm) Convergence in $ \R^n $ with respect to the Euclidean norm is equivalent to the convergence of the coordinates (as real numbers). Formally, if $ x^{(k)}=\left(x^{(k)}_1,\dots, x^{(k)}_n\right) $ is a sequence in $ \R^n $ with $ k\in \N $, and $ x=(x_1,\dots, x_n)\in \R^n $, then
	\[
		x^{(k)}\to x\ \text{in}\ ||\cdot||_2\iff x^{(k)}_j\to x_j\ \forallj\in \{1,2,\dots, n\}.
	\]
\end{proposition}
\pf Fix some $ \varepsilon >0 $ The there exists some $ N $ such that $ k\ge N\implies ||x^{(k)}-x||_2<\varepsilon $. So
\[
	\sum_{j=1}^n(x_j^{(k)}-x_j)^2<\varepsilon^2
\]
so $ \left|x_j^{(k)}-x_j|<\varepsilon $ for all $ j $ with $ k\ge N $.\par
In the other direction, for any fixed $ j $, there is some $ N_j $ such that $ k\ge N_j $ implies that $ |x_k^{(k)}-x_j|<\frac \varepsilon{\sqrt n} $. So if $ k\ge\max\{N_j:j=1,\dots, n\} $, then
\[
	||x^{(k)}-x||_2=\left(\sum_{j=1}^n(x_j^{(k)}-x_j)^2\right)^\frac12<\varepsilon.
\]
So we're done.\qed
\begin{remark}
	The convergence in $ (C([a,b]), ||\cdot ||_\infty $ in just uniform convergence as we've seen earlier.
\end{remark}
\begin{definition}
	(Bounded subset) Let $ (X,d) $ be a metric space. A subset $ E\subseteq X $ is \textit{bounded} if $ E\subseteq B_R(a) $ for some $ a\in X $ and some $ R>0 $.
\end{definition}
\begin{theorem}
	(Bolzano-Weierstrass in $ \R^n $) Every bounded sequence in $ \R^n $ with respect to the Euclidean metric has a convergent subsequence.
\end{theorem}
\pf Proceed by induction on $ n $. We know the base case on $ \R $ holds by standard Bolzano-Weierstrass in IA Analaysis I. Let $ n\ge 2 $, and assume by induction that the theorem holds in $ \R^{n-1} $. Let $ (x^{(k)} $ be a bounded sequence in $ \R^n $, say $ ||x^{(k)}||^2_2\le R^2 $ for some $ R $ and all $ k $. Write $ x^{(k)}=(x_1^{(k)},\dots,x_{n-1}^{(k)},x_n^{(k)}) $ and let $ y^{(k)}=(x_1^{(k)},\dots, x_{n-1}^{(k)})\in \R^{n-1} $. So $ ||y^{(k)}||^2_2+|x_n^{(k)}|^2\le R^2 $. So $ y^{(k)} $ is a bounded sequence in $ \R^{n-1} $, hence by the induction hypothesis, there exists a subsequence $ (k_j) $ of $ (k) $ and a point $ y\in \R^{n-1} $ such that $ y^{(k_j)}\to y $. Also by Bolzano-Weierstrass in $ \R $, there is a further subsequence $ \left(x_n^{(k_{j_\ell})}\right) $ of $ (x_n^{k_j}) $ that converges to say $ y_n\in \R $. Then we know that
\[
	x^{(k_{j_\ell})}\to (y,y_n).
\]
Hence we're finished. \qed\par
Let's show an example where Bolzano-Weierstrass doesn't hold in the infinite dimensional case. Let's look at the the metric space $ (\ell^\infty, ||\cdot ||_\infty). $ If we let $ e_j^{(k)}=\delta_{jk} $ be the sequence with a $ 1 $ in the $ k $th component and $ 0 $ all other components which is clearly bounded. We know that $ e_k^{(k)}\to 0 $ for all fixed $ j $, and hence $ e^{(k)} $ converges componentwise to the zero sequence. However $ e^{(k)} $ does not converge to the zero element since $ ||e^{(k)}-0||_\infty =1 $ for all $ k $. Hence this also doesn't have a convergent subsequence for the same reason.
\begin{remark}
  In fact for normed spaces $ (V,||\cdot ||) $, the Bolzano-Weierstrass property (every bounded sequence has a convergent subsequence) is equivalent to the space being finite dimensional.
\end{remark}
\begin{definition}
	(Limit point) If $ (X,d) $ is a metric space and we have a subset $ E\subseteq X $ and a point $ x\in X $, then say that $ x $ is a \textit{limit point} of $ E $ if there is a sequence $ (x_k)\in E $ with $ x_k\ne x $ for all $ k $ and $ x_k\to x $.
\end{definition}
\begin{definition}
	(Isolated point) If $ (X,d) $ is a metric space and we have a subset $ E\subseteq X $ then $ x\in E $ is a \textit{isolated point} of $ E $ if $ x\in E $ and $ x $ is not a limit point of $ E $. Equivalently there exists a $ r>0 $ such that $ E\cap B_r(x)=\{x\} $.
\end{definition}
\begin{definition}
	(Closure) If $ (X,d) $ is a metric space and we have a subset $ E\subseteq X $ then the \textit{closure} of $ E $ denoted as $ \bar E $ is the union of $ E $ and all of its limit points. (i.e. it's all the points which are the limit of some sequence in $ E $.)
\end{definition}
\begin{definition}
	(Interior point) If $ (X,d) $ is a metric space and we have a subset $ E\subseteq X $ then $ x\in X $ is a \textit{interior point} if there exists a $ r>0 $ such that
	\[
	  B_r(x)\subseteq E.
	\]
\end{definition}
\begin{definition}
(Interior) If $ (X,d) $ is a metric space and we have a subset $ E\subseteq X $ then the \textit{interior} of $ E $ denoted as $ \mathring E $ is the set of interior points of $ E $.
\end{definition}
Let's look at an example. Take $ \R $ with the Euclidean metric and take the set $ E=[0,1)\cup \{2\} $. We can see that the limit points are $ [0,1] $, the closure is $ \{1\} $, the interior points are $ (0,1) $, the isolated points are $ \{2\} $.\par
What about $ E=\Q $? Then there are no isolated points, the closure is $ \R $, there are no interior points, and the limit points are $ \R $.
\begin{proposition}
  Let $ (X,d) $ be a metric space. For any $ E\subseteq X $, the $ \bar E $ is a closed set and in fact
  \[
	  \bar E=\bigcap_{\substack{E\subseteq F\\ F \text{closed}}}F
  \]
\end{proposition}
\pf Example Sheet 2 \qed
\begin{remark}
 What this is really saying is that the closure $ \mathring E $ is the smallest set (by inclusion) which is closed and contains $ E $.
\end{remark}
\begin{proposition}
  Let $ (X,d) $ be a metric space and let $ E\subseteq X $. Then the following statements are equivalent,
  \begin{enumerate}
	  \item If $ (x_k) $ is a sequence in $ E $ with $ (x_k)\to x \in X $, then $ x\in E $;
	  \item $ E=\bar E $;
	  \item $ E $ is closed in $ X $.
  \end{enumerate}
\end{proposition}
\pf Follows directly from the definitions.\qed
\subsection{Cauchy sequences and completeness}
\begin{definition}
	(Cauchy sequence) Let $ (X,d) $ be a metric space. A sequence $ (x_n) $ in $ X $ is a \textit{Cauchy sequence} if
	\[
		(\forall\varepsilon)(\exists N)(\forall n,m\ge N)\ d(x_n,x_m)<\varepsilon.
	\]
\end{definition}
\begin{proposition}
  Let $ (X,d) $ be a metric space. Then we have the following,
  \begin{enumerate}
	  \item Any convergent sequence is Cauchy;
	  \item Any Cauchy sequence is bounded;
	  \item If $ (x_k) $ is a Cauchy sequence that has a convergent subsequence, converging to $ x\in X $, then the whole sequence convergence to $ x $.
  \end{enumerate}
\end{proposition}
\pf 
\begin{enumerate}
	\item If $ x_k\to x $, then we have that
		\[
		  d(x_m,x_n)\le d(x_m,x)+d(x_n,x)\to 0
		\]
		if $ m,n\to \infty $. Hence $ (x_k) $ is Cauchy.
	\item If $ (x_k) $ is Cauchy, then there is some $ N $ such that for all $ n,m\ge N $ we have that
		\[
		  d(x_n,x_m)<1
		\]
		So if we take $ r=\max\{1, d(x_1,x_n), d(x_2,x_n), \dots, d(x_{n-1},x_n)\} $, then the sequence is contained in the ball centred at $ x_n $ with radius $ r+1 $.
	\item Suppose we have a sequence $ x_{k_n}\to x $. We have that $ (x_k) $ is Cauchy, so given some $ \varepsilon>0 $ we can choose $ N $ with $ d(x_n,x_m)<\varepsilon $ for all $ n,m\ge N $. We can also choose $ n_0 $ such that $ k_{n_0}\ge n $ and $ d(x_{k_{n_0}},x)<\varepsilon $. Hence for any $ n\ge N $ we have that
		\[
			d(x_n,x)\le d(x_n,x_{k_{n_0}})+d(x,x_{k_{n_0}})<2\eps.\qed
		\]
\end{enumerate}
\begin{definition}
	(Complete metric space) Let $ (X,d) $ be a metric space. We say that $ X $ is a \textit{complete metric space} if every Cauchy sequence in $ X $ converges to some element in $ X $.
\end{definition}
\begin{definition}
	(Complete normed space)	A normed space $ (V,||\cdot ||) $ is \textit{complete} if $ V $ with the metric defined by $ ||\cdot || $ is complete.
\end{definition}
\begin{theorem}
  $ (\R^n, ||\cdot||_2) $ is complete.
\end{theorem}
\pf If $ (x^{(k)}) $ is a Cauchy sequence in $ \R^n $ then by directly applying the definition, each coordinate is itself a Cauchy sequence of real numbers. Hence by the completeness of $ \R $ (IA Analysis I), we know that $ (x_^{(k)}) $ converges componentwise some $ L\in \R^n $, hence since we have componentwise convergence, we also have convegence in the Euclidean norm. Hence $ (x^{(k)}) $ converges to $ \R^n $, so the metric space $ \R^n $ with the Euclidean metric is complete.\qed
\begin{theorem}
  Any finite dimensional normed space is complete.
\end{theorem}
\pf Follows from the previous theorem and the equivalence of norms.
\begin{theorem}
	The metric space $ (C[a,b],||\cdot ||_\infty) $ is complete.
\end{theorem}
We've proved that uniformly Cauchy implies uniform convergence. Since the uniform limit of continuous functions are continuous we have that all Cauchy sequences converge in $ C[a,b] $.\qed
\begin{theorem}
  The metric spaces $ (\ell_1, ||\cdot ||_1) $, $ (\ell_2, ||\cdot ||_2) $, and $ (\ell_\infty ||\cdot ||_\infty) $ are complete.
\end{theorem}
\pf We'll just prove the $ (\ell_\infty, ||\cdot ||_\infty) $ case. The rest is in Example Sheet 2.\par
When does a subset of a metric space remain complete as a subspace with the induced metric.
\begin{theorem}
  Let $ (X,d) $ be a complete metric space, and $ Y\subseteq X $ any subset. Then $ (Y,d|_Y) $ is complete if and only if $ Y $ is closed.
\end{theorem}
\pf Suppose $ (Y,d|_Y ) $ closed, then let $ (X_k) $ be a sequence in $ Y $, with $ (x_k) $ Cauchy. Then $ x_k\to x\in X $ by completeness in $ X $. By the closure of $ Y $, we have that $ x\in \bar Y=Y $. Conversely suppose that $ (Y,d|_Y) $ is complete then let $ (x_k) $ be a sequence in $ Y $ with $ x_k\to x\in X $. Now $ (x_k) $ is Cauchy in $ X $ hence in $ Y $ as well. By completeness $ x_k\to z\in Y $. By uniqueness of the limit, $ x\in Y $, so $ Y $ is closed.\qed
\par
We can see an example of this, we'll show the that $ L^1 $ is complete with respect to Riemann integration.
\begin{enumerate}
	\item $ (C([a,b]),L^1) $ is complete (proof on Example Sheet 2).
	\item Define $ \tilde{\mathcal R}([a,b])=\mathcal R([a,b])/\sim $, where $ f\sim g $ if $ f=g $ except for a null set. So we have all addition and scalar multiplication defined still, and the $ L^1 $ limit is still well-defined by Lebesgue's theorem.
\end{enumerate}
\begin{theorem}
	If $ V $ is a finite dimensional real vector space and if $ ||\cdot || $, $ ||\cdot ||' $ are two norms on $ V $ then $ ||\cdot ||,||\cdot ||' $ are \textit{Lipchitz-equivalent} (i.e there are constants $ C_1,C_2 >0$ such that
	\[
	  C||x||'\le ||x||\le C||x||'
	\]
	for all $ x\in V$.
\end{theorem} 
\begin{definition}
	(Sequential Compactness) A metric space $ (X,d) $ is \textit{sequentially compact} if every sequence $ (x_n) $ is the space has a convergent subsequence, $ x_{k_j}\to x $. A subset $ K\subseteq X $ is sequentially compact if $(K,d) $ is sequentially compact.
\end{definition}
\begin{remark}
  Another notion of compactness exists which applies more generally to topological spaces. Metric spaces are examples of topological spaces, and a metric space is sequentially compact if and only if the induced topology is compact. In this cases we just write compact to mean sequentially compact. 
\end{remark}
\begin{theorem}
  A subset $ S\subseteq \R^n $ is compact if and only if it is closed and bounded.
\end{theorem}
\pf If the subset is closed and bounded then by Bolzano-Weiestrass and the definition of closed we can prove the space is compact. Conversely let $ S $ be comapct. Then for each $ x_k\to x \in \R^n $, $ x_j\to x\in S $ by uniqueness of the limit and the subsequence goes to the some limit $ z,x\in S $. Suppose $ S $ unbounded. Then pick $ x_n $ such that $ d(x_n,a)>n $. Then $ (x_n) $ has no covergent subsequence.
\begin{theorem}
  If $ (X,d) $ is compact, then $ (X,d) $ is bounded and complete.
\end{theorem}
\pf Any Cauchy sequence has a convergent subsequence, hence converges, so the metric space is complete. Suppose that the metric space is not bounded. Then take the sequence $ x_n $ such that $ d(x_n,x_0)\ge n $. This clearly has no convergent subsequence, contradiction, hence $ (X,d) $ is bounded.
\begin{theorem}
  If $ K\subseteq X $ is a compact subset of a metric space $ X $, then $ K $ is closed and bounded.
\end{theorem}
The converse of this theorem is actually false. Let $ D_r=\overline {B_r} $. Consider $ D_r $ in $ \ell_\infty $ by looking at $ e^{(k)}=(0,0,\dots, 1,\dots ) $. We can see that $ e^{(k)} $ has no convergent subsequence. But $ D_r $ is complete, closed, bounded, but not compact.
\begin{theorem}
  $ K\subseteq \R^n $ with the Euclidean metric is compact if and only if $ K $ is closed and bounded.
\end{theorem}
\pf Suppose that $ K $ is closed and bounded. Let $ (x_k) $ be a sequence in $ K $. Then since $ K $ is bounded, we can apply Bolzano-Weiestrass to get some subsequence $ (x_{k_j}) $ and $ x\in \R^n $ with $ x_{k_j}\to x $. Since $ K $ is closed, we know that $ x\in K $, so $ K $ is compact. We're already proven the converse for general metric space, so it applies here. \qed
\begin{definition}
	(Totally bounded) Let $ (X,d) $ be a metric space. We say that $ (X,d) $ is \textit{totally bounded} if for every $ \eps>0 $ there is a finite set $ \{x_1,\dots, x_n\}\in X $ such that
	\[
		X\subseteq \bigcup_{j=1}^B_\eps(x_j).
	\]
\end{definition}
\begin{theorem}
	$ (X,d) $ is compact if and only if $ (X,d) $ is complete and totally bounded.
\end{theorem}
\subsection{Continuous mappings between metric spaces}
\begin{definition}
	(Continuous) Let $ (X,d) $, $ (X',d') $ be metric spaces, and suppose we have a function $ f:X\to X' $. We say that $ f $ is \textit{conmtinuous} at $ x\in X $ if $ \forall\eps>0 $ there exists a $ \delta>0 $ such that 
	\[
	  d(y,x)<\delta \implies d'(f(y),f(x))<\eps.
	\]
	Equivalently we have that $ \forall\eps>0,\ \exists \delta>0 $ such that $ f(B_\delta(x))\subseteq B_\eps(f(x)) $.
\end{definition}
\begin{theorem}
	(Sequential definition of continuity) Let $ f:X\to X' $. Then $ f $ is continuous at $ x $ if and only if for every sequence $ x_n\to x $ we have that $ f(x_n)\to f(x) $.
\end{theorem}
\pf Suppose that $ f $ is continuous at $ x $. Let $ x_n\to x$ be a sequence in $ X $. Then for a given $ \eps>0 $ we have some $ \delta>0 $ and $ N>0 $ such that $ n\ge N\implies d(x_n,x)< \delta\implies d'(f(x_n),f(x))<\eps $. Conversely if $ f $ is not continuous at $ x $. Then there exists $ \eps>0 $ such that $ \forall n\ge 1 $, there $ \exists x_n $ such that $ d(x_n,x)<\frac 1n $, but $ d'(f(x_n),f(x))\ge \eps $. And so $ x_n\to x $ but $ f(x_n)\to f(x) $ which is a contradiction, hence $ f $ continuous at $ x $.\qed
\begin{theorem}
	Let $ (X,d) $ and $ (X',d') $ be metric spaces. Then the following are equivalent.
	\begin{enumerate}
		\item $ f $ is continuous;
		\item For every sequence $ x_n\to x $, $ f(x_n)\to f(x) $;
		\item $ \inv f(v)=\{x\in X:f(x)\in V\} $ is open in $ X $ for every open $ V\subseteq X' $.
	\end{enumerate}
\end{theorem}
\pf \textit{Exercise.}
\begin{definition}
	(Uniformly continuous) Let $ f:X\to X' $. We say that $ f $ is \textit{uniformly continuous} if there exists some $ \eps>0 $ such that there exists $ \delta>0 $ with $ \forall x,y\in X $
	\[
	  d(x,y)<\delta\implies d'(f(x),f(y))<\eps.
	\]
	Equivalently $ B_\delta(x)\subseteq \inv f(B_\eps(f(x))) $.
\end{definition}
\begin{definition}
  We say that $ f:X\to X' $ is Lipschitz if there exists an $ L $ such that $ \forall x,y\in X $,
  \[
    d'(f(x),f(y))\le Ld(x,y).
  \]
\end{definition}
\begin{theorem}
  If $ f:X\to X' $ is continuous, and $ (X,d) $ is compact and $ (X',d') $ is any metric space then,
  \begin{enumerate}
	  \item $ f $ is uniformly continuous;
	  \item $ f(X) $ is a compact subspace of $ X' $;
	  \item $ f(X) $ is closed and bounded;
	  \item If $ X'=\R $, and $ d' $ is the Euclidean metric, then $ f $ attains its supremum and infimum.
  \end{enumerate}
\end{theorem}
\pf We'll prove the statement part by part.
\begin{enumerate}
\item Argue exactly as we did in the proof for $ \R $, replacing the closed interval $ [a,b] $ for a general compact metric space.
\item Let $ (y_k) $ be a sequence in $ f(X) $. So $ y_k=f(x_k) $ for some sequence $ x_k $ in $ X $. By compactness of $ X $ there exists a subsequence $ x_{k_j} $ in $ X $ such that $ x_{k_j} \to x\in X$ as $ j\to\infty $. By the continuity of $ f $ we know that $ y_{k_j}=f(x_{k_j})\to f(x)\in f(X) $ as $ j\to \infty $.
\item Done by $ f(X) $ compact.
\item By the extreme value of theorem, $ f $ continuous implies that $ f(X) $ is bounded and closed, hence $ f $ attains its supremum and infimu.\qed
\end{enumerate} 
\subsection{Equivalence of metrics and norms}
\begin{definition}
	(Topology) Let $ (X,d) $ be a metric space. The \textit{topology} on $ X $ induced by $ d $ is the collection of open subsets of $ X $.
\end{definition}
\begin{definition}
	(Topologically equivalent) Two metrics $ d,d' $ on $ X $ are \textit{topologically equivalent} if they induce the same topology. So $ U\subseteq X $ is open with resepct to $ d $ if and only if $ U $ is open with resepct to $ d' $.
\end{definition}
\begin{definition}
	(Lipschitz equivalent) Two metrices $ d,d' $ on $ X $ are \textit{lipschitz equivalent} if there exists fixed $ a,b>0 $ such that
	\[
	  ad(x,y)\le d'(x,y)\le bd(x,y)
	\]
	for all $ x,y\in X $.
\end{definition}


\begin{remark}
  We can make some remarks about these definitions.
  \begin{enumerate}
	  \item Topological equivalence and Lipschitz equivalence parition the metrics, hence they form a equivalence relation.
	  \item $ d,d' $ being Lipschitz equivalent $ \implies $ $ d,d' $ are topologically equivalent. But the converse is not true. Lipschitz equivalence is a stronger property than topological equivalence.
  \end{enumerate}
\end{remark}
Let's look at an example to show this. Take $ X=\R $ and $ d(x,y)=|x-y| $, $ d'(x,y)=\min\{1, |x-y|\} $. These are topologically equivalent metrices but $ d(n,0)=n $ and $ d'(n,0)\le 1 $ hence they're not Lipschitz equivalent.\par
A notion or property concerning a metric space $ X $ is a topological property if it depends on the topology on $ X $ and not the specific metric inducing the topology. For example the convergence of sequences is a topological property but the boundedness of a set is not a topological property. However compactness if a topological property. Completeness is not a topological property.
\begin{definition}
  The norms $ ||\cdot ||' $ on a vector space $ V $ are Lipschitz equivalent if there are $ a,b>0 $ such that
  \[
    a||x||\le ||x||'\le b||x||\quad\forall x\in V
  \]
\end{definition}
We write $ B^{||\cdot ||}_R(0)=\{x\in V:||x||<R\} $.
\begin{proposition}
  $ ||\cdot|| $ and $ ||\cdot ||' $ are Lipschitz equiavelnt if and only if there exists $ r,R>0 $ such that
  \[
	  B^{||\cdot ||}_r(0)\subseteq B_1^{||\cdot||'}(0)\subseteq B_R^{||\cdot ||}(0)
  \]
\end{proposition}
\pf \textit{Exercise.}

\begin{theorem}
  Any two norms on a finite dimensional real vector space $ V $ are Lipschitz equivalent.
\end{theorem}
\pf Let $ \dim V=n $. For $ \{e_1,\dots, e_n\} $ a basis for $ V $, define the Euclidean norm $ ||\cdot ||_2 $ on $ V $ by $ ||x||_2=\left(\sum_{j=1}^n a_j^2\right)^{\frac 12} $. Fix $ ||\cdot || $ some other norm on $ V $. For $ x=\sum_{j=1}^n x_je_j $, 
\begin{align*}
	||x||\le \sum_{j=1}^n||x_j\cdot e_j|| &= \sum_{j=1}^n |x_j|||e_j||\\
					      &= \left(\sum_{j=1}^n |x_j|^2\right)^{\frac 12}\left(\sum_{j=1}^n||e_j||^2\right)^{\frac 12}
\end{align*}
So $ ||x||<b||x||_2 $ for all $ x\in V $ where $ b=\left(\sum_{j=1}^n||e_j||^2\right)^{\frac 12} $. To prove the other side, let
\[
	a\le \frac{||x||}{||x||_2}=\left|\left|\frac{x}{||x||_2}\right|\right|.
\]
So need to show that $ a\le ||\hat x|| $ for all $ ||\hat x||_2=1 $. By $ S=\{x\in V: ||x||_2=1\} $ compact we need to show that $ x\to ||x|| $ is continuous which can be shown to complete the proof.\qed
\begin{remark}
  Lipschitz equivalence of two norms, $ ||\cdot || $ and $ ||\cdot||' $ on $ V $ is the statement that the identity map
  \[
    \id_V:(V,||\cdot ||)\to (V,||\cdot||')
  \]
  is Lipschitz (recall that a function is Lipschitz if $ ||f(x)-f(y)||\le L||x-y|| $ for some $ L\in \R $).
\end{remark}
\begin{remark}
  In infinite dimensional vector spaces, inequivalent norms exist. For example see that $ \ell_1\subseteq \ell_\infty $ since absolutely convergent sequences are bounded. So the restriction of $ \ell_\infty $ on the $ \ell_1 $ space is not Lipschitz equivalent to $ \ell_1 $ (take the sequence $ x^(k)=(1,1,\dots,1,0,0,\dots ) $). In fact equivalence of all norms on a $ \R $ vector space characterises finite dimensionality.
\end{remark}

\section{Differentiation in $ \R^n $}
In this secxtion we'll always take the norm $ ||\cdot|| $ on $ \R^n $ to be the Euclidean norm.\par
First we need the following (slightly different definition of the limit).
\begin{definition}
	(Limit) Let $ E\subseteq \R^n $, let $ a\in \R^n $ be a limit point of $ E $. If $ f:E\to \R^m $ and $ b\in \R^m $. Then
	\[
		\lim_{x\to a}f(x)=b
	\]
	means that $ \forall \eps>0 $ there exists $ \delta>0 $ such that $ x\in E $ with $ 0<||x-a||<\delta \implies ||f(x)-b||<\eps$.
\end{definition}
We do not ask that $ f $ be defined at $ x=a $.\par
Given some function $ f:U\to \R^m $, with $ U\subseteq \R^n $ and a point $ a\in U $, how can we define the derivative of $ f $ at $ a $. If $ n=m=1 $ then say $ f:(b,c)\to \R $ and $ a\in(b,c) $ we know that
\[
	f'(a)=\lim_{h\to 0}\frac{f(a+h)-f(a)}h.
\]
If $ n\ge 2 $ we might try to define the derivative as
\[
	\lim_{h\to 0}\frac{f(a+h)-f(a)}{||h||}.\tag{\star}
\]
However, if we take $ n=1 $ in $ \tag{\star} $ we would have $ \lim_{h\to 0}\frac{f(a+h)-f(a)}{|h|} $ which is very different to the definition of the derivative we've been using for years. It also sucks since this definition says that $ y=x $ isn't differentiable at $ 0 $ and $ y=|x| $ is sifferentiable at $ 0 $. This are not properties we want and so $ (\star) $ is not the correct notion we're looking for.\par
Let's look at the 1 dimensional situation a bit closer.
\begin{align*}
	f'(a)=\lim_{h\to 0}\frac{f(a+h)-f(a)}h &\iff \lim_{h\to 0}\frac{f(a+h)-f(a)-f'(a)h}h=0\\
					       &\iff \lim_{h\to 0}\frac{f(a+h)-f(a)-f'(a)h}{|h|}=0\\
					       &\iff \lim_{h\to 0}\frac{f(a+h)-f(a)-Ah}{|h|}=0\quad\text{for some}\ A\in \R.
\end{align*}
This is the correct notion we're looking for.
\begin{definition}
	(Differentiable) Let $ U\subseteq \R^n $ be open and let $ f:U\to \R^m $ and let $ a\in U $. Then we say that $ f $ is \textit{differentiable} at $ a $ if there is a linear map $ A:\R^n\to\R^m $ such that
	\[
		\lim_{h\to 0}\frac{f(a+h)-f(a)-Ah}{||h||}=0.
	\]
	We say that $ A $ is the derivative of $ f $ at $ a $ and write $ A=Df(a) $.
\end{definition}
\begin{remark}
  Since we assume that $ U $ is open, there is aball $ B_r(a)\subseteq U $, hence $ a+h\in U $ for any $ h $ with $ ||h|| $ sufficiently small.
\end{remark}
\begin{proposition}
  Let $ U\subseteq \R^n $ be open and $ f:U\to \R^m $.
  \begin{enumerate}
	  \item If $ f $ is differentiable at $ a\in U $, then $ Df(a) $ is unique.
	  \item If $ f $ is differentiable at $ a\in U $, then $ f $ is continuous at $ a $.
	  \item Write $ f = (f_1,f_2,\dots, f_m) $, where $ f_j:U\to \R $, then $ f $ is differentiable at $ a\in U $ if and only if $ f_j $ is differentiable at $ a $ and
		  \[
		    Df(a)=\begin{pmatrix}
		      Df_1(a) \\
		      \vdots \\
		      Df_m(a)
		    \end{pmatrix}
		  \].
  \end{enumerate} 
\end{proposition}
\pf Exercise.
\begin{remark}
  Differentiability of $ f $ of a point $ a $ means that the affine function $ x\to f(x)+Df(a)(x-a) $ is a good approximation to $ f(x) $ near $ f(a) $. In fact if we look at the definition of $ Df(a) $ this is exactly what it's saying (replacing $ h $ with $ x-a $). The deviation of $ f(x) $ from $ f(a)+Df(a)(x-a) $ tends to zero faster than $ x $ tends to $ a $.
\end{remark} 
Let's introduce the little $ o $ notation formally now.\par
We write $ o(x) $ for $ x\in \R^n $ to denote any function with the property that 
\[
	\frac{o(x)}{||x||}\to 0
\]
as $ x\to 0 $. So in this notation differentiability of $ f $ at $ a $ is the statement that $ f(a+h)=f(a)+Df(a)h+o(h) $.
\subsection{Directional Derivatives}
\begin{definition}
	(Directional derivative) Let $ f:U\to \R^m $ with $ U\subseteq \R^n $ open. Let $ a\in U$. The \textit{directional derivative} of $ f $ at $ a $ in the direction of $ u $ (where $ u\in \R^n $ is fixed), is
	\[
		D_uf(a)=\lim_{t\to 0}\frac{f(a+tu)-f(a)}t
	\]
	if the limit exists.
\end{definition}
\begin{proposition}
  If $ f $ is differentiable at $ a\in U $, then for any $ u\in \R^n $, the directional derivative $ D_uf(a) $ exists and is equal to $ Df(a)u $. In particular the map $ u\to D_uf(a) $ is linear.
\end{proposition}
\pf We know that
\begin{align*}
	\lim_{h\to 0}\frac{f(a+h)-f(a)-Df(x)h}{||h||}=0.
\end{align*}
So that $ h=tu $ with $ t\to 0 $ hence we have that
\begin{align*}
\lim_{t\to 0}\frac{f(a+tu)-f(a)-t(Df(a)u)}{t||u||}=0
\end{align*} 
so we recover that $ Df(a)u=D_uf(a) $.\qed
\par
A special case of this is partial derivatives (usually in the case with $ m = 1$. Let $ \{e_i\} $ be the standard basis for $ \R^n $. Take $ u=e_j $. Then 
\begin{align*}
	D_{e_j}f(a)=\lim_{t\to 0}\frac{f(a+te_j)-f(a)}{t}
\end{align*}
is the $ j $th partial derivative of $ f $ (given the limit exists). We notate this by $ \frac{\partial f}{\partial x_j} $ or $ D_jf(a) $.\par
So if $ f\to U\to \R $ is differentiable at $ a\in U $, then the partial derivatives $ D_jf(a) $ all exist and $ Df(a)=(D_1f(a), \cdots, D_nf(a) )$. More generally,
\begin{proposition}
  If $ U\subseteq \R^n $ is open and $ f:U\to \R^m $, is differentiable at $ a\in U $ then the partial derivatives $ D_jf_i(a) $ all exist and the matrix for $ Df(a) $ is given by $ A=(D_jf_i(a)) $.
\end{proposition}
\pf By the previous discussion and proposition.
\begin{remark}
  In the case where $ m=1 $, $ Df(a) $ is often called the gradient of $ f $ at $ a $.
\end{remark}
We saw that differentiability of $ a $ implies existance of all directoinal derivatives and hence existance of all partials. Let's see an example to show that existance of all partials doesn't imply differentiability. Take the function
\[
  f(x,y)=\begin{cases}
	  0 & xy=0 \\
	  1 & \text{otherwise}
  \end{cases}.
\]
So both partials are zero, but the derivative in the $ (1,1) $ direction does not exist. Hence the function is not differentiable.\par
Even if all directional derivatives exist, we \textit{still} may not have differentiability. Take $ f:\R^2\to\R $ defined by
\[
  f(x,y)=\begin{cases}
	  \frac{x^3}y & y\ne 0\\
	  0 & y =0
  \end{cases}.
\]
Calculating the directional derivatives take $ u=(u_1,u_2) $ and $ t\ne 0 $. Then
\[
	\frac{f(0+tu)-f(0)}t=\begin{cases}
		\frac{tu_1^3}{u_2} & u_2\ne 0\\
		0 & u_2 = 0
	\end{cases}.
\]
So $ D_uf(0) $ exists and is zero for all directions $ u $. But the function is not continuous at the origin hence not differentiable.\par
Let's see a third example. Let $ f:\R^2\to \R $ defined by
\[
  f(x,y)=\begin{cases}
	  \frac{x^3}{x^2+y^2}& (x,y)\ne (0,0) \\
	  0 & (x,y) = (0,0)
  \end{cases}.
\]
We have that $ |f(x,y)|\le |x| $ so $ f $ is continuous everywhere. In fact through direct calculation we can see that all the partials exist at the origin. In fact for any $ u=(u_1,u_2) $ we have that 
\[
	\frac{f(0+tu)-f(0)}t = \frac{u_1^3}{u_1^2+u_2^2}.
\]
So all directional derivatives exist, but the map from $ u\to D_uf(0,0) $ is not linear in $ u $ hence $ f $ can't be differentiable at the origin (we don't have that $ D_uf(0,0)=Df(0,0)u $. Alternatively if $ f $ were differentiable at the origin, then the derivative $ Df(0,0)h=\begin{pmatrix}
	1 & 0 
\end{pmatrix}\begin{pmatrix}
  h_1 \\ h_2
\end{pmatrix}=h_1 $. Hence
\begin{align*}
	\frac{f(0+h)-f(0)-Df(0)h}{||h||}=-\frac{h_1h_2^2}{(h_1^2+h_2^2)^{3/2}}
\end{align*}
which does not tend to zero as $ ||h||\to 0 $. So $ f $ is not differentiable at the origin.
\begin{theorem}
	(Continuity of partial derivatives guarantees differentiability) Let $ f: U \to \R^m $ with $ U\subseteq \R^n $ open. Let $ a\in U $ and suppose that for some $ B_r(a)\in U $ the partial derivatives $ D_jf_i(x) $ exist for every $ x\in B_r(a) $. Suppose also that $ D_jf_i $ is continuous at $ a $ for every $ 1\le i \le n $ and $ 1\le j \le n $. Then $ f $ is differentiable at $ a $.
\end{theorem}
\pf Assume \textit{wlog} that $ m=1 $ by taking coordinatewise functions. Let $ h=(h_1,h_2,\dots, h_n)\in \R^n $. Then we have that 
\begin{align*}
	f(a+h)-f(a)&=\\
		   &\sum_{j=2}^n (f(a+h_1e_1+h_2e_2+\cdots + h_{j-1}e_{j-1}+h_je_j)-f(a+h_1e_1+\cdots +h_{j-1}e_{j-1}))\\
		   &+f(a+h_1e_1)-f(a).
\end{align*}
We can then apply the mean value theorem to the function $ g(t)=f(a+h_1e_1+\dots +h_{j-1}e_{j-1}+te_j) $ for $ t\in [0,h_j] $. Then we get $ f(a+h)-f(a)=\sum_{j=1}^m h_jD_jf(a+h_1e_1+\cdots h_{j-1}e_{j-1}+\theta_jh_je_j) $ for some $ \theta_j\in (0,1) $.
\begin{align*}
	f(a+h)-f(a)-\sum_{j=1}^n h_jD_jf(a)&= \sum_{j=1}^n h_j(D_jf(a+h_1e_1+\cdots \theta_jh_je_j)-D_jf(a))\\
					   &= o(h)
\end{align*}
since the bracketed expression goes to zero. So $ f $ is differentiable at $ a $ with $ Df(a) $ as we've show.\qed
\par
For some of the next results we need a convenient norm on the vector space $ \mathcal L(\R^n,\R^m) $. First we note that any map $ A\in \mathcal L(\R^n,\R^m) $ is Lipschitz hence continuous. Let $ S=\{ x\in \R^n: ||x||=1\} $, $ S $ is compact (closed and bounded), so a map $ A $ is bounded and attains its supremum on the sphere.
\begin{definition}
	(Operator norm on $ \mathcal L(\R^n,\R^m) $) For $ A\in \mathcal L(\R^n,\R^m) $ and define
	\[
		||A||_{\text{op}}=\sup_{|x|=1}||A(x)||=\sup_{x\in \R^n, x\ne 0}\frac{||A(x)||}{||x||}.
	\]
\end{definition}
\begin{remark}
  We can make some remarks about this norm.
  \begin{enumerate}
	  \item $ ||A||_{\text{op}} $ is a norm on $ \mathcal L(\R^n,\R^m) $.
	  \item $ \forall x\in \R^n $, $ ||A(x)||\le ||A||_{\text{op}}||x|| $.
	  \item If $ A\in\mathcal L(\R^n,\R^m) $ and $ B\in \mathcal L(\R^m,\R^p) $ then $ B\circ A\in \mathcal L(\R^n,\R^p) $ and $ ||BA||_{\text{op}}=||B||_{\text{op}}||A||_{\text{op}} $.
	  \item Since $ \dim \mathcal L(\R^n,\R^m)=nm<\infty $ any norm on $ \mathcal L(\R^n, \R^m) $ is equivalent to the operator norm. We will use the operator norm instead because of properties (ii) and (iii). 
  \end{enumerate}
\end{remark}




























\end{document}

