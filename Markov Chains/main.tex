\documentclass{article}
\usepackage{../header}
\newcommand{\E}{\mathbb E}
\title{Markov Chains}
\author{Notes made by Finley Cooper}
\begin{document}
  \maketitle
  \newpage
  \tableofcontents
  \newpage
  \section{Markov Chains}
  \subsection{The Markov property}
  Throughout all our random variables and random processes will be assumed to be defined on an appropiate underlying probablity space $ (\Omega, \mathcal F, \mathbb P) $.
  \begin{definition}
  (Markov chain) A discrete-time Markov chain is a sequence $ \doubleline X=(X_n)_{n\ge 0} $ of random variables taking values in the same discrete countable state space $ I $, such that:
	  \[
		  \prob{X_{n+1}=x_{n+1}|X_0=x_0,\dots, X_n=x_n}=\prob{X_{n+1}=x_{n+1}|X_n=x_n}\quad \forall n\ge 0.
	  \]
  \end{definition}
  If $ \prob{X_{n+1} = y | X_n = x} $ is indepedent of $ n $ for all $ x,y $ then we call $ \doubleline X $ a time-homogeneous Markov chain. For this course all Markov chains are time-homogeneous with a countable state space.\par
\begin{definition}
	(Transition matrix) We define the transition matrix $ P $ as the matrix
	\[
		P(x,y)=P_{xy}=\prob{X_{n+1}=y|X_n=x}.
	\]
\end{definition}
Note that $ P $ is a stochastic matrix i.e. $ P_{xy}\ge 0  $ for all $ x,y $ and the sum of each row is 1.
For example take the simple Markov chain with $ I=\{0,1\} $ moving from $ 0 $ to $ 1 $ w.p. $ \alpha $ and moving from $ 1 $ to $ 0 $ w.p. $ \beta $,
so \[P = 
  \begin{pmatrix}
	  1-\alpha & \alpha \\
	  \beta & 1-\beta 
  \end{pmatrix}
\]
We say that $ \doubleline X= (X_n) $ is a Markov chain with transition matrix $ P $ with initial distribution $ \lambda $ if $ \lambda=(\lambda_n) $ is a distribution and $ I $ is such that $ \prob{X_0=x}=\lambda_i $, for all $ x\in I $, P is the transition matrix of $ \doubleline X $ i.e.
\[
	\prob{X_{n+1}=y|X_n=x,X_{n-1}=i_{n-1},\dots, X_0=i_0}=P_{xy}
\]
for all $ i_0,\dots, i_{n-1}\in I $. Then $ \doubleline X\sim \text{Markov}(\lambda, P) $
\begin{theorem}
$ \doubleline X = (X_n)$ is $ \text{Markov}(\lambda, P) $ on $ I $ if and only if
\[
\prob{X_0=x_0,X_1=x_1,\dots, X_n=x_n}=\lambda_{x_0}p_{x_0x_1},\dots p_{x_{n-1}x_n}
\]
for all $ n\ge 0 $ and all $ x_0,x_1,\dots, x_n\in I $.
\end{theorem} 
\pf First let's prove the forward direction. Suppose that $ \doubleline X $ is Markov. Then \begin{align*}
  &\prob{X_0=x_0,X_1=x_1,\dots,X_n=x_n}
	=& \prob{X_0=x_0,\dots, X_{n-1}=x_{n-1}}\prob{X_n=x_n|X_{n-1}=x_{n-1}\dots, X_0=x_0}
\end{align*}
which iterating over $ n $ gives that
\[
	=\prob{X_0=x_0}P_{x_0x_1}\dots P_{x_{n-1}x_n}
\]
proving the foward direction. For the converse
\begin{align*}
	\prob{X_n=x_n|X_{n-1}=x_{n-1},\dots,X_0=x_0}\\
	= \frac{\prob{X_0=x_0,\dots,X_n=x_n}}{\prob{X_0=x_0,\dots, X_{n-1}=x_{n-1}}}=\frac{\lambda_{x_0}P_{x_0x_1}\dots}{\lambda_{x_0}P_{x_0x_1}\dots}=P_{x_{n-1}x_n}
\end{align*}
and with $ n=0 $ we get our $ \prob{X_0=x_0)}=\lambda_{x_0} $
\begin{definition}
  For $ i\in I $ the $ \delta_i $-mass at $ i $ denotes the probability mass function at $ i $ 
  \[
	  \delta_{ij}=\begin{cases}
		  1 & j=i \\
		  0 & j\ne 1
	  \end{cases}
  \]
\end{definition}
Recall that form a finite collection of random variables $ (X_0,\dots, X_n) $ are indepedent if and only if
\[
	\prob{X_0=x_0,\dots, X_n=x_n)}=\prod_{i=0}^n\prob{X_i=x_i}
\]
for all $ x_0,\dots, x_n \in I$.\par
A process $ (X_n) $ consistant of indepedent RVS ifand only if for any collection of indices $ \{t_1,\dots, t_k\} $ in $ \N $ we have that
\[
	\prob{X_{t_1}=x_{t_1},\dots,X_{t_k}=x_{t_k}}=\prod_{i=1}^k\prob{X_{t_i}=x_{t_i}}
\]
The process $ (X_i) $ is indepedent from the process $ (Y_i) $ iff for any $ \{t_1,t_2,\dots, t_k\} $ and $ \{s_1,\dots, s_m\} $ for any $ k,m\ge \N $ we have that 
\[
	\prob{X_{t_1}=x_{t_1},\cdots, Y_{s_1}=y_{s_1},\cdots} = \prob{X_{t_1}=x_{t_1},\cdots}\prob{Y_{s_1}=y_{s_1},\cdots}
\]
Note that for a Markov chain $ \doubleline X $ it is always the case that $ X_{n+1} $ is conditional independent of $ X_{n-1} $ given $ X_{n} $. But typically $ X_{n+1} $ is not indepedent of $ X_{n-1} $. Let's see an example of this.
\par
If $ (X_n) $ are IID then $ \doubleline X=(X_n) $ is a Markov chain. What is $ \lambda $ and $ P $.
\begin{theorem}
	(Markov property) If $ \overline X \sim \text{Markov}(\lambda, P) $. Then for any $ m\ge 1 $ and $ i\in I $ conditional on $ X_m=i $ the process $ (X_{m+n}) $ is $ \text{Markov}(\delta_i,P) $ and it is indepdent of $ X_0,\dots, X_m $.
\end{theorem}
\pf Clearly, $ \prob{X_m=j|X_m=i}=\delta_{ij} $,
\begin{align*}
	\prob{X_{m+n}=x_{m+n} | X_m=x_m\dots, X_{m+n-1}=x_{m+n-1}} \\
	=\prob{X_{m+n}=x_{m+n} | X_{m+n-1}=x_{m+n-1}}=P_{x_{m+n-1}x_{m_n}}
\end{align*}
so we have that $ (X_{m+n}) $ is $ \text{Markov}(\delta_i, P) $.\par
Now to show independence, is just an application of the law of total probability and is a lot and lot of indices.\qed
\section{Powers of the transition matrix}
Suppose that $ \doubleline X\sim\text{Markov}(\lambda, P) $. Where is $ \prob{X_n=x_n} $ for large $ n $?
\begin{align*}
	\prob{X_n=x}&=\sum_{x_0,\dots, x_{n-1}}\prob{X_0=x_0,\dots, X_n=x_n} \\ &= \sum_{x_0,\dots, x_{n-1}}\lambda_{x_0}P_{x_0x_1}\dots P_{x_{n-1}x_n} \\
		    &= (\lambda P^n)_{x_n}
\end{align*}
So to understand the long time distribution of $ \doubleline X $ it suffices understand the behaviour of $ P^n $ for stochastic matrices. Recall that $ P $ is stochastic if $ P_{xy}\ge 0  $ and each row is a PMF.
\begin{theorem}
Suppose that $ \doubleline X\sim\text{Markov}(\lambda, P) $. Then 
\begin{enumerate}
	\item $ \prob{X_n=x}=(\lambda P^n)_x $ for all $ x\in I, n \ge1 $.
	\item $ \prob{X_{n+m}=y | X_m=x}=(\delta_xP^n)_y=(P^n)_{xy} $.
\end{enumerate}
\end{theorem}
\pf We've proved the first part, let's prove the second statement. Let $ (X_{n+m}) $ be Markov with initial distribution $ \delta_m $ conditional on $ X_m=x $. So by the first statement
\[
	\prob{X_{m+n}=y|X_n=x}=(\delta_x P^n)_y=(P^n)_{xy}
\]\qed
\par
We will use the notation that
\begin{align*}
	\mathbb P_x(\cdots)=\mathbb P(\cdots \mid X_0=x)\\
	\mathbb E_x[\cdots]=\mathbb E[\cdots \mid X_0=x]
\end{align*}
Let's look how to calculate $ P_{ij}(n) $. Suppose that $ I $ is finite, say that $ I=\{1,\dots, k\} $. How do we compute $ P_{11}(n) $? If the matrix $ P $ has $ k $ distinct real eigenvalues, then it is diagonalisable. So we can write
\[
	P=U\mathrm{diag}(\lambda_1,\dots,\lambda_k)\inv U
\]
using a change of basis matrix $ U $. Then
\[
	P^n=U\mathrm{diag}(\lambda_1^n,\dots, \lambda_k^n)\inv U
\].
So $ P_{11}(x)=(P^n)_{11}=a_1\lambda_1^n+\cdots+a_k\lambda_k^n $. Then we can find $ P_{11}(n) $ for small values of $ n $, substitute them to find $ a_1,\dots, a_k $.\par
If $ P $ has some complex eigenvalues, since $ P $ is a real-valued matrix, they necessarily come in complex conjugate pairs. So if $ \lambda_1,\dots, \lambda_{k-2} $ are real and distinct, then $ \lambda_{k-1}=re^{i\theta} $ and $ \lambda_k=re^{-i\theta} $. In this case since all $ P_{ij}(n) $ are real
\[
	P_{11}(n)=\sum_{i=1}^{k-2}a_i\lambda_i^n+a_{k-1}r^n\cos(n\theta)+a_kr^n\sin(n\theta).
\]
If the are repeated eigenvalues, e.g. if $ \lambda_1,\dots, \lambda_{k-2} $ are distinct and $ \lambda_{k-1}=\lambda_k $ then we can use the Jordan normal form of $ P $ to get that the same expansion holds for $ P_{11}(n) $ except that we need to include a term of the form $ (a+bn)\lambda^n_{k-1} $.
\begin{definition}
	We say that state $ i $ \textit{leads to} $ j $, denoted as $ i\to j $ if
	\[
		\prob{X_n=j\text{ for some $ n\ge 0 $}\mid X_0=i}
	\]
	and we say that $ i $ and $ j $ \textit{communicate} if $ i\to j $ and $ j\to i $ we denote this as $ i\longleftrightarrow j $.
\end{definition}
\begin{theorem}
  The following statements are equivalent.
  \begin{enumerate}
	  \item $ i\to j $.
	  \item There is a path $ x_0=i,x_1,\dots, x_n=j $ such that $ p_{x_0x_1},\dots,p_{x_{n-1}x_n} $ are all positive.
	  \item $ P_{ij}(n)\ge 0 $ for some $ n $.
  \end{enumerate}
\end{theorem}
  \pf We have equality in the events
  \[
	  \{x_0=j\text{ for some } n\ge 0 \}=\bigcup_{n\ge 0}\{X_n=j\}
  \]
  hence (i)$ \iff $ (iii)\par
  Also
  \begin{align*}
	  P_{ij}(n)&=\mathbb P_i(X_n=j) \\
		   &= \sum_{\text{all } x_1,\dots, x_{n-1}}P_{ix_1}P_{x_1x_2}\cdots P_{x_{n-1}j}
  \end{align*}
  hence we have that (ii) $ \iff $ (iii).\qed
\begin{corollary}
  Communication defines a equvalence relation on the state space.
\end{corollary}
\pf
By definition $ x\longleftrightarrow x $ and $ x\longleftrightarrow y \iff y\longleftrightarrow x $ are obvious. Suppose that $ x\longleftrightarrow y $ and $ y\longleftrightarrow z $. Then by (ii) in the theorem we have a path from $ x $ to $ y $ to $ z $ so $ x\longleftrightarrow z $.\qed
\begin{definition}
	(Communicating class) The induced equivalences classes are called \textit{communicating classes}. A communicating class $ C\subseteq I $ is \textit{closed} if $ x\to y $ for some $ x\in C $ and $ y\in I $ then we have that $ y\in C $.
\end{definition}
\begin{definition}
	(Absorbing) A state $ x $ is absorbing if $ \{x\} $ is closed.
\end{definition}
This is equivalent to $ P_{xx}=1 $.
\begin{definition}
	(Irreducible) A transition matrix $ P $ is called \textit{irreducible} if $ I $ is a communicating class. i.e. $ x\longleftrightarrow y $ for all $ x,y\in I $.
\end{definition}
\begin{definition}
	(First hitting time) Let $ A\subseteq I $. Then the \textit{first hitting time} $ T_A $ for $ A $ is 
	\[
		T_A=\inf\{x\ge 0: X_n\in A\}
	\]
	which can be infinite if the set empty. The \textit{hitting probability} of $ A $ is the function 
	\[
		h^A:I\to [0,1]
	\]
	defined by
	\[
	  h_i^A=\mathbb P_i(T_A\le \infty)
	\]
	and the \texit{mean hitting time} is the function
	\[
		k^A:I\to(0,\infty]
	\]
	is
	\[
		k^A_i=\mathbb E_i(T_A)=\sum_{n=0}^\infty n\mathbb P_i(T_A=n)+\infty\cdot\prob{T_A=\infty}.
	\]
\end{definition}
\end{document}
